From e91da8d87cde846e2288ca06a6537ed98e1d8620 Mon Sep 17 00:00:00 2001
From: Wang Xuerui <idontknw.wang@gmail.com>
Date: Tue, 30 Aug 2016 15:48:59 +0800
Subject: [PATCH] Squashed Loongnix patches for 2.8.7

---
 configure                            |    6 +-
 libavcodec/bit_depth_template.c      |    4 +-
 libavcodec/h264.c                    |    4 +-
 libavcodec/mips/Makefile             |    2 +
 libavcodec/mips/blockdsp_mmi.c       |  344 +-
 libavcodec/mips/constants.c          |    1 +
 libavcodec/mips/constants.h          |    1 +
 libavcodec/mips/h264chroma_mmi.c     | 1187 ++---
 libavcodec/mips/h264dsp_init_mips.c  |    2 +-
 libavcodec/mips/h264dsp_mmi.c        | 5241 ++++++++++++++---------
 libavcodec/mips/h264pred_init_mips.c |    5 +-
 libavcodec/mips/h264pred_mmi.c       | 1681 +++++---
 libavcodec/mips/h264qpel_mmi.c       | 4048 ++++++++++-------
 libavcodec/mips/hpeldsp_init_mips.c  |   49 +
 libavcodec/mips/hpeldsp_mips.h       |   87 +
 libavcodec/mips/hpeldsp_mmi.c        | 1721 ++++++++
 libavcodec/mips/idctdsp_mmi.c        |  355 +-
 libavcodec/mips/mpegvideo_mmi.c      |  777 ++--
 libavcodec/mips/pixblockdsp_mmi.c    |  134 +-
 libavcodec/mips/vp8dsp_init_mips.c   |   89 +
 libavcodec/mips/vp8dsp_mips.h        |  116 +
 libavcodec/mips/vp8dsp_mmi.c         | 7861 ++++++++++++++++++++++++++++++++++
 libavutil/mips/asmdefs.h             |   12 +
 23 files changed, 18273 insertions(+), 5454 deletions(-)
 create mode 100644 libavcodec/mips/hpeldsp_mmi.c
 create mode 100644 libavcodec/mips/vp8dsp_mmi.c

diff --git a/configure b/configure
index ec4ff08..a08925c 100755
--- a/configure
+++ b/configure
@@ -4060,13 +4060,13 @@ elif enabled mips; then
             disable aligned_stack
             case $cpu in
                 loongson3*)
-                    cpuflags="-march=loongson3a -mhard-float -fno-expensive-optimizations"
+                    cpuflags="-march=loongson3a -mhard-float"
                 ;;
                 loongson2e)
-                    cpuflags="-march=loongson2e -mhard-float -fno-expensive-optimizations"
+                    cpuflags="-march=loongson2e -mhard-float"
                 ;;
                 loongson2f)
-                    cpuflags="-march=loongson2f -mhard-float -fno-expensive-optimizations"
+                    cpuflags="-march=loongson2f -mhard-float"
                 ;;
             esac
         ;;
diff --git a/libavcodec/bit_depth_template.c b/libavcodec/bit_depth_template.c
index 8018489..759cd30 100644
--- a/libavcodec/bit_depth_template.c
+++ b/libavcodec/bit_depth_template.c
@@ -72,7 +72,7 @@
 #   define pixel4 uint32_t
 #   define dctcoef int16_t
 
-#   define INIT_CLIP
+#   define INIT_CLIP const uint8_t *cm = ff_crop_tab + MAX_NEG_CROP;
 #   define no_rnd_avg_pixel4 no_rnd_avg32
 #   define    rnd_avg_pixel4    rnd_avg32
 #   define AV_RN2P  AV_RN16
@@ -84,7 +84,7 @@
 #   define PIXEL_SPLAT_X4(x) ((x)*0x01010101U)
 
 #   define av_clip_pixel(a) av_clip_uint8(a)
-#   define CLIP(a) av_clip_uint8(a)
+#   define CLIP(a) cm[a]
 #endif
 
 #define FUNC3(a, b, c)  a ## _ ## b ## c
diff --git a/libavcodec/h264.c b/libavcodec/h264.c
index 1c7b8a1..53895f3 100644
--- a/libavcodec/h264.c
+++ b/libavcodec/h264.c
@@ -255,8 +255,8 @@ const uint8_t *ff_h264_decode_nal(H264Context *h, H264SliceContext *sl,
 
 #if HAVE_FAST_64BIT
     for (i = 0; i + 1 < length; i += 9) {
-        if (!((~AV_RN64A(src + i) &
-               (AV_RN64A(src + i) - 0x0100010001000101ULL)) &
+        if (!((~AV_RN64(src + i) &
+               (AV_RN64(src + i) - 0x0100010001000101ULL)) &
               0x8000800080008080ULL))
             continue;
         FIND_FIRST_ZERO;
diff --git a/libavcodec/mips/Makefile b/libavcodec/mips/Makefile
index 2e8b1ee..d2d1564 100644
--- a/libavcodec/mips/Makefile
+++ b/libavcodec/mips/Makefile
@@ -77,3 +77,5 @@ MMI-OBJS-$(CONFIG_MPEG4_DECODER)          += mips/xvid_idct_mmi.o
 MMI-OBJS-$(CONFIG_BLOCKDSP)               += mips/blockdsp_mmi.o
 MMI-OBJS-$(CONFIG_PIXBLOCKDSP)            += mips/pixblockdsp_mmi.o
 MMI-OBJS-$(CONFIG_H264QPEL)               += mips/h264qpel_mmi.o
+MMI-OBJS-$(CONFIG_VP8_DECODER)            += mips/vp8dsp_mmi.o
+MMI-OBJS-$(CONFIG_HPELDSP)                += mips/hpeldsp_mmi.o
diff --git a/libavcodec/mips/blockdsp_mmi.c b/libavcodec/mips/blockdsp_mmi.c
index 63eaf69..e32cea1 100644
--- a/libavcodec/mips/blockdsp_mmi.c
+++ b/libavcodec/mips/blockdsp_mmi.c
@@ -22,126 +22,276 @@
  */
 
 #include "blockdsp_mips.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_fill_block16_mmi(uint8_t *block, uint8_t value, int line_size, int h)
 {
+    double ftmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "move $8, %3                \r\n"
-        "move $9, %0                \r\n"
-        "dmtc1 %1, $f2              \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "1:                         \r\n"
-        "gssdlc1 $f2, 7($9)         \r\n"
-        "gssdrc1 $f2, 0($9)         \r\n"
-        "gssdlc1 $f2, 15($9)        \r\n"
-        "gssdrc1 $f2, 8($9)         \r\n"
-        "daddi $8, $8, -1           \r\n"
-        "daddu $9, $9, %2           \r\n"
-        "bnez $8, 1b                \r\n"
-        ::"r"(block),"r"(value),"r"(line_size),"r"(h)
-        : "$8","$9"
+        "mtc1       %[value],   %[ftmp0]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[block])                          \n\t"
+        PTR_ADDI    "%[h],      %[h],           -0x01                   \n\t"
+        "gssdlc1    %[ftmp0],   0x0f(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x08(%[block])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[block])                          \n\t"
+        PTR_ADDI   "%[h],       %[h],           -0x01                   \n\t"
+        "usd        %[all64],   0x08(%[block])                          \n\t"
+#endif
+        PTR_ADDU   "%[block],   %[block],       %[line_size]            \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [block]"+&r"(block),              [h]"+&r"(h),
+          [ftmp0]"=&f"(ftmp[0]),
+          [all64]"=&r"(all64)
+        : [value]"r"(value),                [line_size]"r"((mips_reg)line_size)
+        : "memory"
     );
 }
 
 void ff_fill_block8_mmi(uint8_t *block, uint8_t value, int line_size, int h)
 {
+    double ftmp0;
+    uint64_t all64;
+
     __asm__ volatile (
-        "move $8, %3                \r\n"
-        "move $9, %0                \r\n"
-        "dmtc1 %1, $f2              \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "punpcklbh $f2, $f2, $f2    \r\n"
-        "1:                         \r\n"
-        "gssdlc1 $f2, 7($9)         \r\n"
-        "gssdrc1 $f2, 0($9)         \r\n"
-        "daddi $8, $8, -1           \r\n"
-        "daddu $9, $9, %2           \r\n"
-        "bnez $8, 1b                \r\n"
-        ::"r"(block),"r"(value),"r"(line_size),"r"(h)
-        : "$8","$9"
+        "mtc1       %[value],   %[ftmp0]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[block])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[block])                          \n\t"
+#endif
+        PTR_ADDI   "%[h],       %[h],           -0x01                   \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[line_size]            \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [block]"+&r"(block),              [h]"+&r"(h),
+          [ftmp0]"=&f"(ftmp0),
+          [all64]"=&r"(all64)
+        : [value]"r"(value),                [line_size]"r"((mips_reg)line_size)
+        : "memory"
     );
 }
 
 void ff_clear_block_mmi(int16_t *block)
 {
+    double ftmp[2];
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0              \r\n"
-        "xor $f2, $f2, $f2              \r\n"
-        "gssqc1 $f0, $f2,   0(%0)       \r\n"
-        "gssqc1 $f0, $f2,  16(%0)       \r\n"
-        "gssqc1 $f0, $f2,  32(%0)       \r\n"
-        "gssqc1 $f0, $f2,  48(%0)       \r\n"
-        "gssqc1 $f0, $f2,  64(%0)       \r\n"
-        "gssqc1 $f0, $f2,  80(%0)       \r\n"
-        "gssqc1 $f0, $f2,  96(%0)       \r\n"
-        "gssqc1 $f0, $f2, 112(%0)       \r\n"
-        ::"r"(block)
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x00(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x10(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x20(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x30(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x40(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x50(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x60(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x70(%[block])          \n\t"
+#elif HAVE_LOONGSON2
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x08(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x10(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x18(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x20(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x28(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x30(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x38(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x40(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x48(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x50(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x58(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x60(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x68(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x70(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x78(%[block])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1])
+        : [block]"r"(block)
         : "memory"
     );
 }
 
 void ff_clear_blocks_mmi(int16_t *block)
 {
+    double ftmp[2];
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0              \r\n"
-        "xor $f2, $f2, $f2              \r\n"
-        "gssqc1 $f0, $f2,   0(%0)       \r\n"
-        "gssqc1 $f0, $f2,  16(%0)       \r\n"
-        "gssqc1 $f0, $f2,  32(%0)       \r\n"
-        "gssqc1 $f0, $f2,  48(%0)       \r\n"
-        "gssqc1 $f0, $f2,  64(%0)       \r\n"
-        "gssqc1 $f0, $f2,  80(%0)       \r\n"
-        "gssqc1 $f0, $f2,  96(%0)       \r\n"
-        "gssqc1 $f0, $f2, 112(%0)       \r\n"
-
-        "gssqc1 $f0, $f2, 128(%0)       \r\n"
-        "gssqc1 $f0, $f2, 144(%0)       \r\n"
-        "gssqc1 $f0, $f2, 160(%0)       \r\n"
-        "gssqc1 $f0, $f2, 176(%0)       \r\n"
-        "gssqc1 $f0, $f2, 192(%0)       \r\n"
-        "gssqc1 $f0, $f2, 208(%0)       \r\n"
-        "gssqc1 $f0, $f2, 224(%0)       \r\n"
-        "gssqc1 $f0, $f2, 240(%0)       \r\n"
-
-        "gssqc1 $f0, $f2, 256(%0)       \r\n"
-        "gssqc1 $f0, $f2, 272(%0)       \r\n"
-        "gssqc1 $f0, $f2, 288(%0)       \r\n"
-        "gssqc1 $f0, $f2, 304(%0)       \r\n"
-        "gssqc1 $f0, $f2, 320(%0)       \r\n"
-        "gssqc1 $f0, $f2, 336(%0)       \r\n"
-        "gssqc1 $f0, $f2, 352(%0)       \r\n"
-        "gssqc1 $f0, $f2, 368(%0)       \r\n"
-
-        "gssqc1 $f0, $f2, 384(%0)       \r\n"
-        "gssqc1 $f0, $f2, 400(%0)       \r\n"
-        "gssqc1 $f0, $f2, 416(%0)       \r\n"
-        "gssqc1 $f0, $f2, 432(%0)       \r\n"
-        "gssqc1 $f0, $f2, 448(%0)       \r\n"
-        "gssqc1 $f0, $f2, 464(%0)       \r\n"
-        "gssqc1 $f0, $f2, 480(%0)       \r\n"
-        "gssqc1 $f0, $f2, 496(%0)       \r\n"
-
-        "gssqc1 $f0, $f2, 512(%0)       \r\n"
-        "gssqc1 $f0, $f2, 528(%0)       \r\n"
-        "gssqc1 $f0, $f2, 544(%0)       \r\n"
-        "gssqc1 $f0, $f2, 560(%0)       \r\n"
-        "gssqc1 $f0, $f2, 576(%0)       \r\n"
-        "gssqc1 $f0, $f2, 592(%0)       \r\n"
-        "gssqc1 $f0, $f2, 608(%0)       \r\n"
-        "gssqc1 $f0, $f2, 624(%0)       \r\n"
-
-        "gssqc1 $f0, $f2, 640(%0)       \r\n"
-        "gssqc1 $f0, $f2, 656(%0)       \r\n"
-        "gssqc1 $f0, $f2, 672(%0)       \r\n"
-        "gssqc1 $f0, $f2, 688(%0)       \r\n"
-        "gssqc1 $f0, $f2, 704(%0)       \r\n"
-        "gssqc1 $f0, $f2, 720(%0)       \r\n"
-        "gssqc1 $f0, $f2, 736(%0)       \r\n"
-        "gssqc1 $f0, $f2, 752(%0)       \r\n"
-        ::"r"(block)
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x00(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x10(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x20(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x30(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x40(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x50(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x60(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x70(%[block])          \n\t"
+
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x80(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x90(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xa0(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xb0(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xc0(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xd0(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xe0(%[block])          \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0xf0(%[block])          \n\t"
+
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x100(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x110(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x120(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x130(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x140(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x150(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x160(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x170(%[block])         \n\t"
+
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x180(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x190(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1a0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1b0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1c0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1d0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1e0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x1f0(%[block])         \n\t"
+
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x200(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x210(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x220(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x230(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x240(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x250(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x260(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x270(%[block])         \n\t"
+
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x280(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x290(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2a0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2b0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2c0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2d0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2e0(%[block])         \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp1],       0x2f0(%[block])         \n\t"
+#elif HAVE_LOONGSON2
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x08(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x10(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x18(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x20(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x28(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x30(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x38(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x40(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x48(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x50(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x58(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x60(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x68(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x70(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x78(%[block])                          \n\t"
+
+        "sdc1       %[ftmp0],   0x80(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x88(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0x90(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0x98(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xa0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xa8(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xb0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xb8(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xc0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xc8(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xd0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xd8(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xe0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xe8(%[block])                          \n\t"
+        "sdc1       %[ftmp0],   0xf0(%[block])                          \n\t"
+        "sdc1       %[ftmp1],   0xf8(%[block])                          \n\t"
+
+        "sdc1       %[ftmp0],   0x100(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x108(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x110(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x118(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x120(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x128(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x130(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x138(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x140(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x148(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x150(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x158(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x160(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x168(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x170(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x178(%[block])                         \n\t"
+
+        "sdc1       %[ftmp0],   0x180(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x188(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x190(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x198(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1a0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1a8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1b0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1b8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1c0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1c8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1d0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1d8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1e0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1e8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x1f0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x1f8(%[block])                         \n\t"
+
+        "sdc1       %[ftmp0],   0x200(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x208(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x210(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x218(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x220(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x228(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x230(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x238(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x240(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x248(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x250(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x258(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x260(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x268(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x270(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x278(%[block])                         \n\t"
+
+        "sdc1       %[ftmp0],   0x280(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x288(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x290(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x298(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2a0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2a8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2b0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2b8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2c0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2c8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2d0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2d8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2e0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2e8(%[block])                         \n\t"
+        "sdc1       %[ftmp0],   0x2f0(%[block])                         \n\t"
+        "sdc1       %[ftmp1],   0x2f8(%[block])                         \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1])
+        : [block]"r"((uint64_t *)block)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/constants.c b/libavcodec/mips/constants.c
index f8130d9..3503fad 100644
--- a/libavcodec/mips/constants.c
+++ b/libavcodec/mips/constants.c
@@ -24,6 +24,7 @@
 #include "constants.h"
 
 DECLARE_ALIGNED(8, const uint64_t, ff_pw_1) =       {0x0001000100010001ULL};
+DECLARE_ALIGNED(8, const uint64_t, ff_pw_2) =       {0x0002000200020002ULL};
 DECLARE_ALIGNED(8, const uint64_t, ff_pw_3) =       {0x0003000300030003ULL};
 DECLARE_ALIGNED(8, const uint64_t, ff_pw_4) =       {0x0004000400040004ULL};
 DECLARE_ALIGNED(8, const uint64_t, ff_pw_5) =       {0x0005000500050005ULL};
diff --git a/libavcodec/mips/constants.h b/libavcodec/mips/constants.h
index 0a4effd..19d2d73 100644
--- a/libavcodec/mips/constants.h
+++ b/libavcodec/mips/constants.h
@@ -25,6 +25,7 @@
 #include <stdint.h>
 
 extern const uint64_t ff_pw_1;
+extern const uint64_t ff_pw_2;
 extern const uint64_t ff_pw_3;
 extern const uint64_t ff_pw_4;
 extern const uint64_t ff_pw_5;
diff --git a/libavcodec/mips/h264chroma_mmi.c b/libavcodec/mips/h264chroma_mmi.c
index ef29476..722ccc2 100644
--- a/libavcodec/mips/h264chroma_mmi.c
+++ b/libavcodec/mips/h264chroma_mmi.c
@@ -23,6 +23,8 @@
  */
 
 #include "h264chroma_mips.h"
+#include "constants.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, int stride,
         int h, int x, int y)
@@ -32,171 +34,209 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, int stride,
     const int C = (8 - x) * y;
     const int D = x * y;
     const int E = B + C;
-    int i;
-
-    av_assert2(x<8 && y<8 && x>=0 && y>=0);
+    double ftmp[10];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    uint64_t all64;
 
     if (D) {
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "ldl $4, %6                 \r\n"
-                "ldr $4, %5                 \r\n"
-                "ldl $5, %8                 \r\n"
-                "ldr $5, %7                 \r\n"
-                "daddiu $6, $0, 32          \r\n"
-                "mtc1 %9, $f6               \r\n"
-                "mtc1 %10, $f8              \r\n"
-                "mtc1 %11, $f10             \r\n"
-                "mtc1 %12, $f12             \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "pshufh $f12, $f12, $f20    \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "dsrl32 $2, $2, 0           \r\n"
-                "dsrl32 $3, $3, 0           \r\n"
-                "dsrl32 $4, $4, 0           \r\n"
-                "dsrl32 $5, $5, 0           \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f24, $f2, $f22      \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f24, $f2    \r\n"
-                "sdc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*src),"m"(*(src+7)),"m"(*(src+1)),"m"(*(src+8)),
-                  "m"(*(src+stride)),"m"(*(src+stride+7)),
-                  "m"(*(src+stride+1)),"m"(*(src+stride+8)),
-                  "r"(A),"r"(B),"r"(C),"r"(D)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]            \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp3],   0x07(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp3],   0x00(%[addr0])                      \n\t"
+            "gsldlc1    %[ftmp4],   0x08(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp4],   0x01(%[addr0])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x01(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[B]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[A]                \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[B]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[D]                \n\t"
+            "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[C]                \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[D]                \n\t"
+            "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
     } else if (E) {
         const int step = C ? stride : 1;
 
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "daddiu $6, $0, 32          \r\n"
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "mtc1 %5, $f6               \r\n"
-                "mtc1 %6, $f8               \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "dsrl32 $2, $2, 0           \r\n"
-                "dsrl32 $3, $3, 0           \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f24, $f2, $f22      \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f24, $f2    \r\n"
-                "sdc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),
-                  "m"(*(src+step)),"m"(*(src+step+7)),
-                  "r"(A),"r"(E)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x07(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp2],   0x00(%[addr0])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
     } else {
-        for (i = 0; i < h; i++) {
-            __asm__ volatile (
-                "daddiu $6, $0, 32          \r\n"
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "mtc1 %3, $f6               \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "dsrl32 $2, $2, 0           \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "psrah $f24, $f2, $f22      \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f24, $f2    \r\n"
-                "sdc1 $f2, %0               \r\n"
-                :"=m"(*dst)
-                :"m"(*src),"m"(*(src+7)),"r"(A)
-                :"$2"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "1:                                                         \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x02               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A)
+            : "memory"
+        );
     }
 }
 
@@ -208,140 +248,217 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, int stride,
     const int C = (8 - x) * y;
     const int D = x * y;
     const int E = B + C;
-    int i;
-
-    av_assert2(x<8 && y<8 && x>=0 && y>=0);
+    double ftmp[10];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    uint64_t all64;
 
     if (D) {
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "ldl $4, %6                 \r\n"
-                "ldr $4, %5                 \r\n"
-                "ldl $5, %8                 \r\n"
-                "ldr $5, %7                 \r\n"
-                "daddiu $6, $0, 32          \r\n"
-                "mtc1 %9, $f6               \r\n"
-                "mtc1 %10, $f8              \r\n"
-                "mtc1 %11, $f10             \r\n"
-                "mtc1 %12, $f12             \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "pshufh $f12, $f12, $f20    \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "dsrl32 $2, $2, 0           \r\n"
-                "dsrl32 $3, $3, 0           \r\n"
-                "dsrl32 $4, $4, 0           \r\n"
-                "dsrl32 $5, $5, 0           \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f24, $f2, $f22      \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f24, $f2    \r\n"
-                "ldc1 $f4, %0               \r\n"
-                "pavgb $f2, $f2, $f4        \r\n"
-                "sdc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),"m"(*(src+1)),"m"(*(src+8)),
-                  "m"(*(src+stride)),"m"(*(src+stride+7)),
-                  "m"(*(src+stride+1)),"m"(*(src+stride+8)),
-                  "r"(A),"r"(B),"r"(C),"r"(D)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
-    } else {
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]            \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp3],   0x07(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp3],   0x00(%[addr0])                      \n\t"
+            "gsldlc1    %[ftmp4],   0x08(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp4],   0x01(%[addr0])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x01(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[B]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[A]                \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[B]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[D]                \n\t"
+            "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[C]                \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[D]                \n\t"
+            "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
+    } else if (E) {
         const int step = C ? stride : 1;
 
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "daddiu $6, $0, 32          \r\n"
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "mtc1 %5, $f6               \r\n"
-                "mtc1 %6, $f8               \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "dsrl32 $2, $2, 0           \r\n"
-                "dsrl32 $3, $3, 0           \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f24, $f2, $f22      \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f24, $f2    \r\n"
-                "ldc1 $f4, %0               \r\n"
-                "pavgb $f2, $f2, $f4        \r\n"
-                "sdc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),
-                  "m"(*(src+step)),"m"(*(src+step+7)),"r"(A),"r"(E)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x07(%[addr0])                      \n\t"
+            "gsldrc1    %[ftmp2],   0x00(%[addr0])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
+    } else {
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "1:                                                         \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x02               \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A)
+            : "memory"
+        );
     }
 }
 
@@ -353,118 +470,150 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, int stride,
     const int C = (8 - x) *  y;
     const int D = x *  y;
     const int E = B + C;
-    int i;
-
-    av_assert2(x<8 && y<8 && x>=0 && y>=0);
+    double ftmp[8];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    uint64_t low32;
 
     if (D) {
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "ldl $4, %6                 \r\n"
-                "ldr $4, %5                 \r\n"
-                "ldl $5, %8                 \r\n"
-                "ldr $5, %7                 \r\n"
-                "daddiu $6, $0, 32          \r\n"
-                "mtc1 %9, $f6               \r\n"
-                "mtc1 %10, $f8              \r\n"
-                "mtc1 %11, $f10             \r\n"
-                "mtc1 %12, $f12             \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "pshufh $f12, $f12, $f20    \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f2, $f2     \r\n"
-                "swc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),"m"(*(src+1)),"m"(*(src+8)),
-                  "m"(*(src+stride)),"m"(*(src+stride+7)),
-                  "m"(*(src+stride+1)),"m"(*(src+stride+8)),
-                  "r"(A),"r"(B),"r"(C),"r"(D)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]            \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "uld        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "uld        %[low32],   0x00(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "uld        %[low32],   0x01(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[B]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp6],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[D]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp5],       %[ftmp6]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
     } else if (E) {
         const int step = C ? stride : 1;
 
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "daddiu $4, $0, 32          \r\n"
-                "mtc1 %5, $f6               \r\n"
-                "mtc1 %6, $f8               \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f10              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "daddiu $4, $0, 6           \r\n"
-                "mtc1 $4, $f22              \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f10       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f2, $f20    \r\n"
-                "swc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),"m"(*(src+step)),
-                  "m"(*(src+step+7)),"r"(A),"r"(E)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "uld        %[low32],   0x00(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[E]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp4]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
     } else {
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "lwl $2, %2                 \r\n"
-                "lwr $2, %1                 \r\n"
-                "sw $2, %0                  \r\n"
-                : "=m"(*dst)
-                : "m"(*src),"m"(*(src+3))
-                : "$2"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "1:                                                         \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "addi       %[h],       %[h],           -0x02               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A)
+            : "memory"
+        );
     }
 }
 
@@ -475,108 +624,158 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, int stride,
     const int B = x * (8 - y);
     const int C = (8 - x) * y;
     const int D = x * y;
-    int i;
-
-    av_assert2(x<8 && y<8 && x>=0 && y>=0);
+    const int E = B + C;
+    double ftmp[8];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    uint64_t low32;
 
     if (D) {
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "ldl $4, %6                 \r\n"
-                "ldr $4, %5                 \r\n"
-                "ldl $5, %8                 \r\n"
-                "ldr $5, %7                 \r\n"
-                "daddiu $6, $0, 32          \r\n"
-                "mtc1 %9, $f6               \r\n"
-                "mtc1 %10, $f8              \r\n"
-                "mtc1 %11, $f10             \r\n"
-                "mtc1 %12, $f12             \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f16              \r\n"
-                "mtc1 $5, $f18              \r\n"
-                "mtc1 $6, $f14              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "pshufh $f12, $f12, $f20    \r\n"
-                "pshufh $f14, $f14, $f20    \r\n"
-                "punpcklbh $f16, $f16, $f20 \r\n"
-                "punpcklbh $f18, $f18, $f20 \r\n"
-                "daddiu $6, $0, 6           \r\n"
-                "mtc1 $6, $f22              \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "pmullh $f16, $f10, $f16    \r\n"
-                "pmullh $f18, $f12, $f18    \r\n"
-                "paddh $f2, $f2, $f14       \r\n"
-                "paddh $f4, $f4, $f16       \r\n"
-                "paddh $f2, $f2, $f18       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f2, $f2     \r\n"
-                "lwc1 $f4, %0               \r\n"
-                "pavgb $f2, $f2, $f4        \r\n"
-                "swc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),"m"(*(src+1)),"m"(*(src+8)),
-                  "m"(*(src+stride)),"m"(*(src+stride+7)),
-                  "m"(*(src+stride+1)),"m"(*(src+stride+8)),
-                  "r"(A),"r"(B),"r"(C),"r"(D)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
-    } else {
-        const int E = B + C;
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]            \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "uld        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "uld        %[low32],   0x00(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "uld        %[low32],   0x01(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[B]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp6],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[D]                \n\t"
+            "paddh      %[ftmp2],   %[ftmp5],       %[ftmp6]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
+    } else if (E) {
         const int step = C ? stride : 1;
 
-        for (i=0; i<h; i++) {
-            __asm__ volatile (
-                "ldl $2, %2                 \r\n"
-                "ldr $2, %1                 \r\n"
-                "ldl $3, %4                 \r\n"
-                "ldr $3, %3                 \r\n"
-                "daddiu $4, $0, 32          \r\n"
-                "mtc1 %5, $f6               \r\n"
-                "mtc1 %6, $f8               \r\n"
-                "mtc1 $0, $f20              \r\n"
-                "mtc1 $2, $f2               \r\n"
-                "mtc1 $3, $f4               \r\n"
-                "mtc1 $4, $f10              \r\n"
-                "punpcklbh $f2, $f2, $f20   \r\n"
-                "punpcklbh $f4, $f4, $f20   \r\n"
-                "pshufh $f6, $f6, $f20      \r\n"
-                "pshufh $f8, $f8, $f20      \r\n"
-                "pshufh $f10, $f10, $f20    \r\n"
-                "daddiu $4, $0, 6           \r\n"
-                "mtc1 $4, $f22              \r\n"
-                "pmullh $f2, $f2, $f6       \r\n"
-                "pmullh $f4, $f4, $f8       \r\n"
-                "paddh $f2, $f2, $f10       \r\n"
-                "paddh $f2, $f2, $f4        \r\n"
-                "psrah $f2, $f2, $f22       \r\n"
-                "packushb $f2, $f2, $f20    \r\n"
-                "lwc1 $f4, %0               \r\n"
-                "pavgb $f2, $f2, $f4        \r\n"
-                "swc1 $f2, %0               \r\n"
-                : "=m"(*dst)
-                : "m"(*(src)),"m"(*(src+7)),"m"(*(src+step)),
-                  "m"(*(src+step+7)),"r"(A),"r"(E)
-                : "$2","$3","$4","$5","$6"
-            );
-
-            dst += stride;
-            src += stride;
-        }
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
+            "1:                                                         \n\t"
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "uld        %[low32],   0x00(%[addr0])                      \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[E]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp4]            \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [tmp0]"=&r"(tmp[0]),
+              [addr0]"=&r"(addr[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
+    } else {
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "1:                                                         \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "addi       %[h],       %[h],           -0x02               \n\t"
+            "swc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A)
+            : "memory"
+        );
     }
 }
diff --git a/libavcodec/mips/h264dsp_init_mips.c b/libavcodec/mips/h264dsp_init_mips.c
index 1fe7f84..028524f 100644
--- a/libavcodec/mips/h264dsp_init_mips.c
+++ b/libavcodec/mips/h264dsp_init_mips.c
@@ -128,8 +128,8 @@ static av_cold void h264dsp_init_mmi(H264DSPContext * c, const int bit_depth,
         }
 
         c->h264_v_loop_filter_luma = ff_deblock_v_luma_8_mmi;
-        c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
         c->h264_h_loop_filter_luma = ff_deblock_h_luma_8_mmi;
+        c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
         c->h264_h_loop_filter_luma_intra = ff_deblock_h_luma_intra_8_mmi;
     }
 }
diff --git a/libavcodec/mips/h264dsp_mmi.c b/libavcodec/mips/h264dsp_mmi.c
index 14c4a43..1a7351c 100644
--- a/libavcodec/mips/h264dsp_mmi.c
+++ b/libavcodec/mips/h264dsp_mmi.c
@@ -25,38 +25,68 @@
 
 #include "libavcodec/bit_depth_template.c"
 #include "h264dsp_mips.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
 {
+    double ftmp[9];
+    int low32;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0              \r\n"
-        "ldc1 $f2, 0(%[src])            \r\n"
-        "ldc1 $f4, 8(%[src])            \r\n"
-        "ldc1 $f6, 16(%[src])           \r\n"
-        "ldc1 $f8, 24(%[src])           \r\n"
-        "lwc1 $f10, 0(%[dst0])          \r\n"
-        "lwc1 $f12, 0(%[dst1])          \r\n"
-        "lwc1 $f14, 0(%[dst2])          \r\n"
-        "lwc1 $f16, 0(%[dst3])          \r\n"
-        "punpcklbh $f10, $f10, $f0      \r\n"
-        "punpcklbh $f12, $f12, $f0      \r\n"
-        "punpcklbh $f14, $f14, $f0      \r\n"
-        "punpcklbh $f16, $f16, $f0      \r\n"
-        "paddh $f2, $f2, $f10           \r\n"
-        "paddh $f4, $f4, $f12           \r\n"
-        "paddh $f6, $f6, $f14           \r\n"
-        "paddh $f8, $f8, $f16           \r\n"
-        "packushb $f2, $f2, $f0         \r\n"
-        "packushb $f4, $f4, $f0         \r\n"
-        "packushb $f6, $f6, $f0         \r\n"
-        "packushb $f8, $f8, $f0         \r\n"
-        "swc1 $f2, 0(%[dst0])           \r\n"
-        "swc1 $f4, 0(%[dst1])           \r\n"
-        "swc1 $f6, 0(%[dst2])           \r\n"
-        "swc1 $f8, 0(%[dst3])           \r\n"
-        ::[dst0]"r"(dst),[dst1]"r"(dst+stride),[dst2]"r"(dst+2*stride),
-          [dst3]"r"(dst+3*stride),[src]"r"(src)
-        : "$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "ldc1       %[ftmp1],   0x00(%[src])                            \n\t"
+        "ldc1       %[ftmp2],   0x08(%[src])                            \n\t"
+        "ldc1       %[ftmp3],   0x10(%[src])                            \n\t"
+        "ldc1       %[ftmp4],   0x18(%[src])                            \n\t"
+        "uld        %[low32],   0x00(%[dst0])                           \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "uld        %[low32],   0x00(%[dst1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "uld        %[low32],   0x00(%[dst2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+        "uld        %[low32],   0x00(%[dst3])                           \n\t"
+        "mtc1       %[low32],   %[ftmp8]                                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[dst0])                           \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[dst0])                           \n\t"
+        "gsswlc1    %[ftmp2],   0x03(%[dst1])                           \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst1])                           \n\t"
+        "gsswlc1    %[ftmp3],   0x03(%[dst2])                           \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst2])                           \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[dst3])                           \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[dst3])                           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[dst0])                           \n\t"
+        "mfc1       %[low32],   %[ftmp2]                                \n\t"
+        "usw        %[low32],   0x00(%[dst1])                           \n\t"
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst2])                           \n\t"
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[dst3])                           \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [low32]"=&r"(low32)
+        : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
+          [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
+          [src]"r"(src)
+        : "memory"
     );
 
     memset(src, 0, 32);
@@ -64,79 +94,125 @@ void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
 
 void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 {
+    double ftmp[12];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    int low32;
+
     __asm__ volatile (
-        "dli $8, 1                              \r\n"
-        "ldc1 $f0, 0(%[block])                  \r\n"
-        "dmtc1 $8, $f16                         \r\n"
-        "ldc1 $f2, 8(%[block])                  \r\n"
-        "dli $8, 6                              \r\n"
-        "ldc1 $f4, 16(%[block])                 \r\n"
-        "dmtc1 $8, $f18                         \r\n"
-        "psrah $f8, $f2, $f16                   \r\n"
-        "ldc1 $f6, 24(%[block])                 \r\n"
-        "psrah $f10, $f6, $f16                  \r\n"
-        "psubh $f8, $f8, $f6                    \r\n"
-        "paddh $f10, $f10, $f2                  \r\n"
-        "paddh $f20, $f4, $f0                   \r\n"
-        "psubh $f0, $f0, $f4                    \r\n"
-        "paddh $f22, $f10, $f20                 \r\n"
-        "psubh $f4, $f20, $f10                  \r\n"
-        "paddh $f20, $f8, $f0                   \r\n"
-        "psubh $f0, $f0, $f8                    \r\n"
-        "punpckhhw $f2, $f22, $f20              \r\n"
-        "punpcklhw $f10, $f22, $f20             \r\n"
-        "punpckhhw $f8, $f0, $f4                \r\n"
-        "punpcklhw $f0, $f0, $f4                \r\n"
-        "punpckhwd $f4, $f10, $f0               \r\n"
-        "punpcklwd $f10, $f10, $f0              \r\n"
-        "punpcklwd $f20, $f2, $f8               \r\n"
-        "punpckhwd $f0, $f2, $f8                \r\n"
-        "paddh $f10, $f10, %[ff_pw_32]          \r\n"
-        "psrah $f8, $f4, $f16                   \r\n"
-        "psrah $f6, $f0, $f16                   \r\n"
-        "psubh $f8, $f8, $f0                    \r\n"
-        "paddh $f6, $f6, $f4                    \r\n"
-        "paddh $f2, $f20, $f10                  \r\n"
-        "psubh $f10, $f10, $f20                 \r\n"
-        "paddh $f20, $f6, $f2                   \r\n"
-        "psubh $f2, $f2, $f6                    \r\n"
-        "paddh $f22, $f8, $f10                  \r\n"
-        "xor $f14, $f14, $f14                   \r\n"
-        "psubh $f10, $f10, $f8                  \r\n"
-        "sdc1 $f14, 0(%[block])                 \r\n"
-        "sdc1 $f14, 8(%[block])                 \r\n"
-        "sdc1 $f14, 16(%[block])                \r\n"
-        "sdc1 $f14, 24(%[block])                \r\n"
-        "lwc1 $f4, 0(%[dst])                    \r\n"
-        "psrah $f6, $f20, $f18                  \r\n"
-        "gslwxc1 $f0, 0(%[dst], %[stride])      \r\n"
-        "psrah $f8, $f22, $f18                  \r\n"
-        "punpcklbh $f4, $f4, $f14               \r\n"
-        "punpcklbh $f0, $f0, $f14               \r\n"
-        "paddh $f4, $f4, $f6                    \r\n"
-        "paddh $f0, $f0, $f8                    \r\n"
-        "packushb $f4, $f4, $f14                \r\n"
-        "packushb $f0, $f0, $f14                \r\n"
-        "swc1 $f4, 0(%[dst])                    \r\n"
-        "gsswxc1 $f0, 0(%[dst], %[stride])      \r\n"
-        "daddu %[dst], %[dst], %[stride]        \r\n"
-        "daddu %[dst], %[dst], %[stride]        \r\n"
-        "lwc1 $f4, 0(%[dst])                    \r\n"
-        "psrah $f10, $f10, $f18                 \r\n"
-        "gslwxc1 $f0, 0(%[dst], %[stride])      \r\n"
-        "psrah $f2, $f2, $f18                   \r\n"
-        "punpcklbh $f4, $f4, $f14               \r\n"
-        "punpcklbh $f0, $f0, $f14               \r\n"
-        "paddh $f4, $f4, $f10                   \r\n"
-        "paddh $f0, $f0, $f2                    \r\n"
-        "packushb $f4, $f4, $f14                \r\n"
-        "swc1 $f4, 0(%[dst])                    \r\n"
-        "packushb $f0, $f0, $f14                \r\n"
-        "gsswxc1 $f0, 0(%[dst], %[stride])      \r\n"
-        ::[dst]"r"(dst),[block]"r"(block),[stride]"r"((uint64_t)stride),
-          [ff_pw_32]"f"(ff_pw_32)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20","$f22"
+        "dli        %[tmp0],    0x01                                    \n\t"
+        "ldc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "ldc1       %[ftmp1],   0x08(%[block])                          \n\t"
+        "dli        %[tmp0],    0x06                                    \n\t"
+        "ldc1       %[ftmp2],   0x10(%[block])                          \n\t"
+        "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
+        "psrah      %[ftmp4],   %[ftmp1],       %[ftmp8]                \n\t"
+        "ldc1       %[ftmp3],   0x18(%[block])                          \n\t"
+        "psrah      %[ftmp5],   %[ftmp3],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "paddh      %[ftmp10],  %[ftmp2],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp11],  %[ftmp5],       %[ftmp10]               \n\t"
+        "psubh      %[ftmp2],   %[ftmp10],      %[ftmp5]                \n\t"
+        "paddh      %[ftmp10],  %[ftmp4],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp11],      %[ftmp10]               \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp11],      %[ftmp10]               \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhwd  %[ftmp2],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklwd  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklwd  %[ftmp10],  %[ftmp1],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp1],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_32]             \n\t"
+        "psrah      %[ftmp4],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp0],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp10],      %[ftmp5]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "paddh      %[ftmp10],  %[ftmp3],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp11],  %[ftmp4],       %[ftmp5]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp7],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp7],   0x08(%[block])                          \n\t"
+        "sdc1       %[ftmp7],   0x10(%[block])                          \n\t"
+        "sdc1       %[ftmp7],   0x18(%[block])                          \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "psrah      %[ftmp3],   %[ftmp10],      %[ftmp9]                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr0],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp4],   %[ftmp11],      %[ftmp9]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp2],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst])                            \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp2]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[addr0],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+#endif
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr0],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp2],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst])                            \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp2]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        PTR_ADDU   "%[addr0],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),
+          [low32]"=&r"(low32)
+        : [dst]"r"(dst),                    [block]"r"(block),
+          [stride]"r"((mips_reg)stride),    [ff_pw_32]"f"(ff_pw_32)
+        : "memory"
     );
 
     memset(block, 0, 32);
@@ -144,448 +220,594 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
 void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 {
+    double ftmp[16];
+    uint64_t tmp[8];
+    mips_reg addr[2];
+    int low32;
+
     __asm__ volatile (
-        "lhu $10, 0x0(%[block])                     \r\n"
-        "daddiu $29, $29, -0x20                     \r\n"
-        "daddiu $10, $10, 0x20                      \r\n"
-        "ldc1 $f2, 0x10(%[block])                   \r\n"
-        "sh $10, 0x0(%[block])                      \r\n"
-        "ldc1 $f4, 0x20(%[block])                   \r\n"
-        "dli $10, 0x1                               \r\n"
-        "ldc1 $f6, 0x30(%[block])                   \r\n"
-        "dmtc1 $10, $f16                            \r\n"
-        "ldc1 $f10, 0x50(%[block])                  \r\n"
-        "ldc1 $f12, 0x60(%[block])                  \r\n"
-        "ldc1 $f14, 0x70(%[block])                  \r\n"
-        "mov.d $f0, $f2                             \r\n"
-        "psrah $f2, $f2, $f16                       \r\n"
-        "psrah $f8, $f10, $f16                      \r\n"
-        "paddh $f2, $f2, $f0                        \r\n"
-        "paddh $f8, $f8, $f10                       \r\n"
-        "paddh $f2, $f2, $f10                       \r\n"
-        "paddh $f8, $f8, $f14                       \r\n"
-        "paddh $f2, $f2, $f6                        \r\n"
-        "psubh $f8, $f8, $f0                        \r\n"
-        "psubh $f0, $f0, $f6                        \r\n"
-        "psubh $f10, $f10, $f6                      \r\n"
-        "psrah $f6, $f6, $f16                       \r\n"
-        "paddh $f0, $f0, $f14                       \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "psrah $f14, $f14, $f16                     \r\n"
-        "psubh $f0, $f0, $f6                        \r\n"
-        "dli $10, 0x2                               \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "dmtc1 $10, $f18                            \r\n"
-        "mov.d $f14, $f2                            \r\n"
-        "psrah $f2, $f2, $f18                       \r\n"
-        "psrah $f6, $f8, $f18                       \r\n"
-        "paddh $f6, $f6, $f0                        \r\n"
-        "psrah $f0, $f0, $f18                       \r\n"
-        "paddh $f2, $f2, $f10                       \r\n"
-        "psrah $f10, $f10, $f18                     \r\n"
-        "psubh $f0, $f0, $f8                        \r\n"
-        "psubh $f14, $f14, $f10                     \r\n"
-        "mov.d $f10, $f12                           \r\n"
-        "psrah $f12, $f12, $f16                     \r\n"
-        "psrah $f8, $f4, $f16                       \r\n"
-        "paddh $f12, $f12, $f4                      \r\n"
-        "psubh $f8, $f8, $f10                       \r\n"
-        "ldc1 $f4, 0x0(%[block])                    \r\n"
-        "ldc1 $f10, 0x40(%[block])                  \r\n"
-        "paddh $f10, $f10, $f4                      \r\n"
-        "paddh $f4, $f4, $f4                        \r\n"
-        "paddh $f12, $f12, $f10                     \r\n"
-        "psubh $f4, $f4, $f10                       \r\n"
-        "paddh $f10, $f10, $f10                     \r\n"
-        "paddh $f8, $f8, $f4                        \r\n"
-        "psubh $f10, $f10, $f12                     \r\n"
-        "paddh $f4, $f4, $f4                        \r\n"
-        "paddh $f14, $f14, $f12                     \r\n"
-        "psubh $f4, $f4, $f8                        \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f0, $f0, $f8                        \r\n"
-        "psubh $f12, $f12, $f14                     \r\n"
-        "paddh $f8, $f8, $f8                        \r\n"
-        "paddh $f6, $f6, $f4                        \r\n"
-        "psubh $f8, $f8, $f0                        \r\n"
-        "paddh $f4, $f4, $f4                        \r\n"
-        "paddh $f2, $f2, $f10                       \r\n"
-        "psubh $f4, $f4, $f6                        \r\n"
-        "paddh $f10, $f10, $f10                     \r\n"
-        "sdc1 $f12, 0x0(%[block])                   \r\n"
-        "psubh $f10, $f10, $f2                      \r\n"
-        "punpckhhw $f12, $f14, $f0                  \r\n"
-        "punpcklhw $f14, $f14, $f0                  \r\n"
-        "punpckhhw $f0, $f6, $f2                    \r\n"
-        "punpcklhw $f6, $f6, $f2                    \r\n"
-        "punpckhwd $f2, $f14, $f6                   \r\n"
-        "punpcklwd $f14, $f14, $f6                  \r\n"
-        "punpckhwd $f6, $f12, $f0                   \r\n"
-        "punpcklwd $f12, $f12, $f0                  \r\n"
-        "ldc1 $f0, 0x0(%[block])                    \r\n"
-        "sdc1 $f14, 0x0($29)                        \r\n"
-        "sdc1 $f2, 0x10($29)                        \r\n"
-        "dmfc1 $8, $f12                             \r\n"
-        "dmfc1 $11, $f6                             \r\n"
-        "punpckhhw $f6, $f10, $f4                   \r\n"
-        "punpcklhw $f10, $f10, $f4                  \r\n"
-        "punpckhhw $f4, $f8, $f0                    \r\n"
-        "punpcklhw $f8, $f8, $f0                    \r\n"
-        "punpckhwd $f0, $f10, $f8                   \r\n"
-        "punpcklwd $f10, $f10, $f8                  \r\n"
-        "punpckhwd $f8, $f6, $f4                    \r\n"
-        "punpcklwd $f6, $f6, $f4                    \r\n"
-        "sdc1 $f10, 0x8($29)                        \r\n"
-        "sdc1 $f0, 0x18($29)                        \r\n"
-        "dmfc1 $9, $f6                              \r\n"
-        "dmfc1 $12, $f8                             \r\n"
-        "ldc1 $f2, 0x18(%[block])                   \r\n"
-        "ldc1 $f12, 0x28(%[block])                  \r\n"
-        "ldc1 $f4, 0x38(%[block])                   \r\n"
-        "ldc1 $f0, 0x58(%[block])                   \r\n"
-        "ldc1 $f6, 0x68(%[block])                   \r\n"
-        "ldc1 $f8, 0x78(%[block])                   \r\n"
-        "mov.d $f14, $f2                            \r\n"
-        "psrah $f10, $f0, $f16                      \r\n"
-        "psrah $f2, $f2, $f16                       \r\n"
-        "paddh $f10, $f10, $f0                      \r\n"
-        "paddh $f2, $f2, $f14                       \r\n"
-        "paddh $f10, $f10, $f8                      \r\n"
-        "paddh $f2, $f2, $f0                        \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "paddh $f2, $f2, $f4                        \r\n"
-        "psubh $f14, $f14, $f4                      \r\n"
-        "psubh $f0, $f0, $f4                        \r\n"
-        "psrah $f4, $f4, $f16                       \r\n"
-        "paddh $f14, $f14, $f8                      \r\n"
-        "psubh $f0, $f0, $f8                        \r\n"
-        "psrah $f8, $f8, $f16                       \r\n"
-        "psubh $f14, $f14, $f4                      \r\n"
-        "psubh $f0, $f0, $f8                        \r\n"
-        "mov.d $f8, $f2                             \r\n"
-        "psrah $f4, $f10, $f18                      \r\n"
-        "psrah $f2, $f2, $f18                       \r\n"
-        "paddh $f4, $f4, $f14                       \r\n"
-        "psrah $f14, $f14, $f18                     \r\n"
-        "paddh $f2, $f2, $f0                        \r\n"
-        "psrah $f0, $f0, $f18                       \r\n"
-        "psubh $f14, $f14, $f10                     \r\n"
-        "psubh $f8, $f8, $f0                        \r\n"
-        "mov.d $f0, $f6                             \r\n"
-        "psrah $f6, $f6, $f16                       \r\n"
-        "psrah $f10, $f12, $f16                     \r\n"
-        "paddh $f6, $f6, $f12                       \r\n"
-        "psubh $f10, $f10, $f0                      \r\n"
-        "ldc1 $f12, 0x8(%[block])                   \r\n"
-        "ldc1 $f0, 0x48(%[block])                   \r\n"
-        "paddh $f0, $f0, $f12                       \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f6, $f6, $f0                        \r\n"
-        "psubh $f12, $f12, $f0                      \r\n"
-        "paddh $f0, $f0, $f0                        \r\n"
-        "paddh $f10, $f10, $f12                     \r\n"
-        "psubh $f0, $f0, $f6                        \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f8, $f8, $f6                        \r\n"
-        "psubh $f12, $f12, $f10                     \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "paddh $f14, $f14, $f10                     \r\n"
-        "psubh $f6, $f6, $f8                        \r\n"
-        "paddh $f10, $f10, $f10                     \r\n"
-        "paddh $f4, $f4, $f12                       \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f2, $f2, $f0                        \r\n"
-        "psubh $f12, $f12, $f4                      \r\n"
-        "paddh $f0, $f0, $f0                        \r\n"
-        "sdc1 $f6, 0x8(%[block])                    \r\n"
-        "psubh $f0, $f0, $f2                        \r\n"
-        "punpckhhw $f6, $f8, $f14                   \r\n"
-        "punpcklhw $f8, $f8, $f14                   \r\n"
-        "punpckhhw $f14, $f4, $f2                   \r\n"
-        "punpcklhw $f4, $f4, $f2                    \r\n"
-        "punpckhwd $f2, $f8, $f4                    \r\n"
-        "punpcklwd $f8, $f8, $f4                    \r\n"
-        "punpckhwd $f4, $f6, $f14                   \r\n"
-        "punpcklwd $f6, $f6, $f14                   \r\n"
-        "ldc1 $f14, 0x8(%[block])                   \r\n"
-        "dmfc1 $13, $f8                             \r\n"
-        "dmfc1 $15, $f2                             \r\n"
-        "mov.d $f24, $f6                            \r\n"
-        "mov.d $f28, $f4                            \r\n"
-        "punpckhhw $f4, $f0, $f12                   \r\n"
-        "punpcklhw $f0, $f0, $f12                   \r\n"
-        "punpckhhw $f12, $f10, $f14                 \r\n"
-        "punpcklhw $f10, $f10, $f14                 \r\n"
-        "punpckhwd $f14, $f0, $f10                  \r\n"
-        "punpcklwd $f0, $f0, $f10                   \r\n"
-        "punpckhwd $f10, $f4, $f12                  \r\n"
-        "punpcklwd $f4, $f4, $f12                   \r\n"
-        "dmfc1 $14, $f0                             \r\n"
-        "mov.d $f22, $f14                           \r\n"
-        "mov.d $f26, $f4                            \r\n"
-        "mov.d $f30, $f10                           \r\n"
-        "daddiu $10, %[dst], 0x4                    \r\n"
-        "dmtc1 $15, $f14                            \r\n"
-        "dmtc1 $11, $f12                            \r\n"
-        "ldc1 $f2, 0x10($29)                        \r\n"
-        "dmtc1 $8, $f6                              \r\n"
-        "mov.d $f8, $f2                             \r\n"
-        "psrah $f2, $f2, $f16                       \r\n"
-        "psrah $f0, $f14, $f16                      \r\n"
-        "paddh $f2, $f2, $f8                        \r\n"
-        "paddh $f0, $f0, $f14                       \r\n"
-        "paddh $f2, $f2, $f14                       \r\n"
-        "paddh $f0, $f0, $f28                       \r\n"
-        "paddh $f2, $f2, $f12                       \r\n"
-        "psubh $f0, $f0, $f8                        \r\n"
-        "psubh $f8, $f8, $f12                       \r\n"
-        "psubh $f14, $f14, $f12                     \r\n"
-        "psrah $f12, $f12, $f16                     \r\n"
-        "paddh $f8, $f8, $f28                       \r\n"
-        "psubh $f14, $f14, $f28                     \r\n"
-        "psrah $f10, $f28, $f16                     \r\n"
-        "psubh $f8, $f8, $f12                       \r\n"
-        "psubh $f14, $f14, $f10                     \r\n"
-        "mov.d $f10, $f2                            \r\n"
-        "psrah $f2, $f2, $f18                       \r\n"
-        "psrah $f12, $f0, $f18                      \r\n"
-        "paddh $f2, $f2, $f14                       \r\n"
-        "paddh $f12, $f12, $f8                      \r\n"
-        "psrah $f8, $f8, $f18                       \r\n"
-        "psrah $f14, $f14, $f18                     \r\n"
-        "psubh $f8, $f8, $f0                        \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "mov.d $f14, $f24                           \r\n"
-        "psrah $f4, $f24, $f16                      \r\n"
-        "psrah $f0, $f6, $f16                       \r\n"
-        "paddh $f4, $f4, $f6                        \r\n"
-        "psubh $f0, $f0, $f14                       \r\n"
-        "ldc1 $f6, 0x0($29)                         \r\n"
-        "dmtc1 $13, $f14                            \r\n"
-        "paddh $f14, $f14, $f6                      \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "paddh $f4, $f4, $f14                       \r\n"
-        "psubh $f6, $f6, $f14                       \r\n"
-        "paddh $f14, $f14, $f14                     \r\n"
-        "paddh $f0, $f0, $f6                        \r\n"
-        "psubh $f14, $f14, $f4                      \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "paddh $f10, $f10, $f4                      \r\n"
-        "psubh $f6, $f6, $f0                        \r\n"
-        "paddh $f4, $f4, $f4                        \r\n"
-        "paddh $f8, $f8, $f0                        \r\n"
-        "psubh $f4, $f4, $f10                       \r\n"
-        "paddh $f0, $f0, $f0                        \r\n"
-        "paddh $f12, $f12, $f6                      \r\n"
-        "psubh $f0, $f0, $f8                        \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "paddh $f2, $f2, $f14                       \r\n"
-        "psubh $f6, $f6, $f12                       \r\n"
-        "paddh $f14, $f14, $f14                     \r\n"
-        "sdc1 $f6, 0x0($29)                         \r\n"
-        "psubh $f14, $f14, $f2                      \r\n"
-        "sdc1 $f0, 0x10($29)                        \r\n"
-        "dmfc1 $8, $f4                              \r\n"
-        "xor $f4, $f4, $f4                          \r\n"
-        "sdc1 $f4, 0x0(%[block])                    \r\n"
-        "sdc1 $f4, 0x8(%[block])                    \r\n"
-        "sdc1 $f4, 0x10(%[block])                   \r\n"
-        "sdc1 $f4, 0x18(%[block])                   \r\n"
-        "sdc1 $f4, 0x20(%[block])                   \r\n"
-        "sdc1 $f4, 0x28(%[block])                   \r\n"
-        "sdc1 $f4, 0x30(%[block])                   \r\n"
-        "sdc1 $f4, 0x38(%[block])                   \r\n"
-        "sdc1 $f4, 0x40(%[block])                   \r\n"
-        "sdc1 $f4, 0x48(%[block])                   \r\n"
-        "sdc1 $f4, 0x50(%[block])                   \r\n"
-        "sdc1 $f4, 0x58(%[block])                   \r\n"
-        "sdc1 $f4, 0x60(%[block])                   \r\n"
-        "sdc1 $f4, 0x68(%[block])                   \r\n"
-        "sdc1 $f4, 0x70(%[block])                   \r\n"
-        "sdc1 $f4, 0x78(%[block])                   \r\n"
-        "dli $11, 0x6                               \r\n"
-        "lwc1 $f6, 0x0(%[dst])                      \r\n"
-        "dmtc1 $11, $f20                            \r\n"
-        "gslwxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "psrah $f10, $f10, $f20                     \r\n"
-        "psrah $f8, $f8, $f20                       \r\n"
-        "punpcklbh $f6, $f6, $f4                    \r\n"
-        "punpcklbh $f0, $f0, $f4                    \r\n"
-        "paddh $f6, $f6, $f10                       \r\n"
-        "paddh $f0, $f0, $f8                        \r\n"
-        "packushb $f6, $f6, $f4                     \r\n"
-        "packushb $f0, $f0, $f4                     \r\n"
-        "swc1 $f6, 0x0(%[dst])                      \r\n"
-        "gsswxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "lwc1 $f6, 0x0(%[dst])                      \r\n"
-        "gslwxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "psrah $f12, $f12, $f20                     \r\n"
-        "psrah $f2, $f2, $f20                       \r\n"
-        "punpcklbh $f6, $f6, $f4                    \r\n"
-        "punpcklbh $f0, $f0, $f4                    \r\n"
-        "paddh $f6, $f6, $f12                       \r\n"
-        "paddh $f0, $f0, $f2                        \r\n"
-        "packushb $f6, $f6, $f4                     \r\n"
-        "packushb $f0, $f0, $f4                     \r\n"
-        "swc1 $f6, 0x0(%[dst])                      \r\n"
-        "gsswxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "ldc1 $f10, 0x0($29)                        \r\n"
-        "ldc1 $f8, 0x10($29)                        \r\n"
-        "dmtc1 $8, $f12                             \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "lwc1 $f6, 0x0(%[dst])                      \r\n"
-        "gslwxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "psrah $f14, $f14, $f20                     \r\n"
-        "psrah $f10, $f10, $f20                     \r\n"
-        "punpcklbh $f6, $f6, $f4                    \r\n"
-        "punpcklbh $f0, $f0, $f4                    \r\n"
-        "paddh $f6, $f6, $f14                       \r\n"
-        "paddh $f0, $f0, $f10                       \r\n"
-        "packushb $f6, $f6, $f4                     \r\n"
-        "packushb $f0, $f0, $f4                     \r\n"
-        "swc1 $f6, 0x0(%[dst])                      \r\n"
-        "gsswxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "daddu %[dst], %[dst], %[stride]            \r\n"
-        "lwc1 $f6, 0x0(%[dst])                      \r\n"
-        "gslwxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "psrah $f8, $f8, $f20                       \r\n"
-        "psrah $f12, $f12, $f20                     \r\n"
-        "punpcklbh $f6, $f6, $f4                    \r\n"
-        "punpcklbh $f0, $f0, $f4                    \r\n"
-        "paddh $f6, $f6, $f8                        \r\n"
-        "paddh $f0, $f0, $f12                       \r\n"
-        "packushb $f6, $f6, $f4                     \r\n"
-        "packushb $f0, $f0, $f4                     \r\n"
-        "swc1 $f6, 0x0(%[dst])                      \r\n"
-        "gsswxc1 $f0, 0x0(%[dst], %[stride])        \r\n"
-        "dmtc1 $12, $f2                             \r\n"
-        "dmtc1 $9, $f12                             \r\n"
-        "ldc1 $f8, 0x18($29)                        \r\n"
-        "mov.d $f10, $f8                            \r\n"
-        "psrah $f8, $f8, $f16                       \r\n"
-        "psrah $f14, $f22, $f16                     \r\n"
-        "paddh $f14, $f14, $f22                     \r\n"
-        "paddh $f8, $f8, $f10                       \r\n"
-        "paddh $f14, $f14, $f30                     \r\n"
-        "paddh $f8, $f8, $f22                       \r\n"
-        "psubh $f14, $f14, $f10                     \r\n"
-        "paddh $f8, $f8, $f2                        \r\n"
-        "psubh $f10, $f10, $f2                      \r\n"
-        "psubh $f6, $f22, $f2                       \r\n"
-        "psrah $f2, $f2, $f16                       \r\n"
-        "paddh $f10, $f10, $f30                     \r\n"
-        "psubh $f6, $f6, $f30                       \r\n"
-        "psrah $f4, $f30, $f16                      \r\n"
-        "psubh $f10, $f10, $f2                      \r\n"
-        "psubh $f6, $f6, $f4                        \r\n"
-        "mov.d $f4, $f8                             \r\n"
-        "psrah $f8, $f8, $f18                       \r\n"
-        "psrah $f2, $f14, $f18                      \r\n"
-        "paddh $f8, $f8, $f6                        \r\n"
-        "paddh $f2, $f2, $f10                       \r\n"
-        "psrah $f10, $f10, $f18                     \r\n"
-        "psrah $f6, $f6, $f18                       \r\n"
-        "psubh $f10, $f10, $f14                     \r\n"
-        "psubh $f4, $f4, $f6                        \r\n"
-        "mov.d $f6, $f26                            \r\n"
-        "psrah $f0, $f26, $f16                      \r\n"
-        "psrah $f14, $f12, $f16                     \r\n"
-        "paddh $f0, $f0, $f12                       \r\n"
-        "psubh $f14, $f14, $f6                      \r\n"
-        "ldc1 $f12, 0x8($29)                        \r\n"
-        "dmtc1 $14, $f6                             \r\n"
-        "paddh $f6, $f6, $f12                       \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f0, $f0, $f6                        \r\n"
-        "psubh $f12, $f12, $f6                      \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "paddh $f14, $f14, $f12                     \r\n"
-        "psubh $f6, $f6, $f0                        \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f4, $f4, $f0                        \r\n"
-        "psubh $f12, $f12, $f14                     \r\n"
-        "paddh $f0, $f0, $f0                        \r\n"
-        "paddh $f10, $f10, $f14                     \r\n"
-        "psubh $f0, $f0, $f4                        \r\n"
-        "paddh $f14, $f14, $f14                     \r\n"
-        "paddh $f2, $f2, $f12                       \r\n"
-        "psubh $f14, $f14, $f10                     \r\n"
-        "paddh $f12, $f12, $f12                     \r\n"
-        "paddh $f8, $f8, $f6                        \r\n"
-        "psubh $f12, $f12, $f2                      \r\n"
-        "paddh $f6, $f6, $f6                        \r\n"
-        "sdc1 $f12, 0x8($29)                        \r\n"
-        "psubh $f6, $f6, $f8                        \r\n"
-        "sdc1 $f14, 0x18($29)                       \r\n"
-        "dmfc1 $9, $f0                              \r\n"
-        "xor $f0, $f0, $f0                          \r\n"
-        "lwc1 $f12, 0x0($10)                        \r\n"
-        "gslwxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "psrah $f4, $f4, $f20                       \r\n"
-        "psrah $f10, $f10, $f20                     \r\n"
-        "punpcklbh $f12, $f12, $f0                  \r\n"
-        "punpcklbh $f14, $f14, $f0                  \r\n"
-        "paddh $f12, $f12, $f4                      \r\n"
-        "paddh $f14, $f14, $f10                     \r\n"
-        "packushb $f12, $f12, $f0                   \r\n"
-        "packushb $f14, $f14, $f0                   \r\n"
-        "swc1 $f12, 0x0($10)                        \r\n"
-        "gsswxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "lwc1 $f12, 0x0($10)                        \r\n"
-        "gslwxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "psrah $f2, $f2, $f20                       \r\n"
-        "psrah $f8, $f8, $f20                       \r\n"
-        "punpcklbh $f12, $f12, $f0                  \r\n"
-        "punpcklbh $f14, $f14, $f0                  \r\n"
-        "paddh $f12, $f12, $f2                      \r\n"
-        "paddh $f14, $f14, $f8                      \r\n"
-        "packushb $f12, $f12, $f0                   \r\n"
-        "packushb $f14, $f14, $f0                   \r\n"
-        "swc1 $f12, 0x0($10)                        \r\n"
-        "gsswxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "ldc1 $f4, 0x8($29)                         \r\n"
-        "ldc1 $f10, 0x18($29)                       \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "dmtc1 $9, $f2                              \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "lwc1 $f12, 0x0($10)                        \r\n"
-        "gslwxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "psrah $f6, $f6, $f20                       \r\n"
-        "psrah $f4, $f4, $f20                       \r\n"
-        "punpcklbh $f12, $f12, $f0                  \r\n"
-        "punpcklbh $f14, $f14, $f0                  \r\n"
-        "paddh $f12, $f12, $f6                      \r\n"
-        "paddh $f14, $f14, $f4                      \r\n"
-        "packushb $f12, $f12, $f0                   \r\n"
-        "packushb $f14, $f14, $f0                   \r\n"
-        "swc1 $f12, 0x0($10)                        \r\n"
-        "gsswxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "daddu $10, $10, %[stride]                  \r\n"
-        "lwc1 $f12, 0x0($10)                        \r\n"
-        "gslwxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "psrah $f10, $f10, $f20                     \r\n"
-        "psrah $f2, $f2, $f20                       \r\n"
-        "punpcklbh $f12, $f12, $f0                  \r\n"
-        "punpcklbh $f14, $f14, $f0                  \r\n"
-        "paddh $f12, $f12, $f10                     \r\n"
-        "paddh $f14, $f14, $f2                      \r\n"
-        "packushb $f12, $f12, $f0                   \r\n"
-        "packushb $f14, $f14, $f0                   \r\n"
-        "swc1 $f12, 0x0($10)                        \r\n"
-        "gsswxc1 $f14, 0x0($10, %[stride])          \r\n"
-        "daddiu $29, $29, 0x20                      \r\n"
-        ::[dst]"r"(dst),[block]"r"(block),[stride]"r"((uint64_t)stride)
-        :"$8","$9","$10","$11","$12","$13","$14","$15","$29","$f0","$f2","$f4",
-         "$f8","$f10","$f12","$f14","$f16","$f18","$f20","$f22","$f24","$f26",
-         "$f28","$f30"
+        "lhu        %[tmp0],    0x00(%[block])                          \n\t"
+        PTR_ADDI   "$29,        $29,            -0x20                   \n\t"
+        PTR_ADDIU  "%[tmp0],    %[tmp0],        0x20                    \n\t"
+        "ldc1       %[ftmp1],   0x10(%[block])                          \n\t"
+        "sh         %[tmp0],    0x00(%[block])                          \n\t"
+        "ldc1       %[ftmp2],   0x20(%[block])                          \n\t"
+        "dli        %[tmp0],    0x01                                    \n\t"
+        "ldc1       %[ftmp3],   0x30(%[block])                          \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "ldc1       %[ftmp5],   0x50(%[block])                          \n\t"
+        "ldc1       %[ftmp6],   0x60(%[block])                          \n\t"
+        "ldc1       %[ftmp7],   0x70(%[block])                          \n\t"
+        "mov.d      %[ftmp0],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp4],   %[ftmp5],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
+        "mov.d      %[ftmp7],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp4],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "psrah      %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp6]                                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp4],   %[ftmp2],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "ldc1       %[ftmp2],   0x00(%[block])                          \n\t"
+        "ldc1       %[ftmp5],   0x40(%[block])                          \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "sdc1       %[ftmp6],   0x00(%[block])                          \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp0]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp3],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "ldc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp7],   0x00($29)                               \n\t"
+        "sdc1       %[ftmp1],   0x10($29)                               \n\t"
+        "dmfc1      %[tmp1],    %[ftmp6]                                \n\t"
+        "dmfc1      %[tmp3],    %[ftmp3]                                \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp5],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp5],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp5],   0x08($29)                               \n\t"
+        "sdc1       %[ftmp0],   0x18($29)                               \n\t"
+        "dmfc1      %[tmp2],    %[ftmp3]                                \n\t"
+        "dmfc1      %[tmp4],    %[ftmp4]                                \n\t"
+        "ldc1       %[ftmp1],   0x18(%[block])                          \n\t"
+        "ldc1       %[ftmp6],   0x28(%[block])                          \n\t"
+        "ldc1       %[ftmp2],   0x38(%[block])                          \n\t"
+        "ldc1       %[ftmp0],   0x58(%[block])                          \n\t"
+        "ldc1       %[ftmp3],   0x68(%[block])                          \n\t"
+        "ldc1       %[ftmp4],   0x78(%[block])                          \n\t"
+        "mov.d      %[ftmp7],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp5],   %[ftmp0],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp2],   %[ftmp5],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psrah      %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "mov.d      %[ftmp0],   %[ftmp3]                                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "ldc1       %[ftmp6],   0x08(%[block])                          \n\t"
+        "ldc1       %[ftmp0],   0x48(%[block])                          \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "sdc1       %[ftmp3],   0x08(%[block])                          \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp4],       %[ftmp7]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "punpckhhw  %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp4],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "punpckhwd  %[ftmp2],   %[ftmp3],       %[ftmp7]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp7],   0x08(%[block])                          \n\t"
+        "dmfc1      %[tmp5],    %[ftmp4]                                \n\t"
+        "dmfc1      %[tmp7],    %[ftmp1]                                \n\t"
+        "mov.d      %[ftmp12],  %[ftmp3]                                \n\t"
+        "mov.d      %[ftmp14],  %[ftmp2]                                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp0],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp5],       %[ftmp7]                \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "punpckhwd  %[ftmp7],   %[ftmp0],       %[ftmp5]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp2],       %[ftmp6]                \n\t"
+        "punpcklwd  %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "dmfc1      %[tmp6],    %[ftmp0]                                \n\t"
+        "mov.d      %[ftmp11],  %[ftmp7]                                \n\t"
+        "mov.d      %[ftmp13],  %[ftmp2]                                \n\t"
+        "mov.d      %[ftmp15],  %[ftmp5]                                \n\t"
+        PTR_ADDIU  "%[addr0],   %[dst],         0x04                    \n\t"
+        "dmtc1      %[tmp7],    %[ftmp7]                                \n\t"
+        "dmtc1      %[tmp3],    %[ftmp6]                                \n\t"
+        "ldc1       %[ftmp1],   0x10($29)                               \n\t"
+        "dmtc1      %[tmp1],    %[ftmp3]                                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp0],   %[ftmp7],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp14]               \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp14]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp14]               \n\t"
+        "psrah      %[ftmp5],   %[ftmp14],      %[ftmp8]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp1]                                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp0],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "mov.d      %[ftmp7],   %[ftmp12]                               \n\t"
+        "psrah      %[ftmp2],   %[ftmp12],      %[ftmp8]                \n\t"
+        "psrah      %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp3],   0x00($29)                               \n\t"
+        "dmtc1      %[tmp5],    %[ftmp7]                                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp3],   0x00($29)                               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "sdc1       %[ftmp0],   0x10($29)                               \n\t"
+        "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp2],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x10(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x18(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x20(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x28(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x30(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x38(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x40(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x48(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x50(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x58(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x60(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x68(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x70(%[block])                          \n\t"
+        "sdc1       %[ftmp2],   0x78(%[block])                          \n\t"
+        "dli        %[tmp3],    0x06                                    \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "mtc1       %[tmp3],    %[ftmp10]                               \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp3],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst])                            \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp3],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst])                            \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        "ldc1       %[ftmp5],   0x00($29)                               \n\t"
+        "ldc1       %[ftmp4],   0x10($29)                               \n\t"
+        "dmtc1      %[tmp1],    %[ftmp6]                                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp3],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst])                            \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#endif
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp3],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst])                            \n\t"
+        "gsswxc1    %[ftmp0],   0x00(%[dst],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        "dmtc1      %[tmp4],    %[ftmp1]                                \n\t"
+        "dmtc1      %[tmp2],    %[ftmp6]                                \n\t"
+        "ldc1       %[ftmp4],   0x18($29)                               \n\t"
+        "mov.d      %[ftmp5],   %[ftmp4]                                \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp11],      %[ftmp8]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp15]               \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp11]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp11],      %[ftmp1]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp15]               \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp15]               \n\t"
+        "psrah      %[ftmp2],   %[ftmp15],      %[ftmp8]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "mov.d      %[ftmp2],   %[ftmp4]                                \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "mov.d      %[ftmp3],   %[ftmp13]                               \n\t"
+        "psrah      %[ftmp0],   %[ftmp13],      %[ftmp8]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp6],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "ldc1       %[ftmp6],   0x08($29)                               \n\t"
+        "dmtc1      %[tmp6],    %[ftmp3]                                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp6],   0x08($29)                               \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp7],   0x18($29)                               \n\t"
+        "dmfc1      %[tmp2],    %[ftmp0]                                \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "uld        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+#endif
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp6],   0x03(%[addr0])                          \n\t"
+        "gsswrc1    %[ftmp6],   0x00(%[addr0])                          \n\t"
+        "gsswxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp6]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp7]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+#endif
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp6],   0x03(%[addr0])                          \n\t"
+        "gsswrc1    %[ftmp6],   0x00(%[addr0])                          \n\t"
+        "gsswxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp6]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp7]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        "ldc1       %[ftmp2],   0x08($29)                               \n\t"
+        "ldc1       %[ftmp5],   0x18($29)                               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "dmtc1      %[tmp2],    %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+#endif
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp6],   0x03(%[addr0])                          \n\t"
+        "gsswrc1    %[ftmp6],   0x00(%[addr0])                          \n\t"
+        "gsswxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp6]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp7]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+#if HAVE_LOONGSON3
+        "gslwxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+#endif
+        "psrah      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp10]               \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp6],   0x03(%[addr0])                          \n\t"
+        "gsswrc1    %[ftmp6],   0x00(%[addr0])                          \n\t"
+        "gsswxc1    %[ftmp7],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp6]                                \n\t"
+        "usw        %[low32],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "mfc1       %[low32],   %[ftmp7]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDIU  "$29,        $29,            0x20                    \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [ftmp12]"=&f"(ftmp[12]),          [ftmp13]"=&f"(ftmp[13]),
+          [ftmp14]"=&f"(ftmp[14]),          [ftmp15]"=&f"(ftmp[15]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),              [tmp3]"=&r"(tmp[3]),
+          [tmp4]"=&r"(tmp[4]),              [tmp5]"=&r"(tmp[5]),
+          [tmp6]"=&r"(tmp[6]),              [tmp7]"=&r"(tmp[7]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [low32]"=&r"(low32)
+        : [dst]"r"(dst),                    [block]"r"(block),
+          [stride]"r"((mips_reg)stride)
+        : "$29","memory"
     );
 
     memset(block, 0, 128);
@@ -593,91 +815,145 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
 void ff_h264_idct_dc_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 {
+    int dc = (block[0] + 32) >> 6;
+    double ftmp[6];
+    int low32;
+
+    block[0] = 0;
+
     __asm__ volatile (
-        "lh $8, 0x0(%[block])                       \r\n"
-        "sd $0, 0x0(%[block])                       \r\n"
-        "daddiu $8, $8, 0x20                        \r\n"
-        "daddu $10, %[stride], %[stride]            \r\n"
-        "dsra $8, $8, 0x6                           \r\n"
-        "xor $f2, $f2, $f2                          \r\n"
-        "mtc1 $8, $f0                               \r\n"
-        "pshufh $f0, $f0, $f2                       \r\n"
-        "daddu $8, $10, %[stride]                   \r\n"
-        "psubh $f2, $f2, $f0                        \r\n"
-        "packushb $f0, $f0, $f0                     \r\n"
-        "packushb $f2, $f2, $f2                     \r\n"
-        "lwc1 $f4, 0x0(%[dst])                      \r\n"
-        "gslwxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gslwxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "gslwxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        "paddusb $f4, $f4, $f0                      \r\n"
-        "paddusb $f6, $f6, $f0                      \r\n"
-        "paddusb $f8, $f8, $f0                      \r\n"
-        "paddusb $f10, $f10, $f0                    \r\n"
-        "psubusb $f4, $f4, $f2                      \r\n"
-        "psubusb $f6, $f6, $f2                      \r\n"
-        "psubusb $f8, $f8, $f2                      \r\n"
-        "psubusb $f10, $f10, $f2                    \r\n"
-        "swc1 $f4, 0x0(%[dst])                      \r\n"
-        "gsswxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gsswxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "gsswxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        ::[dst]"r"(dst),[block]"r"(block),[stride]"r"((uint64_t)stride)
-        : "$8","$10","$f0","$f2","$f4","$f6","$f8","$f10"
+        "mtc1       %[dc],      %[ftmp5]                                \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "uld        %[low32],   0x00(%[dst0])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "uld        %[low32],   0x00(%[dst1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "uld        %[low32],   0x00(%[dst2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "uld        %[low32],   0x00(%[dst3])                           \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[dst0])                           \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[dst0])                           \n\t"
+        "gsswlc1    %[ftmp2],   0x03(%[dst1])                           \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst1])                           \n\t"
+        "gsswlc1    %[ftmp3],   0x03(%[dst2])                           \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst2])                           \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[dst3])                           \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[dst3])                           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1      %[low32],    %[ftmp1]                                \n\t"
+        "usw       %[low32],    0x00(%[dst0])                           \n\t"
+        "mfc1      %[low32],    %[ftmp2]                                \n\t"
+        "usw       %[low32],    0x00(%[dst1])                           \n\t"
+        "mfc1      %[low32],    %[ftmp3]                                \n\t"
+        "usw       %[low32],    0x00(%[dst2])                           \n\t"
+        "mfc1      %[low32],    %[ftmp4]                                \n\t"
+        "usw       %[low32],    0x00(%[dst3])                           \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [low32]"=&r"(low32)
+        : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
+          [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
+          [dc]"r"(dc)
+        : "memory"
     );
 }
 
 void ff_h264_idct8_dc_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 {
+    int dc = (block[0] + 32) >> 6;
+    double ftmp[10];
+
+    block[0] = 0;
+
     __asm__ volatile (
-        "lh $8, 0x0(%[block])                       \r\n"
-        "sd $0, 0x0(%[block])                       \r\n"
-        "daddiu $8, $8, 0x20                        \r\n"
-        "daddu $10, %[stride], %[stride]            \r\n"
-        "dsra $8, $8, 0x6                           \r\n"
-        "xor $f2, $f2, $f2                          \r\n"
-        "mtc1 $8, $f0                               \r\n"
-        "pshufh $f0, $f0, $f2                       \r\n"
-        "daddu $8, $10, %[stride]                   \r\n"
-        "psubh $f2, $f2, $f0                        \r\n"
-        "packushb $f0, $f0, $f0                     \r\n"
-        "packushb $f2, $f2, $f2                     \r\n"
-        "ldc1 $f4, 0x0(%[dst])                      \r\n"
-        "gsldxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gsldxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "gsldxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        "paddusb $f4, $f4, $f0                      \r\n"
-        "paddusb $f6, $f6, $f0                      \r\n"
-        "paddusb $f8, $f8, $f0                      \r\n"
-        "paddusb $f10, $f10, $f0                    \r\n"
-        "psubusb $f4, $f4, $f2                      \r\n"
-        "psubusb $f6, $f6, $f2                      \r\n"
-        "psubusb $f8, $f8, $f2                      \r\n"
-        "psubusb $f10, $f10, $f2                    \r\n"
-        "sdc1 $f4, 0x0(%[dst])                      \r\n"
-        "gssdxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gssdxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "daddu $9, $10, $10                         \r\n"
-        "gssdxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        "daddu %[dst], %[dst], $9                   \r\n"
-        "ldc1 $f4, 0x0(%[dst])                      \r\n"
-        "gsldxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gsldxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "gsldxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        "paddusb $f4, $f4, $f0                      \r\n"
-        "paddusb $f6, $f6, $f0                      \r\n"
-        "paddusb $f8, $f8, $f0                      \r\n"
-        "paddusb $f10, $f10, $f0                    \r\n"
-        "psubusb $f4, $f4, $f2                      \r\n"
-        "psubusb $f6, $f6, $f2                      \r\n"
-        "psubusb $f8, $f8, $f2                      \r\n"
-        "psubusb $f10, $f10, $f2                    \r\n"
-        "sdc1 $f4, 0x0(%[dst])                      \r\n"
-        "gssdxc1 $f6, 0x0(%[dst], %[stride])        \r\n"
-        "gssdxc1 $f8, 0x0(%[dst], $10)              \r\n"
-        "gssdxc1 $f10, 0x0(%[dst], $8)              \r\n"
-        ::[dst]"r"(dst),[block]"r"(block),[stride]"r"((uint64_t)stride)
-        : "$8","$9","$10","$f0","$f2","$f4","$f6","$f8","$f10"
+        "mtc1       %[dc],      %[ftmp5]                                \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "ldc1       %[ftmp1],   0x00(%[dst0])                           \n\t"
+        "ldc1       %[ftmp2],   0x00(%[dst1])                           \n\t"
+        "ldc1       %[ftmp3],   0x00(%[dst2])                           \n\t"
+        "ldc1       %[ftmp4],   0x00(%[dst3])                           \n\t"
+        "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp9],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "sdc1       %[ftmp1],   0x00(%[dst0])                           \n\t"
+        "sdc1       %[ftmp2],   0x00(%[dst1])                           \n\t"
+        "sdc1       %[ftmp3],   0x00(%[dst2])                           \n\t"
+        "sdc1       %[ftmp4],   0x00(%[dst3])                           \n\t"
+
+        "ldc1       %[ftmp1],   0x00(%[dst4])                           \n\t"
+        "ldc1       %[ftmp2],   0x00(%[dst5])                           \n\t"
+        "ldc1       %[ftmp3],   0x00(%[dst6])                           \n\t"
+        "ldc1       %[ftmp4],   0x00(%[dst7])                           \n\t"
+        "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp9],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "sdc1       %[ftmp1],   0x00(%[dst4])                           \n\t"
+        "sdc1       %[ftmp2],   0x00(%[dst5])                           \n\t"
+        "sdc1       %[ftmp3],   0x00(%[dst6])                           \n\t"
+        "sdc1       %[ftmp4],   0x00(%[dst7])                           \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9])
+        : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
+          [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
+          [dst4]"r"(dst+4*stride),          [dst5]"r"(dst+5*stride),
+          [dst6]"r"(dst+6*stride),          [dst7]"r"(dst+7*stride),
+          [dc]"r"(dc)
+        : "memory"
     );
 }
 
@@ -775,212 +1051,225 @@ void ff_h264_idct_add8_422_8_mmi(uint8_t **dest, const int *block_offset,
 void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         int qmul)
 {
+    double ftmp[10];
+    uint64_t tmp[2];
+
     __asm__ volatile (
-        ".set noreorder                                 \r\n"
-        "dli $10, 0x8                                   \r\n"
-        "ldc1 $f6, 0x18(%[input])                       \r\n"
-        "dmtc1 $10, $f16                                \r\n"
-        "ldc1 $f4, 0x10(%[input])                       \r\n"
-        "dli $10, 0x20                                  \r\n"
-        "ldc1 $f2, 0x8(%[input])                        \r\n"
-        "dmtc1 $10, $f18                                \r\n"
-        "ldc1 $f0, 0x0(%[input])                        \r\n"
-        "mov.d $f8, $f6                                 \r\n"
-        "paddh $f6, $f6, $f4                            \r\n"
-        "psubh $f4, $f4, $f8                            \r\n"
-        "mov.d $f8, $f2                                 \r\n"
-        "paddh $f2, $f2, $f0                            \r\n"
-        "psubh $f0, $f0, $f8                            \r\n"
-        "mov.d $f8, $f6                                 \r\n"
-        "paddh $f6, $f6, $f2                            \r\n"
-        "psubh $f2, $f2, $f8                            \r\n"
-        "mov.d $f8, $f4                                 \r\n"
-        "paddh $f4, $f4, $f0                            \r\n"
-        "psubh $f0, $f0, $f8                            \r\n"
-        "mov.d $f8, $f6                                 \r\n"
-        "punpcklhw $f6, $f6, $f2                        \r\n"
-        "punpckhhw $f8, $f8, $f2                        \r\n"
-        "punpckhhw $f2, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhwd $f4, $f6, $f0                        \r\n"
-        "punpcklwd $f6, $f6, $f0                        \r\n"
-        "mov.d $f0, $f8                                 \r\n"
-        "punpcklwd $f8, $f8, $f2                        \r\n"
-        "punpckhwd $f0, $f0, $f2                        \r\n"
-        "mov.d $f2, $f0                                 \r\n"
-        "paddh $f0, $f0, $f8                            \r\n"
-        "psubh $f8, $f8, $f2                            \r\n"
-        "mov.d $f2, $f4                                 \r\n"
-        "paddh $f4, $f4, $f6                            \r\n"
-        "psubh $f6, $f6, $f2                            \r\n"
-        "mov.d $f2, $f0                                 \r\n"
-        "paddh $f0, $f0, $f4                            \r\n"
-        "psubh $f4, $f4, $f2                            \r\n"
-        "mov.d $f2, $f8                                 \r\n"
-        "daddiu $10, %[qmul], -0x7fff                   \r\n"
-        "paddh $f8, $f8, $f6                            \r\n"
-        "bgtz $10, 1f                                   \r\n"
-        "psubh $f6, $f6, $f2                            \r\n"
-        "ori $10, $0, 0x80                              \r\n"
-        "dsll $10, $10, 0x10                            \r\n"
-        "punpckhhw $f2, $f0, %[ff_pw_1]                 \r\n"
-        "daddu %[qmul], %[qmul], $10                    \r\n"
-        "punpcklhw $f0, $f0, %[ff_pw_1]                 \r\n"
-        "punpckhhw $f10, $f4, %[ff_pw_1]                \r\n"
-        "punpcklhw $f4, $f4, %[ff_pw_1]                 \r\n"
-        "mtc1 %[qmul], $f14                             \r\n"
-        "punpcklwd $f14, $f14, $f14                     \r\n"
-        "pmaddhw $f0, $f0, $f14                         \r\n"
-        "pmaddhw $f4, $f4, $f14                         \r\n"
-        "pmaddhw $f2, $f2, $f14                         \r\n"
-        "pmaddhw $f10, $f10, $f14                       \r\n"
-        "psraw $f0, $f0, $f16                           \r\n"
-        "psraw $f4, $f4, $f16                           \r\n"
-        "psraw $f2, $f2, $f16                           \r\n"
-        "psraw $f10, $f10, $f16                         \r\n"
-        "packsswh $f0, $f0, $f2                         \r\n"
-        "packsswh $f4, $f4, $f10                        \r\n"
-        "mfc1 $9, $f0                                   \r\n"
-        "dsrl $f0, $f0, $f18                            \r\n"
-        "mfc1 %[input], $f0                             \r\n"
-        "sh $9, 0x0(%[output])                          \r\n"
-        "sh %[input], 0x80(%[output])                   \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x20(%[output])                         \r\n"
-        "sh %[input], 0xa0(%[output])                   \r\n"
-        "mfc1 $9, $f4                                   \r\n"
-        "dsrl $f4, $f4, $f18                            \r\n"
-        "mfc1 %[input], $f4                             \r\n"
-        "sh $9, 0x40(%[output])                         \r\n"
-        "sh %[input], 0xc0(%[output])                   \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x60(%[output])                         \r\n"
-        "sh %[input], 0xe0(%[output])                   \r\n"
-        "punpckhhw $f2, $f6, %[ff_pw_1]                 \r\n"
-        "punpcklhw $f6, $f6, %[ff_pw_1]                 \r\n"
-        "punpckhhw $f10, $f8, %[ff_pw_1]                \r\n"
-        "punpcklhw $f8, $f8, %[ff_pw_1]                 \r\n"
-        "mtc1 %[qmul], $f14                             \r\n"
-        "punpcklwd $f14, $f14, $f14                     \r\n"
-        "pmaddhw $f6, $f6, $f14                         \r\n"
-        "pmaddhw $f8, $f8, $f14                         \r\n"
-        "pmaddhw $f2, $f2, $f14                         \r\n"
-        "pmaddhw $f10, $f10, $f14                       \r\n"
-        "psraw $f6, $f6, $f16                           \r\n"
-        "psraw $f8, $f8, $f16                           \r\n"
-        "psraw $f2, $f2, $f16                           \r\n"
-        "psraw $f10, $f10, $f16                         \r\n"
-        "packsswh $f6, $f6, $f2                         \r\n"
-        "packsswh $f8, $f8, $f10                        \r\n"
-        "mfc1 $9, $f6                                   \r\n"
-        "dsrl $f6, $f6, $f18                            \r\n"
-        "mfc1 %[input], $f6                             \r\n"
-        "sh $9, 0x100(%[output])                        \r\n"
-        "sh %[input], 0x180(%[output])                  \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x120(%[output])                        \r\n"
-        "sh %[input], 0x1a0(%[output])                  \r\n"
-        "mfc1 $9, $f8                                   \r\n"
-        "dsrl $f8, $f8, $f18                            \r\n"
-        "mfc1 %[input], $f8                             \r\n"
-        "sh $9, 0x140(%[output])                        \r\n"
-        "sh %[input], 0x1c0(%[output])                  \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x160(%[output])                        \r\n"
-        "jr $31                                         \r\n"
-        "sh %[input], 0x1e0(%[output])                  \r\n"
-        "1:                                             \r\n"
-        "ori $10, $0, 0x1f                              \r\n"
-        "clz $9, %[qmul]                                \r\n"
-        "ori %[input], $0, 0x7                          \r\n"
-        "dsubu $9, $10, $9                              \r\n"
-        "ori $10, $0, 0x80                              \r\n"
-        "dsll $10, $10, 0x10                            \r\n"
-        "daddu %[qmul], %[qmul], $10                    \r\n"
-        "dsubu $10, $9, %[input]                        \r\n"
-        "movn $9, %[input], $10                         \r\n"
-        "daddiu %[input], %[input], 0x1                 \r\n"
-        "andi $10, $9, 0xff                             \r\n"
-        "dsrlv %[qmul], %[qmul], $10                    \r\n"
-        "dsubu %[input], %[input], $9                   \r\n"
-        "mtc1 %[input], $f12                            \r\n"
-        "punpckhhw $f2, $f0, %[ff_pw_1]                 \r\n"
-        "punpcklhw $f0, $f0, %[ff_pw_1]                 \r\n"
-        "punpckhhw $f10, $f4, %[ff_pw_1]                \r\n"
-        "punpcklhw $f4, $f4, %[ff_pw_1]                 \r\n"
-        "mtc1 %[qmul], $f14                             \r\n"
-        "punpcklwd $f14, $f14, $f14                     \r\n"
-        "pmaddhw $f0, $f0, $f14                         \r\n"
-        "pmaddhw $f4, $f4, $f14                         \r\n"
-        "pmaddhw $f2, $f2, $f14                         \r\n"
-        "pmaddhw $f10, $f10, $f14                       \r\n"
-        "psraw $f0, $f0, $f12                           \r\n"
-        "psraw $f4, $f4, $f12                           \r\n"
-        "psraw $f2, $f2, $f12                           \r\n"
-        "psraw $f10, $f10, $f12                         \r\n"
-        "packsswh $f0, $f0, $f2                         \r\n"
-        "packsswh $f4, $f4, $f10                        \r\n"
-        "mfc1 $9, $f0                                   \r\n"
-        "dsrl $f0, $f0, $f18                            \r\n"
-        "sh $9, 0x0(%[output])                          \r\n"
-        "mfc1 %[input], $f0                             \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "sh %[input], 0x80(%[output])                   \r\n"
-        "sh $9, 0x20(%[output])                         \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "mfc1 $9, $f4                                   \r\n"
-        "sh %[input], 0xa0(%[output])                   \r\n"
-        "dsrl $f4, $f4, $f18                            \r\n"
-        "sh $9, 0x40(%[output])                         \r\n"
-        "mfc1 %[input], $f4                             \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "sh %[input], 0xc0(%[output])                   \r\n"
-        "sh $9, 0x60(%[output])                         \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh %[input], 0xe0(%[output])                   \r\n"
-        "punpckhhw $f2, $f6, %[ff_pw_1]                 \r\n"
-        "punpcklhw $f6, $f6, %[ff_pw_1]                 \r\n"
-        "punpckhhw $f10, $f8, %[ff_pw_1]                \r\n"
-        "punpcklhw $f8, $f8, %[ff_pw_1]                 \r\n"
-        "mtc1 %[qmul], $f14                             \r\n"
-        "punpcklwd $f14, $f14, $f14                     \r\n"
-        "pmaddhw $f6, $f6, $f14                         \r\n"
-        "pmaddhw $f8, $f8, $f14                         \r\n"
-        "pmaddhw $f2, $f2, $f14                         \r\n"
-        "pmaddhw $f10, $f10, $f14                       \r\n"
-        "psraw $f6, $f6, $f12                           \r\n"
-        "psraw $f8, $f8, $f12                           \r\n"
-        "psraw $f2, $f2, $f12                           \r\n"
-        "psraw $f10, $f10, $f12                         \r\n"
-        "packsswh $f6, $f6, $f2                         \r\n"
-        "packsswh $f8, $f8, $f10                        \r\n"
-        "mfc1 $9, $f6                                   \r\n"
-        "dsrl $f6, $f6, $f18                            \r\n"
-        "mfc1 %[input], $f6                             \r\n"
-        "sh $9, 0x100(%[output])                        \r\n"
-        "sh %[input], 0x180(%[output])                  \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x120(%[output])                        \r\n"
-        "sh %[input], 0x1a0(%[output])                  \r\n"
-        "mfc1 $9, $f8                                   \r\n"
-        "dsrl $f8, $f8, $f18                            \r\n"
-        "mfc1 %[input], $f8                             \r\n"
-        "sh $9, 0x140(%[output])                        \r\n"
-        "sh %[input], 0x1c0(%[output])                  \r\n"
-        "dsrl $9, $9, 0x10                              \r\n"
-        "dsrl %[input], %[input], 0x10                  \r\n"
-        "sh $9, 0x160(%[output])                        \r\n"
-        "sh %[input], 0x1e0(%[output])                  \r\n"
-        ".set reorder                                   \r\n"
-        ::[output]"r"(output),[input]"r"(input),[qmul]"r"((uint64_t)qmul),
-          [ff_pw_1]"f"(ff_pw_1)
-        : "$9","$10","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18"
+        ".set       noreorder                                           \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "ldc1       %[ftmp3],   0x18(%[input])                          \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "ldc1       %[ftmp2],   0x10(%[input])                          \n\t"
+        "dli        %[tmp0],    0x20                                    \n\t"
+        "ldc1       %[ftmp1],   0x08(%[input])                          \n\t"
+        "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
+        "ldc1       %[ftmp0],   0x00(%[input])                          \n\t"
+        "mov.d      %[ftmp4],   %[ftmp3]                                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp1]                                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp3]                                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp3]                                \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhwd  %[ftmp2],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "mov.d      %[ftmp0],   %[ftmp4]                                \n\t"
+        "punpcklwd  %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "mov.d      %[ftmp1],   %[ftmp0]                                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "mov.d      %[ftmp1],   %[ftmp2]                                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "mov.d      %[ftmp1],   %[ftmp0]                                \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "mov.d      %[ftmp1],   %[ftmp4]                                \n\t"
+        "daddi      %[tmp0],    %[qmul],        -0x7fff                 \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "bgtz       %[tmp0],    1f                                      \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "ori        %[tmp0],    $0,             0x80                    \n\t"
+        "dsll       %[tmp0],    %[tmp0],        0x10                    \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp0],       %[ff_pw_1]              \n\t"
+        "daddu      %[qmul],    %[qmul],        %[tmp0]                 \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ff_pw_1]              \n\t"
+        "punpckhhw  %[ftmp5],   %[ftmp2],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ff_pw_1]              \n\t"
+        "mtc1       %[qmul],    %[ftmp7]                                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psraw      %[ftmp0],   %[ftmp0],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "packsswh   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "dmfc1      %[tmp1],    %[ftmp0]                                \n\t"
+        "dsrl       %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp0]                                \n\t"
+        "sh         %[tmp1],    0x00(%[output])                         \n\t"
+        "sh         %[input],   0x80(%[output])                         \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x20(%[output])                         \n\t"
+        "sh         %[input],   0xa0(%[output])                         \n\t"
+        "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
+        "dsrl       %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp2]                                \n\t"
+        "sh         %[tmp1],    0x40(%[output])                         \n\t"
+        "sh         %[input],   0xc0(%[output])                         \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x60(%[output])                         \n\t"
+        "sh         %[input],   0xe0(%[output])                         \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp3],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ff_pw_1]              \n\t"
+        "punpckhhw  %[ftmp5],   %[ftmp4],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ff_pw_1]              \n\t"
+        "mtc1       %[qmul],    %[ftmp7]                                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psraw      %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psraw      %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "packsswh   %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "dmfc1      %[tmp1],    %[ftmp3]                                \n\t"
+        "dsrl       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp3]                                \n\t"
+        "sh         %[tmp1],    0x100(%[output])                        \n\t"
+        "sh         %[input],   0x180(%[output])                        \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x120(%[output])                        \n\t"
+        "sh         %[input],   0x1a0(%[output])                        \n\t"
+        "dmfc1      %[tmp1],    %[ftmp4]                                \n\t"
+        "dsrl       %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp4]                                \n\t"
+        "sh         %[tmp1],    0x140(%[output])                        \n\t"
+        "sh         %[input],   0x1c0(%[output])                        \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x160(%[output])                        \n\t"
+        "j          2f                                                  \n\t"
+        "sh         %[input],   0x1e0(%[output])                        \n\t"
+        "1:                                                             \n\t"
+        "ori        %[tmp0],    $0,             0x1f                    \n\t"
+#if HAVE_LOONGSON3
+        "clz        %[tmp1],    %[qmul]                                 \n\t"
+#elif HAVE_LOONGSON2
+#endif
+        "ori        %[input],   $0,             0x07                    \n\t"
+        "dsubu      %[tmp1],    %[tmp0],        %[tmp1]                 \n\t"
+        "ori        %[tmp0],    $0,             0x80                    \n\t"
+        "dsll       %[tmp0],    %[tmp0],        0x10                    \n\t"
+        "daddu      %[qmul],    %[qmul],        %[tmp0]                 \n\t"
+        "dsubu      %[tmp0],    %[tmp1],        %[input]                \n\t"
+        "movn       %[tmp1],    %[input],       %[tmp0]                 \n\t"
+        PTR_ADDIU  "%[input],   %[input],       0x01                    \n\t"
+        "andi       %[tmp0],    %[tmp1],        0xff                    \n\t"
+        "srlv       %[qmul],    %[qmul],        %[tmp0]                 \n\t"
+        PTR_SUBU   "%[input],   %[input],       %[tmp1]                 \n\t"
+        "mtc1       %[input],   %[ftmp6]                                \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp0],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ff_pw_1]              \n\t"
+        "punpckhhw  %[ftmp5],   %[ftmp2],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ff_pw_1]              \n\t"
+        "mtc1       %[qmul],    %[ftmp7]                                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psraw      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "packsswh   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "dmfc1      %[tmp1],    %[ftmp0]                                \n\t"
+        "dsrl       %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "sh         %[tmp1],    0x00(%[output])                         \n\t"
+        "mfc1       %[input],   %[ftmp0]                                \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        "sh         %[input],   0x80(%[output])                         \n\t"
+        "sh         %[tmp1],    0x20(%[output])                         \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
+        "sh         %[input],   0xa0(%[output])                         \n\t"
+        "dsrl       %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
+        "sh         %[tmp1],    0x40(%[output])                         \n\t"
+        "mfc1       %[input],   %[ftmp2]                                \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        "sh         %[input],   0xc0(%[output])                         \n\t"
+        "sh         %[tmp1],    0x60(%[output])                         \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[input],   0xe0(%[output])                         \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp3],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ff_pw_1]              \n\t"
+        "punpckhhw  %[ftmp5],   %[ftmp4],       %[ff_pw_1]              \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ff_pw_1]              \n\t"
+        "mtc1       %[qmul],    %[ftmp7]                                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pmaddhw    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psraw      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psraw      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "packsswh   %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "dmfc1      %[tmp1],    %[ftmp3]                                \n\t"
+        "dsrl       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp3]                                \n\t"
+        "sh         %[tmp1],    0x100(%[output])                        \n\t"
+        "sh         %[input],   0x180(%[output])                        \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x120(%[output])                        \n\t"
+        "sh         %[input],   0x1a0(%[output])                        \n\t"
+        "dmfc1      %[tmp1],    %[ftmp4]                                \n\t"
+        "dsrl       %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "mfc1       %[input],   %[ftmp4]                                \n\t"
+        "sh         %[tmp1],    0x140(%[output])                        \n\t"
+        "sh         %[input],   0x1c0(%[output])                        \n\t"
+        "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
+        PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
+        "sh         %[tmp1],    0x160(%[output])                        \n\t"
+        "sh         %[input],   0x1e0(%[output])                        \n\t"
+        "2:                                                             \n\t"
+        ".set       reorder                                             \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [output]"+&r"(output),            [input]"+&r"(input),
+          [qmul]"+&r"(qmul)
+        : [ff_pw_1]"f"(ff_pw_1)
+        : "memory"
     );
 }
 
@@ -1031,10 +1320,11 @@ void ff_h264_chroma_dc_dequant_idct_8_mmi(int16_t *block, int qmul)
     block[48]= ((d-b)*qmul) >> 7;
 }
 
-void ff_h264_weight_pixels16_8_mmi(uint8_t *block, int stride,
-        int height, int log2_denom, int weight, int offset)
+void ff_h264_weight_pixels16_8_mmi(uint8_t *block, int stride, int height,
+        int log2_denom, int weight, int offset)
 {
     int y;
+    double ftmp[8];
 
     offset <<= log2_denom;
 
@@ -1043,97 +1333,110 @@ void ff_h264_weight_pixels16_8_mmi(uint8_t *block, int stride,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "ldc1 $f2, %0                   \r\n"
-            "ldc1 $f4, %1                   \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "mtc1 %2, $f6                   \r\n"
-            "mtc1 %3, $f8                   \r\n"
-            "mtc1 %4, $f10                  \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "punpckhbh $f14, $f2, $f20      \r\n"
-            "punpckhbh $f16, $f4, $f20      \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "punpcklbh $f4, $f4, $f20       \r\n"
-            "pmullh $f14, $f14, $f6         \r\n"
-            "pmullh $f16, $f16, $f6         \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "pmullh $f4, $f4, $f6           \r\n"
-            "paddsh $f14, $f14, $f8         \r\n"
-            "paddsh $f16, $f16, $f8         \r\n"
-            "paddsh $f2, $f2, $f8           \r\n"
-            "paddsh $f4, $f4, $f8           \r\n"
-            "psrah $f14, $f14, $f10         \r\n"
-            "psrah $f16, $f16, $f10         \r\n"
-            "psrah $f2, $f2, $f10           \r\n"
-            "psrah $f4, $f4, $f10           \r\n"
-            "packushb $f2, $f2, $f14        \r\n"
-            "packushb $f4, $f4, $f16        \r\n"
-            "sdc1 $f2, %0                   \r\n"
-            "sdc1 $f4, %1                   \r\n"
-            : "=m"(*block),"=m"(*(block + 8))
-            : "r"(weight),"r"(offset),"r"(log2_denom)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ldc1       %[ftmp1],   0x00(%[block0])                     \n\t"
+            "ldc1       %[ftmp2],   0x00(%[block1])                     \n\t"
+            "mtc1       %[weight],  %[ftmp3]                            \n\t"
+            "mtc1       %[offset],  %[ftmp4]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp5]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp7],   %[ftmp7],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp2],   %[ftmp2],       %[ftmp5]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "sdc1       %[ftmp1],   0x00(%[block0])                     \n\t"
+            "sdc1       %[ftmp2],   0x00(%[block1])                     \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7])
+            : [block0]"r"(block),           [block1]"r"(block+8),
+              [weight]"r"(weight),          [offset]"r"(offset),
+              [log2_denom]"r"(log2_denom)
+            : "memory"
         );
     }
 }
 
-void ff_h264_biweight_pixels16_8_mmi(uint8_t *dst, uint8_t *src,
-        int stride, int height, int log2_denom, int weightd, int weights,
-        int offset)
+void ff_h264_biweight_pixels16_8_mmi(uint8_t *dst, uint8_t *src, int stride,
+        int height, int log2_denom, int weightd, int weights, int offset)
 {
     int y;
+    double ftmp[9];
 
     offset = ((offset + 1) | 1) << log2_denom;
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "ldc1 $f2, %2                   \r\n"
-            "ldc1 $f4, %3                   \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "mtc1 %6, $f6                   \r\n"
-            "mtc1 %7, $f8                   \r\n"
-            "mtc1 %8, $f10                  \r\n"
-            "mtc1 %9, $f12                  \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "pshufh $f10, $f10, $f20        \r\n"
-            "punpckhbh $f14, $f2, $f20      \r\n"
-            "punpckhbh $f16, $f4, $f20      \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "punpcklbh $f4, $f4, $f20       \r\n"
-            "pmullh $f14, $f14, $f6         \r\n"
-            "pmullh $f16, $f16, $f8         \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "pmullh $f4, $f4, $f8           \r\n"
-            "paddsh $f14, $f14, $f10        \r\n"
-            "paddsh $f2, $f2, $f10          \r\n"
-            "paddsh $f14, $f14, $f16        \r\n"
-            "paddsh $f2, $f2, $f4           \r\n"
-            "psrah $f14, $f14, $f12         \r\n"
-            "psrah $f2, $f2, $f12           \r\n"
-            "packushb $f2, $f2, $f14        \r\n"
-            "sdc1 $f2, %0                   \r\n"
-            "ldc1 $f2, %4                   \r\n"
-            "ldc1 $f4, %5                   \r\n"
-            "punpckhbh $f14, $f2, $f20      \r\n"
-            "punpckhbh $f16, $f4, $f20      \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "punpcklbh $f4, $f4, $f20       \r\n"
-            "pmullh $f14, $f14, $f6         \r\n"
-            "pmullh $f16, $f16, $f8         \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "pmullh $f4, $f4, $f8           \r\n"
-            "paddsh $f14, $f14, $f10        \r\n"
-            "paddsh $f2, $f2, $f10          \r\n"
-            "paddsh $f14, $f14, $f16        \r\n"
-            "paddsh $f2, $f2, $f4           \r\n"
-            "psrah $f14, $f14, $f12         \r\n"
-            "psrah $f2, $f2, $f12           \r\n"
-            "packushb $f2, $f2, $f14        \r\n"
-            "sdc1 $f2, %1                   \r\n"
-            : "=m"(*dst),"=m"(*(dst+8))
-            : "m"(*src),"m"(*dst),"m"(*(src+8)),"m"(*(dst+8)),
-              "r"(weights),"r"(weightd),"r"(offset),"r"(log2_denom+1)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ldc1       %[ftmp1],   0x00(%[src0])                       \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst0])                       \n\t"
+            "mtc1       %[weights], %[ftmp3]                            \n\t"
+            "mtc1       %[weightd], %[ftmp4]                            \n\t"
+            "mtc1       %[offset],  %[ftmp5]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp6]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp7],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp7],   %[ftmp7],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst0])                       \n\t"
+            "ldc1       %[ftmp1],   0x00(%[src1])                       \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst1])                       \n\t"
+            "punpckhbh  %[ftmp7],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp7],   %[ftmp7],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst1])                       \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8])
+            : [dst0]"r"(dst),               [dst1]"r"(dst+8),
+              [src0]"r"(src),               [src1]"r"(src+8),
+              [weights]"r"(weights),        [weightd]"r"(weightd),
+              [offset]"r"(offset),          [log2_denom]"r"(log2_denom+1)
+            : "memory"
         );
     }
 }
@@ -1142,6 +1445,7 @@ void ff_h264_weight_pixels8_8_mmi(uint8_t *block, int stride, int height,
         int log2_denom, int weight, int offset)
 {
     int y;
+    double ftmp[6];
 
     offset <<= log2_denom;
 
@@ -1150,68 +1454,78 @@ void ff_h264_weight_pixels8_8_mmi(uint8_t *block, int stride, int height,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "ldc1 $f2, %0                   \r\n"
-            "mtc1 %1, $f6                   \r\n"
-            "mtc1 %2, $f8                   \r\n"
-            "mtc1 %3, $f10                  \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "punpckhbh $f14, $f2, $f20      \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "pmullh $f14, $f14, $f6         \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "paddsh $f14, $f14, $f8         \r\n"
-            "paddsh $f2, $f2, $f8           \r\n"
-            "psrah $f14, $f14, $f10         \r\n"
-            "psrah $f2, $f2, $f10           \r\n"
-            "packushb $f2, $f2, $f14        \r\n"
-            "sdc1 $f2, %0                   \r\n"
-            : "=m"(*block)
-            : "r"(weight),"r"(offset),"r"(log2_denom)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ldc1       %[ftmp1],   0x00(%[block])                      \n\t"
+            "mtc1       %[weight],  %[ftmp2]                            \n\t"
+            "mtc1       %[offset],  %[ftmp3]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp5]            \n\t"
+            "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp4],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "sdc1       %[ftmp1],   0x00(%[block])                      \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5])
+            : [block]"r"(block),            [weight]"r"(weight),
+              [offset]"r"(offset),          [log2_denom]"r"(log2_denom)
+            : "memory"
         );
     }
 }
 
-void ff_h264_biweight_pixels8_8_mmi(uint8_t *dst, uint8_t *src,
-        int stride, int height, int log2_denom, int weightd, int weights,
-        int offset)
+void ff_h264_biweight_pixels8_8_mmi(uint8_t *dst, uint8_t *src, int stride,
+        int height, int log2_denom, int weightd, int weights, int offset)
 {
     int y;
+    double ftmp[9];
 
     offset = ((offset + 1) | 1) << log2_denom;
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "ldc1 $f2, %1                   \r\n"
-            "ldc1 $f4, %2                   \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "mtc1 %3, $f6                   \r\n"
-            "mtc1 %4, $f8                   \r\n"
-            "mtc1 %5, $f10                  \r\n"
-            "mtc1 %6, $f12                  \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "pshufh $f10, $f10, $f20        \r\n"
-            "punpckhbh $f14, $f2, $f20      \r\n"
-            "punpckhbh $f16, $f4, $f20      \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "punpcklbh $f4, $f4, $f20       \r\n"
-            "pmullh $f14, $f14, $f6         \r\n"
-            "pmullh $f16, $f16, $f8         \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "pmullh $f4, $f4, $f8           \r\n"
-            "paddsh $f14, $f14, $f10        \r\n"
-            "paddsh $f2, $f2, $f10          \r\n"
-            "paddsh $f14, $f14, $f16        \r\n"
-            "paddsh $f2, $f2, $f4           \r\n"
-            "psrah $f14, $f14, $f12         \r\n"
-            "psrah $f2, $f2, $f12           \r\n"
-            "packushb $f2, $f2, $f14        \r\n"
-            "sdc1 $f2, %0                   \r\n"
-            : "=m"(*dst)
-            : "m"(*src),"m"(*dst),"r"(weights),
-              "r"(weightd),"r"(offset),"r"(log2_denom+1)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ldc1       %[ftmp1],   0x00(%[src])                        \n\t"
+            "ldc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "mtc1       %[weights], %[ftmp3]                            \n\t"
+            "mtc1       %[weightd], %[ftmp4]                            \n\t"
+            "mtc1       %[offset],  %[ftmp5]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp6]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp7],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp7],   %[ftmp7],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "sdc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8])
+            : [dst]"r"(dst),                [src]"r"(src),
+              [weights]"r"(weights),        [weightd]"r"(weightd),
+              [offset]"r"(offset),          [log2_denom]"r"(log2_denom+1)
+            : "memory"
         );
     }
 }
@@ -1220,6 +1534,8 @@ void ff_h264_weight_pixels4_8_mmi(uint8_t *block, int stride, int height,
         int log2_denom, int weight, int offset)
 {
     int y;
+    double ftmp[5];
+    int low32;
 
     offset <<= log2_denom;
 
@@ -1228,745 +1544,1100 @@ void ff_h264_weight_pixels4_8_mmi(uint8_t *block, int stride, int height,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "lwc1 $f2, %0                   \r\n"
-            "mtc1 %1, $f6                   \r\n"
-            "mtc1 %2, $f8                   \r\n"
-            "mtc1 %3, $f10                  \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "paddsh $f2, $f2, $f8           \r\n"
-            "psrah $f2, $f2, $f10           \r\n"
-            "packushb $f2, $f2, $f20        \r\n"
-            "swc1 $f2, %0                   \r\n"
-            : "=m"(*block)
-            : "r"(weight),"r"(offset),"r"(log2_denom)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "uld        %[low32],   0x00(%[block])                      \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[weight],  %[ftmp2]                            \n\t"
+            "mtc1       %[offset],  %[ftmp3]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp4]            \n\t"
+            "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[block])                      \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[block])                      \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[block])                      \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [low32]"=&r"(low32)
+            : [block]"r"(block),            [weight]"r"(weight),
+              [offset]"r"(offset),          [log2_denom]"r"(log2_denom)
+            : "memory"
         );
     }
 }
 
-void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
-        int stride, int height, int log2_denom, int weightd, int weights,
-        int offset)
+void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src, int stride,
+        int height, int log2_denom, int weightd, int weights, int offset)
 {
     int y;
+    double ftmp[7];
+    int low32;
 
     offset = ((offset + 1) | 1) << log2_denom;
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "lwc1 $f2, %1                   \r\n"
-            "lwc1 $f4, %2                   \r\n"
-            "dmtc1 $0, $f20                 \r\n"
-            "mtc1 %3, $f6                   \r\n"
-            "mtc1 %4, $f8                   \r\n"
-            "mtc1 %5, $f10                  \r\n"
-            "mtc1 %6, $f12                  \r\n"
-            "pshufh $f6, $f6, $f20          \r\n"
-            "pshufh $f8, $f8, $f20          \r\n"
-            "pshufh $f10, $f10, $f20        \r\n"
-            "punpcklbh $f2, $f2, $f20       \r\n"
-            "punpcklbh $f4, $f4, $f20       \r\n"
-            "pmullh $f2, $f2, $f6           \r\n"
-            "pmullh $f4, $f4, $f8           \r\n"
-            "paddsh $f2, $f2, $f10          \r\n"
-            "paddsh $f2, $f2, $f4           \r\n"
-            "psrah $f2, $f2, $f12           \r\n"
-            "packushb $f2, $f2, $f20        \r\n"
-            "swc1 $f2, %0                   \r\n"
-            : "=m"(*dst)
-            : "m"(*src),"m"(*dst),"r"(weights),
-              "r"(weightd),"r"(offset),"r"(log2_denom+1)
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "uld        %[low32],   0x00(%[dst])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "mtc1       %[weight],  %[ftmp3]                            \n\t"
+            "mtc1       %[weightd], %[ftmp4]                            \n\t"
+            "mtc1       %[offset],  %[ftmp5]                            \n\t"
+            "mtc1       %[log2_denom],              %[ftmp6]            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                [src]"r"(src),
+              [weight]"r"(weights),         [weightd]"r"(weightd),
+              [offset]"r"(offset),          [log2_denom]"r"(log2_denom+1)
+            : "memory"
         );
     }
 }
 
-static void inline chroma_inter_body_mmi(uint8_t *pix, int stride,
-        int alpha, int beta, int8_t *tc0)
-{
-    __asm__ volatile (
-        "xor $f16, $f16, $f16                           \r\n"
-        "mtc1 %[alpha], $f8                             \r\n"
-        "mtc1 %[beta], $f10                             \r\n"
-        "pshufh $f8, $f8, $f16                          \r\n"
-        "pshufh $f10, $f10, $f16                        \r\n"
-        "packushb $f8, $f8, $f8                         \r\n"
-        "packushb $f10, $f10, $f10                      \r\n"
-        "psubusb $f12, $f4, $f2                         \r\n"
-        "psubusb $f14, $f2, $f4                         \r\n"
-        "or $f14, $f14, $f12                            \r\n"
-        "psubusb $f14, $f14, $f8                        \r\n"
-        "psubusb $f12, $f2, $f0                         \r\n"
-        "psubusb $f8, $f0, $f2                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "psubusb $f12, $f4, $f6                         \r\n"
-        "psubusb $f8, $f6, $f4                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "xor $f12, $f12, $f12                           \r\n"
-        "pcmpeqb $f14, $f14, $f12                       \r\n"
-        "lwc1 $f12, 0x0(%[tc0])                         \r\n"
-        "punpcklbh $f12, $f12, $f12                     \r\n"
-        "and $f14, $f14, $f12                           \r\n"
-        "pcmpeqb $f8, $f8, $f8                          \r\n"
-        "xor $f10, $f2, $f4                             \r\n"
-        "xor $f6, $f6, $f8                              \r\n"
-        "and $f10, $f10, %[ff_pb_1]                     \r\n"
-        "pavgb $f6, $f6, $f0                            \r\n"
-        "xor $f8, $f8, $f2                              \r\n"
-        "pavgb $f6, $f6, %[ff_pb_3]                     \r\n"
-        "pavgb $f8, $f8, $f4                            \r\n"
-        "pavgb $f6, $f6, $f10                           \r\n"
-        "paddusb $f6, $f6, $f8                          \r\n"
-        "psubusb $f12, %[ff_pb_A1], $f6                 \r\n"
-        "psubusb $f6, $f6, %[ff_pb_A1]                  \r\n"
-        "pminub $f12, $f12, $f14                        \r\n"
-        "pminub $f6, $f6, $f14                          \r\n"
-        "psubusb $f2, $f2, $f12                         \r\n"
-        "psubusb $f4, $f4, $f6                          \r\n"
-        "paddusb $f2, $f2, $f6                          \r\n"
-        "paddusb $f4, $f4, $f12                         \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),
-          [alpha]"r"((int64_t)alpha),[beta]"r"((int64_t)beta),[tc0]"r"(tc0),
-          [ff_pb_1]"f"(ff_pb_1),[ff_pb_3]"f"(ff_pb_3),[ff_pb_A1]"f"(ff_pb_A1)
-        : "$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16"
-    );
-}
-
-static void inline chroma_intra_body_mmi(uint8_t *pix, int stride,
-        int alpha, int beta)
-{
-    __asm__ volatile (
-        "xor $f16, $f16, $f16                           \r\n"
-        "mtc1 %[alpha], $f8                             \r\n"
-        "mtc1 %[beta], $f10                             \r\n"
-        "pshufh $f8, $f8, $f16                          \r\n"
-        "pshufh $f10, $f10, $f16                        \r\n"
-        "packushb $f8, $f8, $f8                         \r\n"
-        "packushb $f10, $f10, $f10                      \r\n"
-        "psubusb $f12, $f4, $f2                         \r\n"
-        "psubusb $f14, $f2, $f4                         \r\n"
-        "or $f14, $f14, $f12                            \r\n"
-        "psubusb $f14, $f14, $f8                        \r\n"
-        "psubusb $f12, $f2, $f0                         \r\n"
-        "psubusb $f8, $f0, $f2                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "psubusb $f12, $f4, $f6                         \r\n"
-        "psubusb $f8, $f6, $f4                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "xor $f12, $f12, $f12                           \r\n"
-        "pcmpeqb $f14, $f14, $f12                       \r\n"
-        "mov.d $f10, $f2                                \r\n"
-        "mov.d $f12, $f4                                \r\n"
-        "xor $f8, $f2, $f6                              \r\n"
-        "and $f8, $f8, %[ff_pb_1]                       \r\n"
-        "pavgb $f2, $f2, $f6                            \r\n"
-        "psubusb $f2, $f2, $f8                          \r\n"
-        "pavgb $f2, $f2, $f0                            \r\n"
-        "xor $f8, $f4, $f0                              \r\n"
-        "and $f8, $f8, %[ff_pb_1]                       \r\n"
-        "pavgb $f4, $f4, $f0                            \r\n"
-        "psubusb $f4, $f4, $f8                          \r\n"
-        "pavgb $f4, $f4, $f6                            \r\n"
-        "psubb $f2, $f2, $f10                           \r\n"
-        "psubb $f4, $f4, $f12                           \r\n"
-        "and $f2, $f2, $f14                             \r\n"
-        "and $f4, $f4, $f14                             \r\n"
-        "paddb $f2, $f2, $f10                           \r\n"
-        "paddb $f4, $f4, $f12                           \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),
-          [alpha]"r"((int64_t)alpha),[beta]"r"((int64_t)beta),
-          [ff_pb_1]"f"(ff_pb_1)
-        : "$f0","$f2","$f4","$f8","$f10","$f12","$f14","$f16"
-    );
-}
-
 void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         int8_t *tc0)
 {
+    double ftmp[12];
+    mips_reg addr[3];
+    int low32;
+    uint64_t all64;
+
     __asm__ volatile (
-        "daddu $8, %[stride], %[stride]                 \r\n"
-        "xor $f16, $f16, $f16                           \r\n"
-        "daddu $9, %[stride], $8                        \r\n"
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "dsubu $9, $0, $9                               \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "daddu $9, $9, %[pix]                           \r\n"
-        "ldc1 $f4, 0x0(%[pix])                          \r\n"
-        "gsldxc1 $f0, 0x0($9, %[stride])                \r\n"
-        "gsldxc1 $f2, 0x0($9, $8)                       \r\n"
-        "gsldxc1 $f6, 0x0(%[pix], %[stride])            \r\n"
-        "mtc1 %[alpha], $f8                             \r\n"
-        "mtc1 %[beta], $f10                             \r\n"
-        "pshufh $f8, $f8, $f16                          \r\n"
-        "pshufh $f10, $f10, $f16                        \r\n"
-        "packushb $f8, $f8, $f8                         \r\n"
-        "packushb $f10, $f10, $f10                      \r\n"
-        "psubusb $f12, $f4, $f2                         \r\n"
-        "psubusb $f14, $f2, $f4                         \r\n"
-        "or $f14, $f14, $f12                            \r\n"
-        "psubusb $f12, $f2, $f0                         \r\n"
-        "psubusb $f14, $f14, $f8                        \r\n"
-        "psubusb $f8, $f0, $f2                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f12, $f4, $f6                         \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "psubusb $f8, $f6, $f4                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "pcmpeqb $f14, $f14, $f16                       \r\n"
-        "pcmpeqb $f6, $f6, $f6                          \r\n"
-        "gslwlc1 $f8, 0x3(%[tc0])                       \r\n"
-        "gslwrc1 $f8, 0x0(%[tc0])                       \r\n"
-        "punpcklbh $f8, $f8, $f8                        \r\n"
-        "punpcklbh $f18, $f8, $f8                       \r\n"
-        "pcmpgtb $f8, $f18, $f6                         \r\n"
-        "ldc1 $f6, 0x0($9)                              \r\n"
-        "and $f20, $f8, $f14                            \r\n"
-        "psubusb $f14, $f6, $f2                         \r\n"
-        "psubusb $f12, $f2, $f6                         \r\n"
-        "psubusb $f14, $f14, $f10                       \r\n"
-        "psubusb $f12, $f12, $f10                       \r\n"
-        "pcmpeqb $f12, $f12, $f14                       \r\n"
-        "and $f12, $f12, $f20                           \r\n"
-        "and $f8, $f20, $f18                            \r\n"
-        "psubb $f14, $f8, $f12                          \r\n"
-        "and $f12, $f12, $f8                            \r\n"
-        "pavgb $f8, $f2, $f4                            \r\n"
-        "ldc1 $f22, 0x0($9)                             \r\n"
-        "pavgb $f6, $f6, $f8                            \r\n"
-        "xor $f8, $f8, $f22                             \r\n"
-        "and $f8, $f8, %[ff_pb_1]                       \r\n"
-        "psubusb $f6, $f6, $f8                          \r\n"
-        "psubusb $f8, $f0, $f12                         \r\n"
-        "paddusb $f12, $f12, $f0                        \r\n"
-        "pmaxub $f6, $f6, $f8                           \r\n"
-        "pminub $f6, $f6, $f12                          \r\n"
-        "gssdxc1 $f6, 0x0($9, %[stride])                \r\n"
-        "gsldxc1 $f8, 0x0(%[pix], $8)                   \r\n"
-        "psubusb $f6, $f8, $f4                          \r\n"
-        "psubusb $f12, $f4, $f8                         \r\n"
-        "psubusb $f6, $f6, $f10                         \r\n"
-        "psubusb $f12, $f12, $f10                       \r\n"
-        "pcmpeqb $f12, $f12, $f6                        \r\n"
-        "and $f12, $f12, $f20                           \r\n"
-        "psubb $f14, $f14, $f12                         \r\n"
-        "and $f10, $f18, $f12                           \r\n"
-        "gsldxc1 $f6, 0x0(%[pix], %[stride])            \r\n"
-        "pavgb $f12, $f2, $f4                           \r\n"
-        "gsldxc1 $f22, 0x0(%[pix], $8)                  \r\n"
-        "pavgb $f8, $f8, $f12                           \r\n"
-        "xor $f12, $f12, $f22                           \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "psubusb $f8, $f8, $f12                         \r\n"
-        "psubusb $f12, $f6, $f10                        \r\n"
-        "paddusb $f10, $f10, $f6                        \r\n"
-        "pmaxub $f8, $f8, $f12                          \r\n"
-        "pminub $f8, $f8, $f10                          \r\n"
-        "gssdxc1 $f8, 0x0(%[pix], %[stride])            \r\n"
-        "xor $f10, $f2, $f4                             \r\n"
-        "pcmpeqb $f8, $f8, $f8                          \r\n"
-        "and $f10, $f10, %[ff_pb_1]                     \r\n"
-        "xor $f6, $f6, $f8                              \r\n"
-        "xor $f8, $f8, $f2                              \r\n"
-        "pavgb $f6, $f6, $f0                            \r\n"
-        "pavgb $f6, $f6, %[ff_pb_3]                     \r\n"
-        "pavgb $f8, $f8, $f4                            \r\n"
-        "pavgb $f6, $f6, $f10                           \r\n"
-        "paddusb $f6, $f6, $f8                          \r\n"
-        "psubusb $f12, %[ff_pb_A1], $f6                 \r\n"
-        "psubusb $f6, $f6, %[ff_pb_A1]                  \r\n"
-        "pminub $f12, $f12, $f14                        \r\n"
-        "pminub $f6, $f6, $f14                          \r\n"
-        "psubusb $f2, $f2, $f12                         \r\n"
-        "psubusb $f4, $f4, $f6                          \r\n"
-        "paddusb $f2, $f2, $f6                          \r\n"
-        "paddusb $f4, $f4, $f12                         \r\n"
-        "gssdxc1 $f2, 0x0($9, $8)                       \r\n"
-        "sdc1 $f4, 0x0(%[pix])                          \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),
-          [alpha]"r"((int64_t)alpha),[beta]"r"((int64_t)beta),[tc0]"r"(tc0),
-          [ff_pb_1]"f"(ff_pb_1),[ff_pb_3]"f"(ff_pb_3),[ff_pb_A1]"f"(ff_pb_A1)
-        : "$8","$9","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20","$f22"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        PTR_ADDU   "%[addr1],   %[stride],      %[addr0]                \n\t"
+        "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
+        PTR_SUBU   "%[addr1],   $0,             %[addr1]                \n\t"
+        "addi       %[beta],    %[beta],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[pix]                  \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp1],   0x00(%[addr1],  %[stride])              \n\t"
+        "gsldxc1    %[ftmp2],   0x00(%[addr1],  %[addr0])               \n\t"
+        "gsldxc1    %[ftmp4],   0x00(%[pix],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr2],   %[addr1],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr1],       %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[pix],         %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        "mtc1       %[alpha],   %[ftmp5]                                \n\t"
+        "mtc1       %[beta],    %[ftmp6]                                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
+        "pcmpeqb    %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "uld        %[low32],   0x00(%[tc0])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp5],       %[ftmp5]                \n\t"
+        "pcmpgtb    %[ftmp5],   %[ftmp9],       %[ftmp4]                \n\t"
+        "ldc1       %[ftmp4],   0x00(%[addr1])                          \n\t"
+        "and        %[ftmp10],  %[ftmp5],       %[ftmp8]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp4],       %[ftmp2]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "and        %[ftmp5],   %[ftmp10],      %[ftmp9]                \n\t"
+        "psubb      %[ftmp8],   %[ftmp5],       %[ftmp7]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp2],       %[ftmp3]                \n\t"
+        "ldc1       %[ftmp11],  0x00(%[addr1])                          \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp11]               \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp7]                \n\t"
+        "paddusb    %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "pmaxub     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "pminub     %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp4],   0x00(%[addr1],  %[stride])              \n\t"
+        "gsldxc1    %[ftmp5],   0x00(%[pix],    %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr2],   %[addr1],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[pix],         %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "psubusb    %[ftmp4],   %[ftmp5],       %[ftmp3]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "and        %[ftmp6],   %[ftmp9],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp4],   0x00(%[pix],    %[stride])              \n\t"
+        "pavgb      %[ftmp7],   %[ftmp2],       %[ftmp3]                \n\t"
+        "gsldxc1    %[ftmp11],  0x00(%[pix],    %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr2],   %[pix],         %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "pavgb      %[ftmp7],   %[ftmp2],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr2],   %[pix],         %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr2])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp11]                               \n\t"
+#endif
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ff_pb_1]              \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddusb    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "pmaxub     %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pminub     %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp5],   0x00(%[pix],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr2],   %[pix],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        "xor        %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ff_pb_3]              \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ff_pb_A1],    %[ftmp4]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ff_pb_A1]             \n\t"
+        "pminub     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "pminub     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddusb    %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddusb    %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp2],   0x00(%[addr1],  %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr2],   %[addr1],       %[addr0]                \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        "sdc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),
+          [low32]"=&r"(low32),              [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [alpha]"r"((mips_reg)alpha),      [beta]"r"((mips_reg)beta),
+          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1),
+          [ff_pb_3]"f"(ff_pb_3),            [ff_pb_A1]"f"(ff_pb_A1)
+        : "memory"
     );
 }
 
-void ff_deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         int beta)
 {
-    uint64_t stack[0xa];
+    DECLARE_ALIGNED(8, const uint64_t, stack[0x0a]);
+    double ftmp[16];
+    uint64_t tmp[1];
+    mips_reg addr[4];
+    uint64_t all64;
 
     __asm__ volatile (
-        "ori $8, $0, 0x1                                \r\n"
-        "xor $f30, $f30, $f30                           \r\n"
-        "dmtc1 $8, $f16                                 \r\n"
-        "dsll $8, %[stride], 2                          \r\n"
-        "daddu $10, %[stride], %[stride]                \r\n"
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "dsll $f20, $f16, $f16                          \r\n"
-        "bltz %[alpha], 1f                              \r\n"
-        "daddu $9, $10, %[stride]                       \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "bltz %[beta], 1f                               \r\n"
-        "dsubu $8, $0, $8                               \r\n"
-        "daddu $8, $8, %[pix]                           \r\n"
-        "ldc1 $f4, 0x0(%[pix])                          \r\n"
-        "gsldxc1 $f0, 0x0($8, $10)                      \r\n"
-        "gsldxc1 $f2, 0x0($8, $9)                       \r\n"
-        "gsldxc1 $f6, 0x0(%[pix], %[stride])            \r\n"
-        "mtc1 %[alpha], $f8                             \r\n"
-        "mtc1 %[beta], $f10                             \r\n"
-        "pshufh $f8, $f8, $f30                          \r\n"
-        "pshufh $f10, $f10, $f30                        \r\n"
-        "packushb $f8, $f8, $f8                         \r\n"
-        "psubusb $f12, $f4, $f2                         \r\n"
-        "psubusb $f14, $f2, $f4                         \r\n"
-        "packushb $f10, $f10, $f10                      \r\n"
-        "or $f14, $f14, $f12                            \r\n"
-        "sdc1 $f8, 0x10+%[stack]                        \r\n"
-        "psubusb $f14, $f14, $f8                        \r\n"
-        "psubusb $f12, $f2, $f0                         \r\n"
-        "psubusb $f8, $f0, $f2                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "psubusb $f12, $f4, $f6                         \r\n"
-        "psubusb $f8, $f6, $f4                          \r\n"
-        "or $f8, $f8, $f12                              \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "or $f14, $f14, $f8                             \r\n"
-        "xor $f12, $f12, $f12                           \r\n"
-        "ldc1 $f8, 0x10+%[stack]                        \r\n"
-        "pcmpeqb $f14, $f14, $f12                       \r\n"
-        "sdc1 $f14, 0x20+%[stack]                       \r\n"
-        "pavgb $f8, $f8, $f30                           \r\n"
-        "psubusb $f14, $f4, $f2                         \r\n"
-        "pavgb $f8, $f8, %[ff_pb_1]                     \r\n"
-        "psubusb $f12, $f2, $f4                         \r\n"
-        "psubusb $f14, $f14, $f8                        \r\n"
-        "psubusb $f12, $f12, $f8                        \r\n"
-        "ldc1 $f28, 0x20+%[stack]                       \r\n"
-        "pcmpeqb $f12, $f12, $f14                       \r\n"
-        "and $f12, $f12, $f28                           \r\n"
-        "gsldxc1 $f28, 0x0($8, %[stride])               \r\n"
-        "psubusb $f14, $f28, $f2                        \r\n"
-        "psubusb $f8, $f2, $f28                         \r\n"
-        "psubusb $f14, $f14, $f10                       \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "pcmpeqb $f8, $f8, $f14                         \r\n"
-        "and $f8, $f8, $f12                             \r\n"
-        "gsldxc1 $f26, 0x0(%[pix], $10)                 \r\n"
-        "sdc1 $f8, 0x30+%[stack]                        \r\n"
-        "psubusb $f14, $f26, $f4                        \r\n"
-        "psubusb $f8, $f4, $f26                         \r\n"
-        "psubusb $f14, $f14, $f10                       \r\n"
-        "psubusb $f8, $f8, $f10                         \r\n"
-        "pcmpeqb $f8, $f8, $f14                         \r\n"
-        "and $f8, $f8, $f12                             \r\n"
-        "sdc1 $f8, 0x40+%[stack]                        \r\n"
-        "pavgb $f8, $f28, $f0                           \r\n"
-        "pavgb $f10, $f2, $f4                           \r\n"
-        "pavgb $f8, $f8, $f10                           \r\n"
-        "sdc1 $f10, 0x10+%[stack]                       \r\n"
-        "paddb $f12, $f28, $f0                          \r\n"
-        "paddb $f14, $f2, $f4                           \r\n"
-        "paddb $f12, $f12, $f14                         \r\n"
-        "mov.d $f14, $f12                               \r\n"
-        "sdc1 $f12, 0x0+%[stack]                        \r\n"
-        "psrlh $f12, $f12, $f16                         \r\n"
-        "pavgb $f12, $f12, $f30                         \r\n"
-        "xor $f12, $f12, $f8                            \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "psubb $f8, $f8, $f12                           \r\n"
-        "pavgb $f10, $f28, $f6                          \r\n"
-        "psubb $f12, $f28, $f6                          \r\n"
-        "paddb $f14, $f14, $f14                         \r\n"
-        "psubb $f14, $f14, $f12                         \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "psubb $f10, $f10, $f12                         \r\n"
-        "ldc1 $f24, 0x10+%[stack]                       \r\n"
-        "pavgb $f10, $f10, $f0                          \r\n"
-        "psrlh $f14, $f14, $f20                         \r\n"
-        "pavgb $f10, $f10, $f24                         \r\n"
-        "pavgb $f14, $f14, $f30                         \r\n"
-        "xor $f14, $f14, $f10                           \r\n"
-        "and $f14, $f14, %[ff_pb_1]                     \r\n"
-        "psubb $f10, $f10, $f14                         \r\n"
-        "xor $f14, $f2, $f6                             \r\n"
-        "pavgb $f12, $f2, $f6                           \r\n"
-        "and $f14, $f14, %[ff_pb_1]                     \r\n"
-        "psubb $f12, $f12, $f14                         \r\n"
-        "ldc1 $f24, 0x30+%[stack]                       \r\n"
-        "pavgb $f12, $f12, $f0                          \r\n"
-        "ldc1 $f22, 0x20+%[stack]                       \r\n"
-        "xor $f10, $f10, $f12                           \r\n"
-        "xor $f12, $f12, $f2                            \r\n"
-        "and $f10, $f10, $f24                           \r\n"
-        "and $f12, $f12, $f22                           \r\n"
-        "xor $f10, $f10, $f12                           \r\n"
-        "xor $f10, $f10, $f2                            \r\n"
-        "gssdxc1 $f10, 0x0($8, $9)                      \r\n"
-        "ldc1 $f10, 0x0($8)                             \r\n"
-        "paddb $f12, $f28, $f10                         \r\n"
-        "pavgb $f10, $f10, $f28                         \r\n"
-        "ldc1 $f22, 0x0+%[stack]                        \r\n"
-        "pavgb $f10, $f10, $f8                          \r\n"
-        "paddb $f12, $f12, $f12                         \r\n"
-        "paddb $f12, $f12, $f22                         \r\n"
-        "psrlh $f12, $f12, $f20                         \r\n"
-        "pavgb $f12, $f12, $f30                         \r\n"
-        "xor $f12, $f12, $f10                           \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "ldc1 $f22, 0x30+%[stack]                       \r\n"
-        "psubb $f10, $f10, $f12                         \r\n"
-        "xor $f8, $f8, $f0                              \r\n"
-        "xor $f10, $f10, $f28                           \r\n"
-        "and $f8, $f8, $f22                             \r\n"
-        "and $f10, $f10, $f22                           \r\n"
-        "xor $f8, $f8, $f0                              \r\n"
-        "xor $f10, $f10, $f28                           \r\n"
-        "gssdxc1 $f8, 0x0($8, $10)                      \r\n"
-        "gssdxc1 $f10, 0x0($8, %[stride])               \r\n"
-        "pavgb $f8, $f26, $f6                           \r\n"
-        "pavgb $f10, $f4, $f2                           \r\n"
-        "pavgb $f8, $f8, $f10                           \r\n"
-        "sdc1 $f10, 0x10+%[stack]                       \r\n"
-        "paddb $f12, $f26, $f6                          \r\n"
-        "paddb $f14, $f4, $f2                           \r\n"
-        "paddb $f12, $f12, $f14                         \r\n"
-        "mov.d $f14, $f12                               \r\n"
-        "sdc1 $f12, 0x0+%[stack]                        \r\n"
-        "psrlh $f12, $f12, $f16                         \r\n"
-        "pavgb $f12, $f12, $f30                         \r\n"
-        "xor $f12, $f12, $f8                            \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "psubb $f8, $f8, $f12                           \r\n"
-        "pavgb $f10, $f26, $f0                          \r\n"
-        "paddb $f14, $f14, $f14                         \r\n"
-        "psubb $f12, $f26, $f0                          \r\n"
-        "psubb $f14, $f14, $f12                         \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "psubb $f10, $f10, $f12                         \r\n"
-        "ldc1 $f22, 0x10+%[stack]                       \r\n"
-        "pavgb $f10, $f10, $f6                          \r\n"
-        "pavgb $f10, $f10, $f22                         \r\n"
-        "psrlh $f14, $f14, $f20                         \r\n"
-        "pavgb $f14, $f14, $f30                         \r\n"
-        "xor $f14, $f14, $f10                           \r\n"
-        "and $f14, $f14, %[ff_pb_1]                     \r\n"
-        "psubb $f10, $f10, $f14                         \r\n"
-        "xor $f14, $f4, $f0                             \r\n"
-        "pavgb $f12, $f4, $f0                           \r\n"
-        "and $f14, $f14, %[ff_pb_1]                     \r\n"
-        "ldc1 $f22, 0x40+%[stack]                       \r\n"
-        "psubb $f12, $f12, $f14                         \r\n"
-        "ldc1 $f24, 0x20+%[stack]                       \r\n"
-        "pavgb $f12, $f12, $f6                          \r\n"
-        "xor $f10, $f10, $f12                           \r\n"
-        "xor $f12, $f12, $f4                            \r\n"
-        "and $f10, $f10, $f22                           \r\n"
-        "and $f12, $f12, $f24                           \r\n"
-        "xor $f10, $f10, $f12                           \r\n"
-        "xor $f10, $f10, $f4                            \r\n"
-        "sdc1 $f10, 0x0(%[pix])                         \r\n"
-        "gsldxc1 $f10, 0x0(%[pix], $9)                  \r\n"
-        "paddb $f12, $f26, $f10                         \r\n"
-        "pavgb $f10, $f10, $f26                         \r\n"
-        "ldc1 $f22, 0x0+%[stack]                        \r\n"
-        "pavgb $f10, $f10, $f8                          \r\n"
-        "paddb $f12, $f12, $f12                         \r\n"
-        "paddb $f12, $f12, $f22                         \r\n"
-        "psrlh $f12, $f12, $f20                         \r\n"
-        "pavgb $f12, $f12, $f30                         \r\n"
-        "xor $f12, $f12, $f10                           \r\n"
-        "and $f12, $f12, %[ff_pb_1]                     \r\n"
-        "ldc1 $f22, 0x40+%[stack]                       \r\n"
-        "psubb $f10, $f10, $f12                         \r\n"
-        "xor $f8, $f8, $f6                              \r\n"
-        "xor $f10, $f10, $f26                           \r\n"
-        "and $f8, $f8, $f22                             \r\n"
-        "and $f10, $f10, $f22                           \r\n"
-        "xor $f8, $f8, $f6                              \r\n"
-        "xor $f10, $f10, $f26                           \r\n"
-        "gssdxc1 $f8, 0x0(%[pix], %[stride])            \r\n"
-        "gssdxc1 $f10, 0x0(%[pix], $10)                 \r\n"
-        "1:                                             \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),
-          [alpha]"r"((int64_t)alpha),[beta]"r"((int64_t)beta),
-          [stack]"m"(stack[0]),[ff_pb_1]"f"(ff_pb_1)
-        : "$8","$9","$10","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14",
-          "$f16","$f18","$f20","$f22","$f24","$f26","$f28","$f30"
+        "ori        %[tmp0],    $0,             0x01                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
+        PTR_SLL    "%[addr0],   %[stride],      0x02                    \n\t"
+        PTR_ADDU   "%[addr2],   %[stride],      %[stride]               \n\t"
+        PTR_ADDIU  "%[alpha],   %[alpha],       -0x01                   \n\t"
+        PTR_SLL    "%[ftmp11],  %[ftmp9],       %[ftmp9]                \n\t"
+        "bltz       %[alpha],   1f                                      \n\t"
+        PTR_ADDU   "%[addr1],   %[addr2],       %[stride]               \n\t"
+        PTR_ADDIU  "%[beta],    %[beta],        -0x01                   \n\t"
+        "bltz       %[beta],    1f                                      \n\t"
+        PTR_SUBU   "%[addr0],   $0,             %[addr0]                \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[pix]                  \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp1],   0x00(%[addr0],  %[addr2])               \n\t"
+        "gsldxc1    %[ftmp2],   0x00(%[addr0],  %[addr1])               \n\t"
+        "gsldxc1    %[ftmp4],   0x00(%[pix],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[addr0],       %[addr2]                \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr0],       %[addr1]                \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        "mtc1       %[alpha],   %[ftmp5]                                \n\t"
+        "mtc1       %[beta],    %[ftmp6]                                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp5],   0x10+%[stack]                           \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp5],   0x10+%[stack]                           \n\t"
+        "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp10],  %[ff_pb_1]                              \n\t"
+        "sdc1       %[ftmp8],   0x20+%[stack]                           \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp3],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp10]               \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "ldc1       %[ftmp15],  0x20+%[stack]                           \n\t"
+        "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp15]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp15],  0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[addr0],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp15]                               \n\t"
+#endif
+        "psubusb    %[ftmp8],   %[ftmp15],      %[ftmp2]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp2],       %[ftmp15]               \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp14],  0x00(%[pix],    %[addr2])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr2]                \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp14]                               \n\t"
+#endif
+        "sdc1       %[ftmp5],   0x30+%[stack]                           \n\t"
+        "psubusb    %[ftmp8],   %[ftmp14],      %[ftmp3]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp3],       %[ftmp14]               \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp5],   0x40+%[stack]                           \n\t"
+        "pavgb      %[ftmp5],   %[ftmp15],      %[ftmp1]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "sdc1       %[ftmp6],   0x10+%[stack]                           \n\t"
+        "paddb      %[ftmp7],   %[ftmp15],      %[ftmp1]                \n\t"
+        "paddb      %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "mov.d      %[ftmp8],   %[ftmp7]                                \n\t"
+        "sdc1       %[ftmp7],   0x00+%[stack]                           \n\t"
+        "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp15],      %[ftmp4]                \n\t"
+        "psubb      %[ftmp7],   %[ftmp15],      %[ftmp4]                \n\t"
+        "paddb      %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp13],  0x10+%[stack]                           \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp11]               \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp13]               \n\t"
+        "pavgb      %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "xor        %[ftmp8],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp7],   %[ftmp2],       %[ftmp4]                \n\t"
+        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "ldc1       %[ftmp13],  0x30+%[stack]                           \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "ldc1       %[ftmp12],  0x20+%[stack]                           \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ftmp13]               \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp6],   0x00(%[addr0],  %[addr1])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[addr0],       %[addr1]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+#endif
+        "ldc1       %[ftmp6],   0x00(%[addr0])                          \n\t"
+        "paddb      %[ftmp7],   %[ftmp15],      %[ftmp6]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
+        "ldc1       %[ftmp12],  0x00+%[stack]                           \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
+        "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "ldc1       %[ftmp12],  0x30+%[stack]                           \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp5],   0x00(%[addr0],  %[addr2])               \n\t"
+        "gssdxc1    %[ftmp6],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[addr0],       %[addr2]                \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr0],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+#endif
+        "pavgb      %[ftmp5],   %[ftmp14],      %[ftmp4]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp3],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "sdc1       %[ftmp6],   0x10+%[stack]                           \n\t"
+        "paddb      %[ftmp7],   %[ftmp14],      %[ftmp4]                \n\t"
+        "paddb      %[ftmp8],   %[ftmp3],       %[ftmp2]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "mov.d      %[ftmp8],   %[ftmp7]                                \n\t"
+        "sdc1       %[ftmp7],   0x00+%[stack]                           \n\t"
+        "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp14],      %[ftmp1]                \n\t"
+        "paddb      %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "psubb      %[ftmp7],   %[ftmp14],      %[ftmp1]                \n\t"
+        "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "ldc1       %[ftmp12],  0x10+%[stack]                           \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp11]               \n\t"
+        "pavgb      %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "xor        %[ftmp8],   %[ftmp3],       %[ftmp1]                \n\t"
+        "pavgb      %[ftmp7],   %[ftmp3],       %[ftmp1]                \n\t"
+        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "ldc1       %[ftmp12],  0x40+%[stack]                           \n\t"
+        "psubb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "ldc1       %[ftmp13],  0x20+%[stack]                           \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp13]               \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp6],   0x00(%[pix])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp6],   0x00(%[pix],    %[addr1])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr1]                \n\t"
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "paddb      %[ftmp7],   %[ftmp14],      %[ftmp6]                \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
+        "ldc1       %[ftmp12],  0x00+%[stack]                           \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "paddb      %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
+        "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "ldc1       %[ftmp12],  0x40+%[stack]                           \n\t"
+        "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp5],   0x00(%[pix],    %[stride])              \n\t"
+        "gssdxc1    %[ftmp6],   0x00(%[pix],    %[addr2])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr2]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+#endif
+        "1:                                                             \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [ftmp12]"=&f"(ftmp[12]),          [ftmp13]"=&f"(ftmp[13]),
+          [ftmp14]"=&f"(ftmp[14]),          [ftmp15]"=&f"(ftmp[15]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [all64]"=&r"(all64),
+          [alpha]"+&r"(alpha),              [beta]"+&r"(beta)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [stack]"m"(stack[0]),             [ff_pb_1]"m"(ff_pb_1)
+        : "memory"
     );
 }
 
 void ff_deblock_v_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         int8_t *tc0)
 {
-    __asm__ volatile (
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "or $16, $0, %[pix]                             \r\n"
-        "dsubu $16, $16, %[stride]                      \r\n"
-        "dsubu $16, $16, %[stride]                      \r\n"
-        "ldc1 $f0, 0x0($16)                             \r\n"
-        "gsldxc1 $f2, 0x0($16, %[stride])               \r\n"
-        "ldc1 $f4, 0x0(%[pix])                          \r\n"
-        "gsldxc1 $f6, 0x0(%[pix], %[stride])            \r\n"
-        : [pix]"+r"(pix),[stride]"+r"(stride),[alpha]"+r"(alpha),
-          [beta]"+r"(beta)
-        : [tc0]"r"(tc0)
-        : "$16","$f2","$f4"
-    );
-
-    chroma_inter_body_mmi(pix, stride, alpha, beta, tc0);
+    double ftmp[9];
+    mips_reg addr[2];
+    int low32;
+    uint64_t all64;
 
     __asm__ volatile (
-        "gssdxc1 $f2, 0x0($16, %[stride])               \r\n"
-        "sdc1 $f4, 0x0(%[pix])                          \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride)
-        : "$16","$f2","$f4"
+        "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
+        "addi       %[beta],    %[beta],        -0x01                   \n\t"
+        "or         %[addr0],   $0,             %[pix]                  \n\t"
+        PTR_SUBU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_SUBU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "ldc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp2],   0x00(%[addr0],  %[stride])              \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        "gsldxc1    %[ftmp4],   0x00(%[pix],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[pix],         %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[alpha],   %[ftmp5]                                \n\t"
+        "mtc1       %[beta],    %[ftmp6]                                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "uld        %[low32],   0x00(%[tc0])                            \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "and        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "xor        %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "and        %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ff_pb_3]              \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ff_pb_A1],    %[ftmp4]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ff_pb_A1]             \n\t"
+        "pminub     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "pminub     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddusb    %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddusb    %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp2],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        "sdc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [low32]"=&r"(low32),              [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [alpha]"r"(alpha),                [beta]"r"(beta),
+          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1),
+          [ff_pb_3]"f"(ff_pb_3),            [ff_pb_A1]"f"(ff_pb_A1)
+        : "memory"
     );
 }
 
 void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         int beta)
 {
-    __asm__ volatile (
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "or $16, $0, %[pix]                             \r\n"
-        "dsubu $16, $16, %[stride]                      \r\n"
-        "dsubu $16, $16, %[stride]                      \r\n"
-        "ldc1 $f0, 0x0($16)                             \r\n"
-        "gsldxc1 $f2, 0x0($16, %[stride])               \r\n"
-        "ldc1 $f4, 0x0(%[pix])                          \r\n"
-        "gsldxc1 $f6, 0x0(%[pix], %[stride])            \r\n"
-        : [pix]"+r"(pix),[stride]"+r"(stride),[alpha]"+r"(alpha),
-          [beta]"+r"(beta)
-        ::"$16","$f0","$f2","$f4","$f6"
-    );
-
-    chroma_intra_body_mmi(pix, stride, alpha, beta);
+    double ftmp[9];
+    mips_reg addr[2];
+    uint64_t all64;
 
     __asm__ volatile (
-        "gssdxc1 $f2, 0x0($16, %[stride])               \r\n"
-        "sdc1 $f4, 0x0(%[pix])                          \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride)
-        : "$16","$f2","$f4"
+        "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
+        "addi       %[beta],    %[beta],        -0x01                   \n\t"
+        "or         %[addr0],   $0,             %[pix]                  \n\t"
+        PTR_SUBU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_SUBU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "ldc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp2],   0x00(%[addr0],  %[stride])              \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        "gsldxc1    %[ftmp4],   0x00(%[pix],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "ldc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        PTR_ADDU   "%[addr1],   %[pix],         %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[alpha],   %[ftmp5]                                \n\t"
+        "mtc1       %[beta],    %[ftmp6]                                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
+        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "mov.d      %[ftmp6],   %[ftmp2]                                \n\t"
+        "mov.d      %[ftmp7],   %[ftmp3]                                \n\t"
+        "xor        %[ftmp5],   %[ftmp2],       %[ftmp4]                \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "xor        %[ftmp5],   %[ftmp3],       %[ftmp1]                \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "psubb      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "and        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "and        %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "paddb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "paddb      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp2],   0x00(%[addr0],  %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        "sdc1       %[ftmp3],   0x00(%[pix])                            \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [alpha]"r"(alpha),                [beta]"r"(beta),
+          [ff_pb_1]"f"(ff_pb_1)
+        : "memory"
     );
 }
 
 void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         int8_t *tc0)
 {
-    __asm__ volatile (
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "daddu $16, %[stride], %[stride]                \r\n"
-        "daddiu %[pix], %[pix], -0x2                    \r\n"
-        "daddu $17, $16, %[stride]                      \r\n"
-        "daddu $19, $16, $16                            \r\n"
-        "or $18, $0, %[pix]                             \r\n"
-        "daddu %[pix], %[pix], $17                      \r\n"
-        "gslwlc1 $f0, 0x3($18)                          \r\n"
-        "daddu $12, $18, %[stride]                      \r\n"
-        "gslwrc1 $f0, 0x0($18)                          \r\n"
-        "gslwlc1 $f4, 0x3($12)                          \r\n"
-        "daddu $13, $18, $16                            \r\n"
-        "gslwrc1 $f4, 0x0($12)                          \r\n"
-        "gslwlc1 $f2, 0x3($13)                          \r\n"
-        "gslwrc1 $f2, 0x0($13)                          \r\n"
-        "gslwlc1 $f6, 0x3(%[pix])                       \r\n"
-        "gslwrc1 $f6, 0x0(%[pix])                       \r\n"
-        "punpcklbh $f0, $f0, $f4                        \r\n"
-        "punpcklbh $f2, $f2, $f6                        \r\n"
-        "daddu $12, %[pix], %[stride]                   \r\n"
-        "punpckhhw $f4, $f0, $f2                        \r\n"
-        "punpcklhw $f0, $f0, $f2                        \r\n"
-        "gslwlc1 $f8, 0x3($12)                          \r\n"
-        "daddu $13, %[pix], $16                         \r\n"
-        "gslwrc1 $f8, 0x0($12)                          \r\n"
-        "gslwlc1 $f12, 0x3($13)                         \r\n"
-        "daddu $12, %[pix], $17                         \r\n"
-        "gslwrc1 $f12, 0x0($13)                         \r\n"
-        "gslwlc1 $f10, 0x3($12)                         \r\n"
-        "daddu $13, %[pix], $19                         \r\n"
-        "gslwrc1 $f10, 0x0($12)                         \r\n"
-        "gslwlc1 $f14, 0x3($13)                         \r\n"
-        "gslwrc1 $f14, 0x0($13)                         \r\n"
-        "punpcklbh $f8, $f8, $f12                       \r\n"
-        "punpcklbh $f10, $f10, $f14                     \r\n"
-        "mov.d $f12, $f8                                \r\n"
-        "punpcklhw $f8, $f8, $f10                       \r\n"
-        "punpckhhw $f12, $f12, $f10                     \r\n"
-        "punpckhwd $f2, $f0, $f8                        \r\n"
-        "punpckhwd $f6, $f4, $f12                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpcklwd $f4, $f4, $f12                       \r\n"
-        "mov.d $f20, $f0                                \r\n"
-        "mov.d $f22, $f6                                \r\n"
-        : [pix]"+r"(pix),[stride]"+r"(stride),[alpha]"+r"(alpha),
-          [beta]"+r"(beta)
-        ::"$12","$13","$16","$17","$18","$19","$f0","$f2","$f4","$f6","$f8",
-          "$f10","$f12","$f14","$f20","$f22"
-    );
-
-    chroma_inter_body_mmi(pix, stride, alpha, beta, tc0);
+    double ftmp[11];
+    mips_reg addr[6];
+    int low32;
 
     __asm__ volatile (
-        "punpckhwd $f8, $f20, $f20                      \r\n"
-        "punpckhwd $f10, $f2, $f2                       \r\n"
-        "punpckhwd $f12, $f4, $f4                       \r\n"
-        "punpcklbh $f0, $f20, $f2                       \r\n"
-        "punpcklbh $f4, $f4, $f22                       \r\n"
-        "punpcklhw $f2, $f0, $f4                        \r\n"
-        "punpckhhw $f0, $f0, $f4                        \r\n"
-        "gsswlc1 $f2, 0x3($18)                          \r\n"
-        "gsswrc1 $f2, 0x0($18)                          \r\n"
-        "daddu $12, $18, %[stride]                      \r\n"
-        "punpckhwd $f2, $f2, $f2                        \r\n"
-        "gsswlc1 $f2, 0x3($12)                          \r\n"
-        "daddu $13, $18, $16                            \r\n"
-        "gsswrc1 $f2, 0x0($12)                          \r\n"
-        "gsswlc1 $f0, 0x3($13)                          \r\n"
-        "gsswrc1 $f0, 0x0($13)                          \r\n"
-        "punpckhwd $f0, $f0, $f0                        \r\n"
-        "punpckhwd $f6, $f22, $f22                      \r\n"
-        "gsswlc1 $f0, 0x3(%[pix])                       \r\n"
-        "gsswrc1 $f0, 0x0(%[pix])                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "punpcklbh $f12, $f12, $f6                      \r\n"
-        "daddu $12, %[pix], %[stride]                   \r\n"
-        "punpcklhw $f10, $f8, $f12                      \r\n"
-        "punpckhhw $f8, $f8, $f12                       \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "punpckhwd $f10, $f10, $f10                     \r\n"
-        "daddu $12, %[pix], $16                         \r\n"
-        "daddu $13, %[pix], $17                         \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "gsswlc1 $f8, 0x3($13)                          \r\n"
-        "daddu $12, %[pix], $19                         \r\n"
-        "punpckhwd $f20, $f8, $f8                       \r\n"
-        "gsswrc1 $f8, 0x0($13)                          \r\n"
-        "gsswlc1 $f20, 0x3($12)                         \r\n"
-        "gsswrc1 $f20, 0x0($12)                         \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride)
-        : "$12","$13","$16","$17","$18","$19","$f0","$f2","$f4","$f6","$f8",
-          "$f10","$f12","$f20"
+        "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
+        "addi       %[beta],    %[beta],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        PTR_ADDI   "%[pix],     %[pix],         -0x02                   \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr2],   %[addr0],       %[addr0]                \n\t"
+        "or         %[addr5],   $0,             %[pix]                  \n\t"
+        PTR_ADDU   "%[pix],     %[pix],         %[addr1]                \n\t"
+        "uld        %[low32],   0x00(%[addr5])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr5],       %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr5],       %[addr0]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "uld        %[low32],   0x00(%[pix])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr0]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr1]                \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr2]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "mov.d      %[ftmp6],   %[ftmp4]                                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp2],       %[ftmp6]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "mov.d      %[ftmp9],   %[ftmp0]                                \n\t"
+        "mov.d      %[ftmp10],  %[ftmp3]                                \n\t"
+
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "mtc1       %[alpha],   %[ftmp4]                                \n\t"
+        "mtc1       %[beta],    %[ftmp5]                                \n\t"
+        "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp0],       %[ftmp1]                \n\t"
+        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
+        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "uld        %[low32],   0x00(%[tc0])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "and        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "xor        %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "pavgb      %[ftmp3],   %[ftmp3],       %[ff_pb_3]              \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "paddusb    %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp6],   %[ff_pb_A1],    %[ftmp3]                \n\t"
+        "psubusb    %[ftmp3],   %[ftmp3],       %[ff_pb_A1]             \n\t"
+        "pminub     %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "pminub     %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "psubusb    %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "paddusb    %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddusb    %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+
+        "punpckhwd  %[ftmp4],   %[ftmp9],       %[ftmp9]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp1],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp6],   %[ftmp2],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp9],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp10]               \n\t"
+        "punpcklhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr5],       %[stride]               \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "gsswlc1    %[ftmp1],   0x03(%[addr3])                          \n\t"
+        PTR_ADDU   "%[addr4],   %[addr5],       %[addr0]                \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp0],   0x03(%[addr4])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp10],      %[ftmp10]               \n\t"
+        "gsswlc1    %[ftmp0],   0x03(%[pix])                            \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[pix])                            \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr0]                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr1]                \n\t"
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr2]                \n\t"
+        "punpckhwd  %[ftmp9],   %[ftmp4],       %[ftmp4]                \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr4])                          \n\t"
+        "gsswlc1    %[ftmp9],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp9],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr5],       %[stride]               \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        PTR_ADDU   "%[addr4],   %[addr5],       %[addr0]                \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp10],      %[ftmp10]               \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[pix])                            \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr0]                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr1]                \n\t"
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr2]                \n\t"
+        "punpckhwd  %[ftmp9],   %[ftmp4],       %[ftmp4]                \n\t"
+        "mfc1       %[low32],   %[ftmp9]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [pix]"+&r"(pix),
+          [low32]"=&r"(low32)
+        : [alpha]"r"(alpha),                [beta]"r"(beta),
+          [stride]"r"((mips_reg)stride),    [tc0]"r"(tc0),
+          [ff_pb_1]"f"(ff_pb_1),            [ff_pb_3]"f"(ff_pb_3),
+          [ff_pb_A1]"f"(ff_pb_A1)
+        : "memory"
     );
 }
 
 void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         int beta)
 {
-    __asm__ volatile (
-        "daddiu %[alpha], %[alpha], -0x1                \r\n"
-        "daddiu %[beta], %[beta], -0x1                  \r\n"
-        "daddu $16, %[stride], %[stride]                \r\n"
-        "daddiu %[pix], %[pix], -0x2                    \r\n"
-        "daddu $17, $16, %[stride]                      \r\n"
-        "daddu $19, $16, $16                            \r\n"
-        "or $18, $0, %[pix]                             \r\n"
-        "daddu %[pix], %[pix], $17                      \r\n"
-        "gslwlc1 $f0, 0x3($18)                          \r\n"
-        "daddu $12, $18, %[stride]                      \r\n"
-        "gslwrc1 $f0, 0x0($18)                          \r\n"
-        "gslwlc1 $f4, 0x3($12)                          \r\n"
-        "daddu $13, $18, $16                            \r\n"
-        "gslwrc1 $f4, 0x0($12)                          \r\n"
-        "gslwlc1 $f2, 0x3($13)                          \r\n"
-        "gslwrc1 $f2, 0x0($13)                          \r\n"
-        "gslwlc1 $f6, 0x3(%[pix])                       \r\n"
-        "gslwrc1 $f6, 0x0(%[pix])                       \r\n"
-        "punpcklbh $f0, $f0, $f4                        \r\n"
-        "punpcklbh $f2, $f2, $f6                        \r\n"
-        "daddu $12, %[pix], %[stride]                   \r\n"
-        "punpckhhw $f4, $f0, $f2                        \r\n"
-        "punpcklhw $f0, $f0, $f2                        \r\n"
-        "gslwlc1 $f8, 0x3($12)                          \r\n"
-        "daddu $13, %[pix], $16                         \r\n"
-        "gslwrc1 $f8, 0x0($12)                          \r\n"
-        "gslwlc1 $f12, 0x3($13)                         \r\n"
-        "daddu $12, %[pix], $17                         \r\n"
-        "gslwrc1 $f12, 0x0($13)                         \r\n"
-        "gslwlc1 $f10, 0x3($12)                         \r\n"
-        "daddu $13, %[pix], $19                         \r\n"
-        "gslwrc1 $f10, 0x0($12)                         \r\n"
-        "gslwlc1 $f14, 0x3($13)                         \r\n"
-        "gslwrc1 $f14, 0x0($13)                         \r\n"
-        "punpcklbh $f8, $f8, $f12                       \r\n"
-        "punpcklbh $f10, $f10, $f14                     \r\n"
-        "mov.d $f12, $f8                                \r\n"
-        "punpcklhw $f8, $f8, $f10                       \r\n"
-        "punpckhhw $f12, $f12, $f10                     \r\n"
-        "punpckhwd $f2, $f0, $f8                        \r\n"
-        "punpckhwd $f6, $f4, $f12                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpcklwd $f4, $f4, $f12                       \r\n"
-        : [pix]"+r"(pix),[stride]"+r"(stride),[alpha]"+r"(alpha),
-          [beta]"+r"(beta)
-        ::"$12","$13","$16","$17","$18","$19","$f0","$f2","$f4","$f6","$f8",
-          "$f10","$f12","$f14","$f20","$f22"
-    );
-
-    chroma_intra_body_mmi(pix, stride, alpha, beta);
+    double ftmp[11];
+    mips_reg addr[6];
+    int low32;
 
     __asm__ volatile (
-        "punpckhwd $f8, $f0, $f0                        \r\n"
-        "punpckhwd $f10, $f2, $f2                       \r\n"
-        "punpckhwd $f12, $f4, $f4                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpcklhw $f2, $f0, $f4                        \r\n"
-        "punpckhhw $f0, $f0, $f4                        \r\n"
-        "gsswlc1 $f2, 0x3($18)                          \r\n"
-        "gsswrc1 $f2, 0x0($18)                          \r\n"
-        "daddu $12, $18, %[stride]                      \r\n"
-        "punpckhwd $f2, $f2, $f2                        \r\n"
-        "gsswlc1 $f2, 0x3($12)                          \r\n"
-        "daddu $13, $18, $16                            \r\n"
-        "gsswrc1 $f2, 0x0($12)                          \r\n"
-        "gsswlc1 $f0, 0x3($13)                          \r\n"
-        "gsswrc1 $f0, 0x0($13)                          \r\n"
-        "punpckhwd $f0, $f0, $f0                        \r\n"
-        "punpckhwd $f6, $f6, $f6                        \r\n"
-        "gsswlc1 $f0, 0x3(%[pix])                       \r\n"
-        "gsswrc1 $f0, 0x0(%[pix])                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "punpcklbh $f12, $f12, $f6                      \r\n"
-        "daddu $12, %[pix], %[stride]                   \r\n"
-        "punpcklhw $f10, $f8, $f12                      \r\n"
-        "punpckhhw $f8, $f8, $f12                       \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "punpckhwd $f10, $f10, $f10                     \r\n"
-        "daddu $12, %[pix], $16                         \r\n"
-        "daddu $13, %[pix], $17                         \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "gsswlc1 $f8, 0x3($13)                          \r\n"
-        "daddu $12, %[pix], $19                         \r\n"
-        "punpckhwd $f20, $f8, $f8                       \r\n"
-        "gsswrc1 $f8, 0x0($13)                          \r\n"
-        "gsswlc1 $f20, 0x3($12)                         \r\n"
-        "gsswrc1 $f20, 0x0($12)                         \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride)
-        : "$12","$13","$16","$17","$18","$19","$f0","$f2","$f4","$f6","$f8",
-          "$f10","$f12","$f20"
+        "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
+        "addi       %[beta],    %[beta],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        PTR_ADDI   "%[pix],     %[pix],         -0x02                   \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr2],   %[addr0],       %[addr0]                \n\t"
+        "or         %[addr5],   $0,             %[pix]                  \n\t"
+        PTR_ADDU   "%[pix],     %[pix],         %[addr1]                \n\t"
+        "uld        %[low32],   0x00(%[addr5])                          \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr5],       %[stride]               \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr5],       %[addr0]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "uld        %[low32],   0x00(%[pix])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr0]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr1]                \n\t"
+        "uld        %[low32],   0x00(%[addr3])                          \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr2]                \n\t"
+        "uld        %[low32],   0x00(%[addr4])                          \n\t"
+        "mtc1       %[low32],   %[ftmp7]                                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "mov.d      %[ftmp6],   %[ftmp4]                                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp2],       %[ftmp6]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "mtc1       %[alpha],   %[ftmp4]                                \n\t"
+        "mtc1       %[beta],    %[ftmp5]                                \n\t"
+        "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp1],       %[ftmp2]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp0],       %[ftmp1]                \n\t"
+        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
+        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp1]                                \n\t"
+        "mov.d      %[ftmp6],   %[ftmp2]                                \n\t"
+        "xor        %[ftmp4],   %[ftmp1],       %[ftmp3]                \n\t"
+        "and        %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psubusb    %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "xor        %[ftmp4],   %[ftmp2],       %[ftmp0]                \n\t"
+        "and        %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
+        "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psubb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "psubb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "and        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "and        %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "paddb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+
+        "punpckhwd  %[ftmp4],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp1],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp6],   %[ftmp2],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr5],       %[stride]               \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr5],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp0],   0x03(%[addr4])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp0],   0x03(%[pix])                            \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[pix])                            \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[pix])                            \n\t"
+#endif
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[stride]               \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr0]                \n\t"
+        PTR_ADDU   "%[addr4],   %[pix],         %[addr1]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[pix],         %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp4],   0x03(%[addr4])                          \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp9],   %[ftmp4],       %[ftmp4]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp9],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp9],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp9]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [pix]"+&r"(pix),
+          [low32]"=&r"(low32)
+        : [alpha]"r"(alpha),                [beta]"r"(beta),
+          [stride]"r"((mips_reg)stride),    [ff_pb_1]"f"(ff_pb_1)
+        : "memory"
     );
 }
 
@@ -1982,233 +2653,399 @@ void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
 void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         int beta)
 {
-    ff_deblock_v8_luma_intra_8_mmi(pix + 0, stride, alpha, beta);
-    ff_deblock_v8_luma_intra_8_mmi(pix + 8, stride, alpha, beta);
+    deblock_v8_luma_intra_8_mmi(pix + 0, stride, alpha, beta);
+    deblock_v8_luma_intra_8_mmi(pix + 8, stride, alpha, beta);
 }
 
 void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         int8_t *tc0)
 {
     uint64_t stack[0xd];
+    double ftmp[9];
+    mips_reg addr[8];
+    uint64_t all64;
+    int low32;
 
     __asm__ volatile (
-        "daddu $15, %[stride], %[stride]                \r\n"
-        "daddiu $8, %[pix], -0x4                        \r\n"
-        "daddu $9, %[stride], $15                       \r\n"
-        "gsldlc1 $f0, 0x7($8)                           \r\n"
-        "gsldrc1 $f0, 0x0($8)                           \r\n"
-        "daddu $12, $8, %[stride]                       \r\n"
-        "daddu $10, $8, $9                              \r\n"
-        "gsldlc1 $f2, 0x7($12)                          \r\n"
-        "daddu $11, $8, $15                             \r\n"
-        "gsldrc1 $f2, 0x0($12)                          \r\n"
-        "gsldlc1 $f4, 0x7($11)                          \r\n"
-        "gsldrc1 $f4, 0x0($11)                          \r\n"
-        "gsldlc1 $f6, 0x7($10)                          \r\n"
-        "daddu $12, $10, %[stride]                      \r\n"
-        "gsldrc1 $f6, 0x0($10)                          \r\n"
-        "gsldlc1 $f8, 0x7($12)                          \r\n"
-        "daddu $11, $10, $15                            \r\n"
-        "gsldrc1 $f8, 0x0($12)                          \r\n"
-        "gsldlc1 $f10, 0x7($11)                         \r\n"
-        "daddu $12, $10, $9                             \r\n"
-        "gsldrc1 $f10, 0x0($11)                         \r\n"
-        "gsldlc1 $f12, 0x7($12)                         \r\n"
-        "gsldrc1 $f12, 0x0($12)                         \r\n"
-        "daddu $14, $15, $15                            \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "daddu $12, $10, $14                            \r\n"
-        "sdc1 $f2, 0x10+%[stack]                        \r\n"
-        "gsldlc1 $f16, 0x7($12)                         \r\n"
-        "gsldrc1 $f16, 0x0($12)                         \r\n"
-        "daddu $13, $14, $14                            \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "punpckhhw $f2, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "ldc1 $f16, 0x10+%[stack]                       \r\n"
-        "punpckhwd $f0, $f0, $f8                        \r\n"
-        "sdc1 $f0, 0x0+%[stack]                         \r\n"
-        "punpckhhw $f12, $f14, $f16                     \r\n"
-        "punpcklhw $f14, $f14, $f16                     \r\n"
-        "punpckhhw $f0, $f6, $f10                       \r\n"
-        "punpcklhw $f6, $f6, $f10                       \r\n"
-        "punpcklwd $f12, $f12, $f0                      \r\n"
-        "punpckhwd $f10, $f14, $f6                      \r\n"
-        "punpcklwd $f14, $f14, $f6                      \r\n"
-        "punpckhwd $f6, $f2, $f4                        \r\n"
-        "punpcklwd $f2, $f2, $f4                        \r\n"
-        "sdc1 $f2, 0x10+%[stack]                        \r\n"
-        "sdc1 $f6, 0x20+%[stack]                        \r\n"
-        "sdc1 $f14, 0x30+%[stack]                       \r\n"
-        "sdc1 $f10, 0x40+%[stack]                       \r\n"
-        "sdc1 $f12, 0x50+%[stack]                       \r\n"
-        "daddu $8, $8, $13                              \r\n"
-        "daddu $10, $10, $13                            \r\n"
-        "gsldlc1 $f0, 0x7($8)                           \r\n"
-        "daddu $12, $8, %[stride]                       \r\n"
-        "gsldrc1 $f0, 0x0($8)                           \r\n"
-        "gsldlc1 $f2, 0x7($12)                          \r\n"
-        "daddu $11, $8, $15                             \r\n"
-        "gsldrc1 $f2, 0x0($12)                          \r\n"
-        "gsldlc1 $f4, 0x7($11)                          \r\n"
-        "gsldrc1 $f4, 0x0($11)                          \r\n"
-        "gsldlc1 $f6, 0x7($10)                          \r\n"
-        "daddu $12, $10, %[stride]                      \r\n"
-        "gsldrc1 $f6, 0x0($10)                          \r\n"
-        "gsldlc1 $f8, 0x7($12)                          \r\n"
-        "daddu $11, $10, $15                            \r\n"
-        "gsldrc1 $f8, 0x0($12)                          \r\n"
-        "gsldlc1 $f10, 0x7($11)                         \r\n"
-        "daddu $12, $10, $9                             \r\n"
-        "gsldrc1 $f10, 0x0($11)                         \r\n"
-        "gsldlc1 $f12, 0x7($12)                         \r\n"
-        "gsldrc1 $f12, 0x0($12)                         \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "daddu $12, $10, $14                            \r\n"
-        "sdc1 $f2, 0x18+%[stack]                        \r\n"
-        "gsldlc1 $f16, 0x7($12)                         \r\n"
-        "gsldrc1 $f16, 0x0($12)                         \r\n"
-        "punpckhhw $f2, $f0, $f4                        \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "punpckhwd $f0, $f0, $f8                        \r\n"
-        "ldc1 $f16, 0x18+%[stack]                       \r\n"
-        "sdc1 $f0, 0x8+%[stack]                         \r\n"
-        "punpckhhw $f12, $f14, $f16                     \r\n"
-        "punpcklhw $f14, $f14, $f16                     \r\n"
-        "punpckhhw $f0, $f6, $f10                       \r\n"
-        "punpcklhw $f6, $f6, $f10                       \r\n"
-        "punpckhwd $f10, $f14, $f6                      \r\n"
-        "punpcklwd $f14, $f14, $f6                      \r\n"
-        "punpckhwd $f6, $f2, $f4                        \r\n"
-        "punpcklwd $f2, $f2, $f4                        \r\n"
-        "punpcklwd $f12, $f12, $f0                      \r\n"
-        "sdc1 $f2, 0x18+%[stack]                        \r\n"
-        "sdc1 $f6, 0x28+%[stack]                        \r\n"
-        "sdc1 $f14, 0x38+%[stack]                       \r\n"
-        "sdc1 $f10, 0x48+%[stack]                       \r\n"
-        "sdc1 $f12, 0x58+%[stack]                       \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),[stack]"m"(stack[0])
-        : "$8","$9","$10","$11","$12","$13","$14","$15","$f0","$f2","$f4",
-          "$f6","$f8","$f10","$f12","$f14","$f16"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        PTR_ADDI   "%[addr1],   %[pix],         -0x4                    \n\t"
+        PTR_ADDU   "%[addr2],   %[stride],      %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr1],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr4],   %[addr1],       %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp2],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr4])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr4])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp4],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp6],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr6],   %[addr0],       %[addr0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr6]                \n\t"
+        "sdc1       %[ftmp1],   0x10(%[stack])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp8],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr7],   %[addr6],       %[addr6]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "ldc1       %[ftmp8],   0x10(%[stack])                          \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[stack])                          \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp8]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp3],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp1],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp1],   0x10(%[stack])                          \n\t"
+        "sdc1       %[ftmp3],   0x20(%[stack])                          \n\t"
+        "sdc1       %[ftmp7],   0x30(%[stack])                          \n\t"
+        "sdc1       %[ftmp5],   0x40(%[stack])                          \n\t"
+        "sdc1       %[ftmp6],   0x50(%[stack])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[addr7]                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr1],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp2],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr4])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr4])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp4],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp6],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr6]                \n\t"
+        "sdc1       %[ftmp1],   0x18(%[stack])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp8],   0x07(%[addr3])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr3])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        "punpckhhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "ldc1       %[ftmp8],   0x18(%[stack])                          \n\t"
+        "sdc1       %[ftmp0],   0x08(%[stack])                          \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp8]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp3],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp1],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "sdc1       %[ftmp1],   0x18(%[stack])                          \n\t"
+        "sdc1       %[ftmp3],   0x28(%[stack])                          \n\t"
+        "sdc1       %[ftmp7],   0x38(%[stack])                          \n\t"
+        "sdc1       %[ftmp5],   0x48(%[stack])                          \n\t"
+        "sdc1       %[ftmp6],   0x58(%[stack])                          \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),            [addr7]"=&r"(addr[7]),
+          [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [stack]"r"(stack)
+        : "memory"
     );
 
     ff_deblock_v_luma_8_mmi((uint8_t *) &stack[6], 0x10, alpha, beta, tc0);
 
     __asm__ volatile (
-        "daddu $15, %[stride], %[stride]                \r\n"
-        "daddiu $8, %[pix], -0x2                        \r\n"
-        "daddu $14, $15, $15                            \r\n"
-        "daddu $9, $15, %[stride]                       \r\n"
-        "daddu $13, $14, $14                            \r\n"
-        "daddu $10, $8, $9                              \r\n"
-        "ldc1 $f0, 0x10+%[stack]                        \r\n"
-        "ldc1 $f2, 0x20+%[stack]                        \r\n"
-        "ldc1 $f4, 0x30+%[stack]                        \r\n"
-        "ldc1 $f6, 0x40+%[stack]                        \r\n"
-        "punpckhwd $f8, $f0, $f0                        \r\n"
-        "punpckhwd $f10, $f2, $f2                       \r\n"
-        "punpckhwd $f12, $f4, $f4                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpcklhw $f2, $f0, $f4                        \r\n"
-        "punpckhhw $f0, $f0, $f4                        \r\n"
-        "gsswlc1 $f2, 0x3($8)                           \r\n"
-        "gsswrc1 $f2, 0x0($8)                           \r\n"
-        "daddu $12, $8, %[stride]                       \r\n"
-        "punpckhwd $f2, $f2, $f2                        \r\n"
-        "daddu $11, $8, $15                             \r\n"
-        "gsswlc1 $f2, 0x3($12)                          \r\n"
-        "gsswrc1 $f2, 0x0($12)                          \r\n"
-        "gsswlc1 $f0, 0x3($11)                          \r\n"
-        "gsswrc1 $f0, 0x0($11)                          \r\n"
-        "punpckhwd $f0, $f0, $f0                        \r\n"
-        "punpckhwd $f6, $f6, $f6                        \r\n"
-        "gsswlc1 $f0, 0x3($10)                          \r\n"
-        "gsswrc1 $f0, 0x0($10)                          \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "punpcklbh $f12, $f12, $f6                      \r\n"
-        "punpcklhw $f10, $f8, $f12                      \r\n"
-        "daddu $12, $10, %[stride]                      \r\n"
-        "punpckhhw $f8, $f8, $f12                       \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "daddu $12, $10, $15                            \r\n"
-        "punpckhwd $f10, $f10, $f10                     \r\n"
-        "daddu $11, $10, $9                             \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "gsswlc1 $f8, 0x3($11)                          \r\n"
-        "gsswrc1 $f8, 0x0($11)                          \r\n"
-        "daddu $12, $10, $14                            \r\n"
-        "punpckhwd $f8, $f8, $f8                        \r\n"
-        "daddu $8, $8, $13                              \r\n"
-        "gsswlc1 $f8, 0x3($12)                          \r\n"
-        "gsswrc1 $f8, 0x0($12)                          \r\n"
-        "daddu $10, $10, $13                            \r\n"
-        "ldc1 $f0, 0x18+%[stack]                        \r\n"
-        "ldc1 $f2, 0x28+%[stack]                        \r\n"
-        "ldc1 $f4, 0x38+%[stack]                        \r\n"
-        "ldc1 $f6, 0x48+%[stack]                        \r\n"
-        "daddu $15, %[stride], %[stride]                \r\n"
-        "punpckhwd $f8, $f0, $f0                        \r\n"
-        "daddu $14, $15, $15                            \r\n"
-        "punpckhwd $f10, $f2, $f2                       \r\n"
-        "punpckhwd $f12, $f4, $f4                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "daddu $12, $8, %[stride]                       \r\n"
-        "punpcklhw $f2, $f0, $f4                        \r\n"
-        "punpckhhw $f0, $f0, $f4                        \r\n"
-        "gsswlc1 $f2, 0x3($8)                           \r\n"
-        "gsswrc1 $f2, 0x0($8)                           \r\n"
-        "punpckhwd $f2, $f2, $f2                        \r\n"
-        "daddu $11, $8, $15                             \r\n"
-        "gsswlc1 $f2, 0x3($12)                          \r\n"
-        "gsswrc1 $f2, 0x0($12)                          \r\n"
-        "gsswlc1 $f0, 0x3($11)                          \r\n"
-        "gsswrc1 $f0, 0x0($11)                          \r\n"
-        "punpckhwd $f0, $f0, $f0                        \r\n"
-        "punpckhwd $f6, $f6, $f6                        \r\n"
-        "gsswlc1 $f0, 0x3($10)                          \r\n"
-        "gsswrc1 $f0, 0x0($10)                          \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "punpcklbh $f12, $f12, $f6                      \r\n"
-        "daddu $12, $10, %[stride]                      \r\n"
-        "punpcklhw $f10, $f8, $f12                      \r\n"
-        "punpckhhw $f8, $f8, $f12                       \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "daddu $12, $10, $15                            \r\n"
-        "punpckhwd $f10, $f10, $f10                     \r\n"
-        "daddu $11, $10, $9                             \r\n"
-        "gsswlc1 $f10, 0x3($12)                         \r\n"
-        "gsswrc1 $f10, 0x0($12)                         \r\n"
-        "gsswlc1 $f8, 0x3($11)                          \r\n"
-        "gsswrc1 $f8, 0x0($11)                          \r\n"
-        "daddu $12, $10, $14                            \r\n"
-        "punpckhwd $f8, $f8, $f8                        \r\n"
-        "gsswlc1 $f8, 0x3($12)                          \r\n"
-        "gsswrc1 $f8, 0x0($12)                          \r\n"
-        ::[pix]"r"(pix),[stride]"r"((int64_t)stride),[stack]"m"(stack[0])
-        : "$8","$9","$10","$11","$12","$13","$14","$15","$f0","$f2","$f4",
-          "$f6","$f8","$f10","$f12","$f14","$f16"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        PTR_ADDI   "%[addr1],   %[pix],         -0x02                   \n\t"
+        PTR_ADDU   "%[addr6],   %[addr0],       %[addr0]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr7],   %[addr6],       %[addr6]                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr1],       %[addr2]                \n\t"
+        "ldc1       %[ftmp0],   0x10(%[stack])                          \n\t"
+        "ldc1       %[ftmp1],   0x20(%[stack])                          \n\t"
+        "ldc1       %[ftmp2],   0x30(%[stack])                          \n\t"
+        "ldc1       %[ftmp3],   0x40(%[stack])                          \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp1],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp6],   %[ftmp2],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr1])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr1],       %[stride]               \n\t"
+        "punpckhwd  %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp0],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp0],   0x03(%[addr4])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+#endif
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp4],       %[ftmp6]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr4],       %[stride]               \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr6]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[addr7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp4],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr7]                \n\t"
+        "ldc1       %[ftmp0],   0x18(%[stack])                          \n\t"
+        "ldc1       %[ftmp1],   0x28(%[stack])                          \n\t"
+        "ldc1       %[ftmp2],   0x38(%[stack])                          \n\t"
+        "ldc1       %[ftmp3],   0x48(%[stack])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp0],       %[ftmp0]                \n\t"
+        PTR_ADDU   "%[addr6],   %[addr0],       %[addr0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp1],       %[ftmp1]                \n\t"
+        "punpckhwd  %[ftmp6],   %[ftmp2],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr1],       %[stride]               \n\t"
+        "punpcklhw  %[ftmp1],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr1])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr1])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp0],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        "punpckhwd  %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "punpckhwd  %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp0],   0x03(%[addr4])                          \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[addr4])                          \n\t"
+#endif
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr4],       %[stride]               \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr0]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp5],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp5],   0x00(%[addr3])                          \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[addr5])                          \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp5]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr3],   %[addr4],       %[addr6]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp4],   0x03(%[addr3])                          \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[addr3])                          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),            [addr7]"=&r"(addr[7]),
+          [low32]"=&r"(low32)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [stack]"r"(stack)
+        : "memory"
     );
 }
 
@@ -2217,284 +3054,440 @@ void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
 {
     uint64_t ptmp[0x11];
     uint64_t pdat[4];
+    double ftmp[9];
+    mips_reg addr[7];
+    uint64_t all64;
 
     __asm__ volatile (
-        "daddu $12, %[stride], %[stride]                \r\n"
-        "daddiu $10, %[pix], -0x4                       \r\n"
-        "daddu $11, $12, %[stride]                      \r\n"
-        "daddu $13, $12, $12                            \r\n"
-        "daddu $9, $10, $11                             \r\n"
-        "daddu $8, $10, %[stride]                       \r\n"
-        "gsldlc1 $f0, 0x7($10)                          \r\n"
-        "gsldrc1 $f0, 0x0($10)                          \r\n"
-        "daddu $14, $10, $12                            \r\n"
-        "gsldlc1 $f2, 0x7($8)                           \r\n"
-        "gsldrc1 $f2, 0x0($8)                           \r\n"
-        "gsldlc1 $f4, 0x7($14)                          \r\n"
-        "gsldrc1 $f4, 0x0($14)                          \r\n"
-        "daddu $8, $9, %[stride]                        \r\n"
-        "gsldlc1 $f6, 0x7($9)                           \r\n"
-        "gsldrc1 $f6, 0x0($9)                           \r\n"
-        "daddu $14, $9, $12                             \r\n"
-        "gsldlc1 $f8, 0x7($8)                           \r\n"
-        "gsldrc1 $f8, 0x0($8)                           \r\n"
-        "daddu $8, $9, $11                              \r\n"
-        "gsldlc1 $f10, 0x7($14)                         \r\n"
-        "gsldrc1 $f10, 0x0($14)                         \r\n"
-        "gsldlc1 $f12, 0x7($8)                          \r\n"
-        "gsldrc1 $f12, 0x0($8)                          \r\n"
-        "daddu $8, $9, $13                              \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "gsldlc1 $f16, 0x7($8)                          \r\n"
-        "gsldrc1 $f16, 0x0($8)                          \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "sdc1 $f6, 0x0+%[ptmp]                          \r\n"
-        "punpckhhw $f6, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "punpckhhw $f12, $f14, $f2                      \r\n"
-        "punpcklhw $f14, $f14, $f2                      \r\n"
-        "sdc1 $f4, 0x20+%[ptmp]                         \r\n"
-        "ldc1 $f4, 0x0+%[ptmp]                          \r\n"
-        "punpckhhw $f2, $f4, $f10                       \r\n"
-        "punpcklhw $f4, $f4, $f10                       \r\n"
-        "punpckhwd $f10, $f0, $f8                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpckhwd $f8, $f14, $f4                       \r\n"
-        "punpcklwd $f14, $f14, $f4                      \r\n"
-        "sdc1 $f0, 0x0+%[ptmp]                          \r\n"
-        "sdc1 $f10, 0x10+%[ptmp]                        \r\n"
-        "sdc1 $f14, 0x40+%[ptmp]                        \r\n"
-        "sdc1 $f8, 0x50+%[ptmp]                         \r\n"
-        "ldc1 $f16, 0x20+%[ptmp]                        \r\n"
-        "punpckhwd $f0, $f6, $f16                       \r\n"
-        "punpcklwd $f6, $f6, $f16                       \r\n"
-        "punpckhwd $f10, $f12, $f2                      \r\n"
-        "punpcklwd $f12, $f12, $f2                      \r\n"
-        "daddu $8, $13, $13                             \r\n"
-        "sdc1 $f6, 0x20+%[ptmp]                         \r\n"
-        "sdc1 $f0, 0x30+%[ptmp]                         \r\n"
-        "sdc1 $f12, 0x60+%[ptmp]                        \r\n"
-        "sdc1 $f10, 0x70+%[ptmp]                        \r\n"
-        "daddu $10, $10, $8                             \r\n"
-        "daddu $9, $9, $8                               \r\n"
-        "daddu $8, $10, %[stride]                       \r\n"
-        "gsldlc1 $f0, 0x7($10)                          \r\n"
-        "gsldrc1 $f0, 0x0($10)                          \r\n"
-        "daddu $14, $10, $12                            \r\n"
-        "gsldlc1 $f2, 0x7($8)                           \r\n"
-        "gsldrc1 $f2, 0x0($8)                           \r\n"
-        "gsldlc1 $f4, 0x7($14)                          \r\n"
-        "gsldrc1 $f4, 0x0($14)                          \r\n"
-        "daddu $8, $9, %[stride]                        \r\n"
-        "gsldlc1 $f6, 0x7($9)                           \r\n"
-        "gsldrc1 $f6, 0x0($9)                           \r\n"
-        "daddu $14, $9, $12                             \r\n"
-        "gsldlc1 $f8, 0x7($8)                           \r\n"
-        "gsldrc1 $f8, 0x0($8)                           \r\n"
-        "daddu $8, $9, $11                              \r\n"
-        "gsldlc1 $f10, 0x7($14)                         \r\n"
-        "gsldrc1 $f10, 0x0($14)                         \r\n"
-        "gsldlc1 $f12, 0x7($8)                          \r\n"
-        "gsldrc1 $f12, 0x0($8)                          \r\n"
-        "daddu $8, $9, $13                              \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "gsldlc1 $f16, 0x7($8)                          \r\n"
-        "gsldrc1 $f16, 0x0($8)                          \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "sdc1 $f6, 0x8+%[ptmp]                          \r\n"
-        "punpckhhw $f6, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "punpckhhw $f12, $f14, $f2                      \r\n"
-        "punpcklhw $f14, $f14, $f2                      \r\n"
-        "sdc1 $f4, 0x28+%[ptmp]                         \r\n"
-        "ldc1 $f4, 0x8+%[ptmp]                          \r\n"
-        "punpckhhw $f2, $f4, $f10                       \r\n"
-        "punpcklhw $f4, $f4, $f10                       \r\n"
-        "punpckhwd $f10, $f0, $f8                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpckhwd $f8, $f14, $f4                       \r\n"
-        "punpcklwd $f14, $f14, $f4                      \r\n"
-        "sdc1 $f0, 0x8+%[ptmp]                          \r\n"
-        "sdc1 $f10, 0x18+%[ptmp]                        \r\n"
-        "sdc1 $f14, 0x48+%[ptmp]                        \r\n"
-        "sdc1 $f8, 0x58+%[ptmp]                         \r\n"
-        "ldc1 $f16, 0x28+%[ptmp]                        \r\n"
-        "punpckhwd $f0, $f6, $f16                       \r\n"
-        "punpcklwd $f6, $f6, $f16                       \r\n"
-        "punpckhwd $f10, $f12, $f2                      \r\n"
-        "punpcklwd $f12, $f12, $f2                      \r\n"
-        "sdc1 $f6, 0x28+%[ptmp]                         \r\n"
-        "sdc1 $f0, 0x38+%[ptmp]                         \r\n"
-        "sdc1 $f12, 0x68+%[ptmp]                        \r\n"
-        "sdc1 $f10, 0x78+%[ptmp]                        \r\n"
-        "sd $10, 0x00+%[pdat]                           \r\n"
-        "sd $11, 0x08+%[pdat]                           \r\n"
-        "sd $12, 0x10+%[pdat]                           \r\n"
-        "sd $13, 0x18+%[pdat]                           \r\n"
-        ::[pix]"r"(pix),[stride]"r"((uint64_t)stride),[ptmp]"m"(ptmp[0]),
-          [pdat]"m"(pdat[0])
-        : "$8","$9","$10","$11","$12","$13","$14","$f0","$f2","$f4","$f6",
-          "$f8","$f10","$f12","$f14","$f16"
+        PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
+        PTR_ADDI   "%[addr1],   %[pix],         -0x04                   \n\t"
+        PTR_ADDU   "%[addr2],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr3],   %[addr0],       %[addr0]                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr1],       %[addr2]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[stride]               \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr4])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr0]                \n\t"
+        "gsldlc1    %[ftmp4],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr6])                          \n\t"
+        "gsldlc1    %[ftmp6],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr4])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr3]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp8],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "sdc1       %[ftmp3],   0x00(%[ptmp])                           \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "sdc1       %[ftmp2],   0x20(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp2],   0x00(%[ptmp])                           \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp7],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp5],   0x10(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp7],   0x40(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp4],   0x50(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp8],   0x20(%[ptmp])                           \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp6],       %[ftmp1]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr3],       %[addr3]                \n\t"
+        "sdc1       %[ftmp3],   0x20(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp0],   0x30(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp6],   0x60(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp5],   0x70(%[ptmp])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[addr5]                \n\t"
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr5]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[stride]               \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr4])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr0]                \n\t"
+        "gsldlc1    %[ftmp4],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr6])                          \n\t"
+        "gsldlc1    %[ftmp6],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[stride]               \n\t"
+        "uld        %[all64],   0x00(%[addr4])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr0]                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr3]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp8],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "sdc1       %[ftmp3],   0x08(%[ptmp])                           \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "sdc1       %[ftmp2],   0x28(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp2],   0x08(%[ptmp])                           \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp7],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp0],   0x08(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp5],   0x18(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp7],   0x48(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp4],   0x58(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp8],   0x28(%[ptmp])                           \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp6],       %[ftmp1]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "sdc1       %[ftmp3],   0x28(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp0],   0x38(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp6],   0x68(%[ptmp])                           \n\t"
+        "sdc1       %[ftmp5],   0x78(%[ptmp])                           \n\t"
+        PTR_S      "%[addr1],   0x00(%[pdat])                           \n\t"
+        PTR_S      "%[addr2],   0x08(%[pdat])                           \n\t"
+        PTR_S      "%[addr0],   0x10(%[pdat])                           \n\t"
+        PTR_S      "%[addr3],   0x18(%[pdat])                           \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),
+          [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [ptmp]"r"(ptmp),                  [pdat]"r"(pdat)
+        : "memory"
     );
 
     ff_deblock_v_luma_intra_8_mmi((uint8_t *) &ptmp[8], 0x10, alpha, beta);
 
     __asm__ volatile (
-        "ld $10, 0x00+%[pdat]                           \r\n"
-        "ld $11, 0x08+%[pdat]                           \r\n"
-        "ld $12, 0x10+%[pdat]                           \r\n"
-        "ld $13, 0x18+%[pdat]                           \r\n"
-        "daddu $9, $10, $11                             \r\n"
-        "ldc1 $f0, 0x8+%[ptmp]                          \r\n"
-        "ldc1 $f2, 0x18+%[ptmp]                         \r\n"
-        "ldc1 $f4, 0x28+%[ptmp]                         \r\n"
-        "ldc1 $f6, 0x38+%[ptmp]                         \r\n"
-        "ldc1 $f8, 0x48+%[ptmp]                         \r\n"
-        "ldc1 $f10, 0x58+%[ptmp]                        \r\n"
-        "ldc1 $f12, 0x68+%[ptmp]                        \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "ldc1 $f16, 0x78+%[ptmp]                        \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "gssdlc1 $f6, 0x7($10)                          \r\n"
-        "gssdrc1 $f6, 0x0($10)                          \r\n"
-        "daddu $8, $10, $12                             \r\n"
-        "punpckhhw $f6, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "punpckhhw $f12, $f14, $f2                      \r\n"
-        "punpcklhw $f14, $f14, $f2                      \r\n"
-        "gssdlc1 $f4, 0x7($8)                           \r\n"
-        "gssdrc1 $f4, 0x0($8)                           \r\n"
-        "gsldlc1 $f4, 0x7($10)                          \r\n"
-        "gsldrc1 $f4, 0x0($10)                          \r\n"
-        "punpckhhw $f2, $f4, $f10                       \r\n"
-        "punpcklhw $f4, $f4, $f10                       \r\n"
-        "punpckhwd $f10, $f0, $f8                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpckhwd $f8, $f14, $f4                       \r\n"
-        "punpcklwd $f14, $f14, $f4                      \r\n"
-        "daddu $8, $10, %[stride]                       \r\n"
-        "gssdlc1 $f0, 0x7($10)                          \r\n"
-        "gssdrc1 $f0, 0x0($10)                          \r\n"
-        "daddu $14, $9, %[stride]                       \r\n"
-        "gssdlc1 $f10, 0x7($8)                          \r\n"
-        "gssdrc1 $f10, 0x0($8)                          \r\n"
-        "daddu $8, $9, $12                              \r\n"
-        "gssdlc1 $f14, 0x7($14)                         \r\n"
-        "gssdrc1 $f14, 0x0($14)                         \r\n"
-        "daddu $14, $10, $12                            \r\n"
-        "gssdlc1 $f8, 0x7($8)                           \r\n"
-        "gssdrc1 $f8, 0x0($8)                           \r\n"
-        "gsldlc1 $f16, 0x7($14)                         \r\n"
-        "gsldrc1 $f16, 0x0($14)                         \r\n"
-        "daddu $8, $10, $12                             \r\n"
-        "punpckhwd $f0, $f6, $f16                       \r\n"
-        "punpcklwd $f6, $f6, $f16                       \r\n"
-        "punpckhwd $f10, $f12, $f2                      \r\n"
-        "punpcklwd $f12, $f12, $f2                      \r\n"
-        "gssdlc1 $f6, 0x7($8)                           \r\n"
-        "gssdrc1 $f6, 0x0($8)                           \r\n"
-        "daddu $8, $9, $11                              \r\n"
-        "gssdlc1 $f0, 0x7($9)                           \r\n"
-        "gssdrc1 $f0, 0x0($9)                           \r\n"
-        "daddu $14, $9, $13                             \r\n"
-        "gssdlc1 $f12, 0x7($8)                          \r\n"
-        "gssdrc1 $f12, 0x0($8)                          \r\n"
-        "daddu $8, $13, $13                             \r\n"
-        "gssdlc1 $f10, 0x7($14)                         \r\n"
-        "gssdrc1 $f10, 0x0($14)                         \r\n"
-        "dsubu $10, $10, $8                             \r\n"
-        "dsubu $9, $9, $8                               \r\n"
-        "ldc1 $f0, 0x0+%[ptmp]                          \r\n"
-        "ldc1 $f2, 0x10+%[ptmp]                         \r\n"
-        "ldc1 $f4, 0x20+%[ptmp]                         \r\n"
-        "ldc1 $f6, 0x30+%[ptmp]                         \r\n"
-        "ldc1 $f8, 0x40+%[ptmp]                         \r\n"
-        "ldc1 $f10, 0x50+%[ptmp]                        \r\n"
-        "ldc1 $f12, 0x60+%[ptmp]                        \r\n"
-        "punpckhbh $f14, $f0, $f2                       \r\n"
-        "punpcklbh $f0, $f0, $f2                        \r\n"
-        "punpckhbh $f2, $f4, $f6                        \r\n"
-        "punpcklbh $f4, $f4, $f6                        \r\n"
-        "punpckhbh $f6, $f8, $f10                       \r\n"
-        "punpcklbh $f8, $f8, $f10                       \r\n"
-        "ldc1 $f16, 0x70+%[ptmp]                        \r\n"
-        "punpckhbh $f10, $f12, $f16                     \r\n"
-        "punpcklbh $f12, $f12, $f16                     \r\n"
-        "gssdlc1 $f6, 0x7($10)                          \r\n"
-        "gssdrc1 $f6, 0x0($10)                          \r\n"
-        "daddu $8, $10, $12                             \r\n"
-        "punpckhhw $f6, $f0, $f4                        \r\n"
-        "punpcklhw $f0, $f0, $f4                        \r\n"
-        "punpckhhw $f4, $f8, $f12                       \r\n"
-        "punpcklhw $f8, $f8, $f12                       \r\n"
-        "punpckhhw $f12, $f14, $f2                      \r\n"
-        "punpcklhw $f14, $f14, $f2                      \r\n"
-        "gssdlc1 $f4, 0x7($8)                           \r\n"
-        "gssdrc1 $f4, 0x0($8)                           \r\n"
-        "gsldlc1 $f4, 0x7($10)                          \r\n"
-        "gsldrc1 $f4, 0x0($10)                          \r\n"
-        "punpckhhw $f2, $f4, $f10                       \r\n"
-        "punpcklhw $f4, $f4, $f10                       \r\n"
-        "punpckhwd $f10, $f0, $f8                       \r\n"
-        "punpcklwd $f0, $f0, $f8                        \r\n"
-        "punpckhwd $f8, $f14, $f4                       \r\n"
-        "punpcklwd $f14, $f14, $f4                      \r\n"
-        "daddu $8, $10, %[stride]                       \r\n"
-        "gssdlc1 $f0, 0x7($10)                          \r\n"
-        "gssdrc1 $f0, 0x0($10)                          \r\n"
-        "daddu $14, $9, %[stride]                       \r\n"
-        "gssdlc1 $f10, 0x7($8)                          \r\n"
-        "gssdrc1 $f10, 0x0($8)                          \r\n"
-        "daddu $8, $9, $12                              \r\n"
-        "gssdlc1 $f14, 0x7($14)                         \r\n"
-        "gssdrc1 $f14, 0x0($14)                         \r\n"
-        "daddu $14, $10, $12                            \r\n"
-        "gssdlc1 $f8, 0x7($8)                           \r\n"
-        "gssdrc1 $f8, 0x0($8)                           \r\n"
-        "gsldlc1 $f16, 0x7($14)                         \r\n"
-        "gsldrc1 $f16, 0x0($14)                         \r\n"
-        "daddu $8, $10, $12                             \r\n"
-        "punpckhwd $f0, $f6, $f16                       \r\n"
-        "punpcklwd $f6, $f6, $f16                       \r\n"
-        "punpckhwd $f10, $f12, $f2                      \r\n"
-        "punpcklwd $f12, $f12, $f2                      \r\n"
-        "gssdlc1 $f6, 0x7($8)                           \r\n"
-        "gssdrc1 $f6, 0x0($8)                           \r\n"
-        "daddu $8, $9, $11                              \r\n"
-        "gssdlc1 $f0, 0x7($9)                           \r\n"
-        "gssdrc1 $f0, 0x0($9)                           \r\n"
-        "daddu $14, $9, $13                             \r\n"
-        "gssdlc1 $f12, 0x7($8)                          \r\n"
-        "gssdrc1 $f12, 0x0($8)                          \r\n"
-        "gssdlc1 $f10, 0x7($14)                         \r\n"
-        "gssdrc1 $f10, 0x0($14)                         \r\n"
-        ::[pix]"r"(pix),[stride]"r"((uint64_t)stride),[ptmp]"m"(ptmp[0]),
-          [pdat]"m"(pdat[0])
-        : "$8","$9","$10","$11","$12","$13","$14","$f0","$f2","$f4","$f6",
-          "$f8","$f10","$f12","$f14","$f16"
+        PTR_L      "%[addr1],   0x00(%[pdat])                           \n\t"
+        PTR_L      "%[addr2],   0x08(%[pdat])                           \n\t"
+        PTR_L      "%[addr0],   0x10(%[pdat])                           \n\t"
+        PTR_L      "%[addr3],   0x18(%[pdat])                           \n\t"
+        PTR_ADDU   "%[addr4],   %[addr1],       %[addr2]                \n\t"
+        "ldc1       %[ftmp0],   0x08(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp1],   0x18(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp2],   0x28(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp3],   0x38(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp4],   0x48(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp5],   0x58(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp6],   0x68(%[ptmp])                           \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "ldc1       %[ftmp8],   0x78(%[ptmp])                           \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gssdrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp2],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp2],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "punpckhhw  %[ftmp1],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp7],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[stride]               \n\t"
+        "gssdlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+        "gssdlc1    %[ftmp7],   0x07(%[addr6])                          \n\t"
+        "gssdrc1    %[ftmp7],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "gssdlc1    %[ftmp4],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp8],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr6])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+        "dmfc1      %[all64],   %[ftmp7]                                \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp6],       %[ftmp1]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp3],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp3],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[addr4])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr3]                \n\t"
+        "gssdlc1    %[ftmp6],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp6],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr3],       %[addr3]                \n\t"
+        "gssdlc1    %[ftmp5],   0x07(%[addr6])                          \n\t"
+        "gssdrc1    %[ftmp5],   0x00(%[addr6])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr3]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr3],       %[addr3]                \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+#endif
+        PTR_SUBU   "%[addr1],   %[addr1],       %[addr5]                \n\t"
+        PTR_SUBU   "%[addr4],   %[addr4],       %[addr5]                \n\t"
+        "ldc1       %[ftmp0],   0x00(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp1],   0x10(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp2],   0x20(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp3],   0x30(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp4],   0x40(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp5],   0x50(%[ptmp])                           \n\t"
+        "ldc1       %[ftmp6],   0x60(%[ptmp])                           \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp4],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "ldc1       %[ftmp8],   0x70(%[ptmp])                           \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp6],       %[ftmp8]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gssdrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpcklhw  %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "punpckhhw  %[ftmp2],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpcklhw  %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "punpckhhw  %[ftmp6],   %[ftmp7],       %[ftmp1]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp2],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp2],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "punpckhhw  %[ftmp1],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpcklhw  %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhwd  %[ftmp4],   %[ftmp7],       %[ftmp2]                \n\t"
+        "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        PTR_ADDU   "%[addr5],   %[addr1],       %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[stride]               \n\t"
+        "gssdlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+        "gssdlc1    %[ftmp7],   0x07(%[addr6])                          \n\t"
+        "gssdrc1    %[ftmp7],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "gssdlc1    %[ftmp4],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp4],   0x00(%[addr5])                          \n\t"
+        "gsldlc1    %[ftmp8],   0x07(%[addr6])                          \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[addr6])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr0]                \n\t"
+        "dmfc1      %[all64],   %[ftmp7]                                \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr1],       %[addr0]                \n\t"
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "uld        %[all64],   0x00(%[addr6])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+        PTR_ADDU   "%[addr5],   %[addr1],       %[addr0]                \n\t"
+        "punpckhwd  %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "punpckhwd  %[ftmp5],   %[ftmp6],       %[ftmp1]                \n\t"
+        "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp3],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp3],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[addr4])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr3]                \n\t"
+        "gssdlc1    %[ftmp6],   0x07(%[addr5])                          \n\t"
+        "gssdrc1    %[ftmp6],   0x00(%[addr5])                          \n\t"
+        "gssdlc1    %[ftmp5],   0x07(%[addr6])                          \n\t"
+        "gssdrc1    %[ftmp5],   0x00(%[addr6])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr4],       %[addr2]                \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr4])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr4],       %[addr3]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),
+          [all64]"=&r"(all64)
+        : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
+          [ptmp]"r"(ptmp),                  [pdat]"r"(pdat)
+        : "memory"
     );
 }
diff --git a/libavcodec/mips/h264pred_init_mips.c b/libavcodec/mips/h264pred_init_mips.c
index 93a2409..c33d8f7 100644
--- a/libavcodec/mips/h264pred_init_mips.c
+++ b/libavcodec/mips/h264pred_init_mips.c
@@ -115,23 +115,22 @@ static av_cold void h264_pred_init_mmi(H264PredContext *h, int codec_id,
         h->pred8x8l [TOP_DC_PRED            ] = ff_pred8x8l_top_dc_8_mmi;
         h->pred8x8l [DC_PRED                ] = ff_pred8x8l_dc_8_mmi;
 
+#if ARCH_MIPS64
         switch (codec_id) {
         case AV_CODEC_ID_SVQ3:
             h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_svq3_8_mmi;
-            ;
             break;
         case AV_CODEC_ID_RV40:
             h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_rv40_8_mmi;
-            ;
             break;
         case AV_CODEC_ID_VP7:
         case AV_CODEC_ID_VP8:
-            ;
             break;
         default:
             h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_h264_8_mmi;
             break;
         }
+#endif
 
         if (codec_id == AV_CODEC_ID_SVQ3 || codec_id == AV_CODEC_ID_H264) {
             if (chroma_format_idc == 1) {
diff --git a/libavcodec/mips/h264pred_mmi.c b/libavcodec/mips/h264pred_mmi.c
index e949d11..a398910 100644
--- a/libavcodec/mips/h264pred_mmi.c
+++ b/libavcodec/mips/h264pred_mmi.c
@@ -23,87 +23,155 @@
  */
 
 #include "h264pred_mips.h"
+#include "libavcodec/bit_depth_template.c"
+#include "libavutil/mips/asmdefs.h"
+#include "constants.h"
 
 void ff_pred16x16_vertical_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    double ftmp[2];
+    uint64_t tmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "dli $8, 16                         \r\n"
-        "gsldlc1 $f2, 7(%[srcA])            \r\n"
-        "gsldrc1 $f2, 0(%[srcA])            \r\n"
-        "gsldlc1 $f4, 15(%[srcA])           \r\n"
-        "gsldrc1 $f4, 8(%[srcA])            \r\n"
-        "1:                                 \r\n"
-        "gssdlc1 $f2, 7(%[src])             \r\n"
-        "gssdrc1 $f2, 0(%[src])             \r\n"
-        "gssdlc1 $f4, 15(%[src])            \r\n"
-        "gssdrc1 $f4, 8(%[src])             \r\n"
-        "daddu %[src], %[src], %[stride]    \r\n"
-        "daddi $8, $8, -1                   \r\n"
-        "bnez $8, 1b                        \r\n"
-        : [src]"+&r"(src)
-        : [stride]"r"(stride),[srcA]"r"(src-stride)
-        : "$8","$f2","$f4"
+        "dli        %[tmp0],    0x08                                    \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[srcA])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x0f(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp1],   0x08(%[srcA])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        "uld        %[all64],   0x08(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+#endif
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdlc1    %[ftmp1],   0x0f(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x08(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdlc1    %[ftmp1],   0x0f(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x08(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x08(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x08(%[src])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [tmp0]"=&r"(tmp[0]),
+          [src]"+&r"(src),
+          [all64]"=&r"(all64)
+        : [stride]"r"((mips_reg)stride),    [srcA]"r"((mips_reg)(src-stride))
+        : "memory"
     );
 }
 
 void ff_pred16x16_horizontal_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    uint64_t tmp[3];
+    mips_reg addr[2];
+
     __asm__ volatile (
-        "daddiu $2, %[src], -1              \r\n"
-        "daddu $3, %[src], $0               \r\n"
-        "dli $6, 0x10                       \r\n"
-        "1:                                 \r\n"
-        "lbu $4, 0($2)                      \r\n"
-        "dmul $5, $4, %[ff_pb_1]            \r\n"
-        "sdl $5, 7($3)                      \r\n"
-        "sdr $5, 0($3)                      \r\n"
-        "sdl $5, 15($3)                     \r\n"
-        "sdr $5, 8($3)                      \r\n"
-        "daddu $2, %[stride]                \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddiu $6, -1                      \r\n"
-        "bnez $6, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[ff_pb_1]"r"(ff_pb_1)
-        : "$2","$3","$4","$5","$6"
+        PTR_ADDI   "%[addr0],   %[src],         -0x01                   \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         $0                      \n\t"
+        "dli        %[tmp2],    0x08                                    \n\t"
+        "1:                                                             \n\t"
+        "lbu        %[tmp0],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp1],    %[tmp0],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp1],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp1],    0x00(%[addr1])                          \n\t"
+        "swl        %[tmp1],    0x0f(%[addr1])                          \n\t"
+        "swr        %[tmp1],    0x08(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "lbu        %[tmp0],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp1],    %[tmp0],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp1],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp1],    0x00(%[addr1])                          \n\t"
+        "swl        %[tmp1],    0x0f(%[addr1])                          \n\t"
+        "swr        %[tmp1],    0x08(%[addr1])                          \n\t"
+        "daddi      %[tmp2],    %[tmp2],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "bnez       %[tmp2],    1b                                      \n\t"
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride),
+          [ff_pb_1]"r"(ff_pb_1)
+        : "memory"
     );
 }
 
 void ff_pred16x16_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    uint64_t tmp[4];
+    mips_reg addr[2];
+
     __asm__ volatile (
-        "daddiu $2, %[src], -1              \r\n"
-        "dli $6, 0x10                       \r\n"
-        "xor $8, $8, $8                     \r\n"
-        "1:                                 \r\n"
-        "lbu $4, 0($2)                      \r\n"
-        "daddu $8, $8, $4                   \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "daddiu $6, $6, -1                  \r\n"
-        "bnez $6, 1b                        \r\n"
-        "dli $6, 0x10                       \r\n"
-        "negu $3, %[stride]                 \r\n"
-        "daddu $2, %[src], $3               \r\n"
-        "2:                                 \r\n"
-        "lbu $4, 0($2)                      \r\n"
-        "daddu $8, $8, $4                   \r\n"
-        "daddiu $2, $2, 1                   \r\n"
-        "daddiu $6, $6, -1                  \r\n"
-        "bnez $6, 2b                        \r\n"
-        "daddiu $8, $8, 0x10                \r\n"
-        "dsra $8, 5                         \r\n"
-        "dmul $5, $8, %[ff_pb_1]            \r\n"
-        "daddu $2, %[src], $0               \r\n"
-        "dli $6, 0x10                       \r\n"
-        "3:                                 \r\n"
-        "sdl $5, 7($2)                      \r\n"
-        "sdr $5, 0($2)                      \r\n"
-        "sdl $5, 15($2)                     \r\n"
-        "sdr $5, 8($2)                      \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "daddiu $6, $6, -1                  \r\n"
-        "bnez $6, 3b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[ff_pb_1]"r"(ff_pb_1)
-        : "$2","$3","$4","$5","$6","$8"
+        PTR_ADDI   "%[addr0],   %[src],         -0x01                   \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "xor        %[tmp3],    %[tmp3],        %[tmp3]                 \n\t"
+        "1:                                                             \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "daddu      %[tmp3],    %[tmp3],        %[tmp1]                 \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        "daddu      %[tmp3],    %[tmp3],        %[tmp1]                 \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+
+        "dli        %[tmp0],    0x08                                    \n\t"
+        PTR_SUBU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "2:                                                             \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "daddu      %[tmp3],    %[tmp3],        %[tmp1]                 \n\t"
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x01                    \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        "daddu      %[tmp3],    %[tmp3],        %[tmp1]                 \n\t"
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x01                    \n\t"
+        "bnez       %[tmp0],    2b                                      \n\t"
+
+        "daddiu     %[tmp3],    %[tmp3],        0x10                    \n\t"
+        "dsra       %[tmp3],    0x05                                    \n\t"
+        "dmul       %[tmp2],    %[tmp3],        %[ff_pb_1]              \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         $0                      \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "3:                                                             \n\t"
+        "swl        %[tmp2],    0x07(%[addr0])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr0])                          \n\t"
+        "swl        %[tmp2],    0x0f(%[addr0])                          \n\t"
+        "swr        %[tmp2],    0x08(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "swl        %[tmp2],    0x07(%[addr0])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr0])                          \n\t"
+        "swl        %[tmp2],    0x0f(%[addr0])                          \n\t"
+        "swr        %[tmp2],    0x08(%[addr0])                          \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "bnez       %[tmp0],    3b                                      \n\t"
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),              [tmp3]"=&r"(tmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride),
+          [ff_pb_1]"r"(ff_pb_1)
+        : "memory"
     );
 }
 
@@ -111,76 +179,127 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
         int has_topright, ptrdiff_t stride)
 {
     uint32_t dc;
+    double ftmp[11];
+    mips_reg tmp[3];
+    mips_reg addr[1];
+    uint64_t all64;
 
     __asm__ volatile (
-        "ldl $8, 7(%[srcA])                 \r\n"
-        "ldr $8, 0(%[srcA])                 \r\n"
-        "ldl $9, 7(%[src0])                 \r\n"
-        "ldr $9, 0(%[src0])                 \r\n"
-        "ldl $10, 7(%[src1])                \r\n"
-        "ldr $10, 0(%[src1])                \r\n"
-        "dmtc1 $8, $f2                      \r\n"
-        "dmtc1 $9, $f4                      \r\n"
-        "dmtc1 $10, $f6                     \r\n"
-        "dmtc1 $0, $f0                      \r\n"
-        "punpcklbh $f8, $f2, $f0            \r\n"
-        "punpckhbh $f10, $f2, $f0           \r\n"
-        "punpcklbh $f12, $f4, $f0           \r\n"
-        "punpckhbh $f14, $f4, $f0           \r\n"
-        "punpcklbh $f16, $f6, $f0           \r\n"
-        "punpckhbh $f18, $f6, $f0           \r\n"
-        "bnez %[has_topleft], 1f            \r\n"
-        "pinsrh_0 $f8, $f8, $f12            \r\n"
-        "1:                                 \r\n"
-        "bnez %[has_topright], 2f           \r\n"
-        "pinsrh_3 $f18, $f18, $f14          \r\n"
-        "2:                                 \r\n"
-        "daddiu $8, $0, 2                   \r\n"
-        "dmtc1 $8, $f20                     \r\n"
-        "pshufh $f22, $f20, $f0             \r\n"
-        "pmullh $f12, $f12, $f22            \r\n"
-        "pmullh $f14, $f14, $f22            \r\n"
-        "paddh $f8, $f8, $f12               \r\n"
-        "paddh $f10, $f10, $f14             \r\n"
-        "paddh $f8, $f8, $f16               \r\n"
-        "paddh $f10, $f10, $f18             \r\n"
-        "paddh $f8, $f8, $f22               \r\n"
-        "paddh $f10, $f10, $f22             \r\n"
-        "psrah $f8, $f8, $f20               \r\n"
-        "psrah $f10, $f10, $f20             \r\n"
-        "packushb $f4, $f8, $f10            \r\n"
-        "biadd $f2, $f4                     \r\n"
-        "mfc1 $9, $f2                       \r\n"
-        "addiu $9, $9, 4                    \r\n"
-        "dsrl $9, $9, 3                     \r\n"
-        "mul %[dc], $9, %[ff_pb_1]          \r\n"
-        : [dc]"=r"(dc)
-        : [srcA]"r"(src-stride-1),[src0]"r"(src-stride),
-          [src1]"r"(src-stride+1),[has_topleft]"r"(has_topleft),
-          [has_topright]"r"(has_topright),[ff_pb_1]"r"(ff_pb_1)
-        : "$8","$9","$10","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20","$f22"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp10],  0x07(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp10],  0x00(%[srcA])                           \n\t"
+        "gsldlc1    %[ftmp9],   0x07(%[src0])                           \n\t"
+        "gsldrc1    %[ftmp9],   0x00(%[src0])                           \n\t"
+        "gsldlc1    %[ftmp8],   0x07(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp8],   0x00(%[src1])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp10]                               \n\t"
+        "uld        %[all64],   0x00(%[src0])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp9]                                \n\t"
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp8]                                \n\t"
+#endif
+
+        "punpcklbh  %[ftmp7],   %[ftmp10],      %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp6],   %[ftmp10],      %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp9],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp4],   %[ftmp9],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp8],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp2],   %[ftmp8],       %[ftmp0]                \n\t"
+        "bnez       %[has_topleft],             1f                      \n\t"
+        "pinsrh_0   %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+
+        "1:                                                             \n\t"
+        "bnez       %[has_topright],            2f                      \n\t"
+        "pinsrh_3   %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+
+        "2:                                                             \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pmullh     %[ftmp5],   %[ftmp5],       %[ff_pw_2]              \n\t"
+        "pmullh     %[ftmp4],   %[ftmp4],       %[ff_pw_2]              \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ff_pw_2]              \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ff_pw_2]              \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp9],   %[ftmp7],       %[ftmp6]                \n\t"
+        "biadd      %[ftmp10],  %[ftmp9]                                \n\t"
+        "mfc1       %[tmp1],    %[ftmp10]                               \n\t"
+        "addiu      %[tmp1],    %[tmp1],        0x04                    \n\t"
+        "srl        %[tmp1],    %[tmp1],        0x03                    \n\t"
+        "mul        %[dc],      %[tmp1],        %[ff_pb_1]              \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [all64]"=&r"(all64),
+          [dc]"=r"(dc)
+        : [srcA]"r"((mips_reg)(src-stride-1)),
+          [src0]"r"((mips_reg)(src-stride)),
+          [src1]"r"((mips_reg)(src-stride+1)),
+          [has_topleft]"r"(has_topleft),    [has_topright]"r"(has_topright),
+          [ff_pb_1]"r"(ff_pb_1),            [ff_pw_2]"f"(ff_pw_2)
+        : "memory"
     );
 
     __asm__ volatile (
-        "dli $8, 8                          \r\n"
-        "1:                                 \r\n"
-        "punpcklwd $f2, %[dc], %[dc]        \r\n"
-        "gssdlc1 $f2, 7(%[src])             \r\n"
-        "gssdrc1 $f2, 0(%[src])             \r\n"
-        "daddu %[src], %[src], %[stride]    \r\n"
-        "daddi $8, $8, -1                   \r\n"
-        "bnez $8, 1b                        \r\n"
-        : [src]"+&r"(src)
-        : [dc]"f"(dc),[stride]"r"(stride)
-        : "$8","$f2"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "punpcklwd  %[ftmp0],   %[dc],          %[dc]                   \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdxc1    %[ftmp0],   0x00(%[src],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+#endif
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdxc1    %[ftmp0],   0x00(%[src],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64),
+          [src]"+&r"(src)
+        : [dc]"f"(dc),                      [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
 
-void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft,
-        int has_topright, ptrdiff_t stride)
+void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft, int has_topright,
+        ptrdiff_t stride)
 {
     uint32_t dc, dc1, dc2;
+    double ftmp[14];
+    mips_reg tmp[1];
+    mips_reg addr[1];
+    uint64_t all64;
 
     const int l0 = ((has_topleft ? src[-1+-1*stride] : src[-1+0*stride]) + 2*src[-1+0*stride] + src[-1+1*stride] + 2) >> 2;
     const int l1 = (src[-1+0*stride] + 2*src[-1+1*stride] + src[-1+2*stride] + 2) >> 2;
@@ -192,140 +311,222 @@ void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft,
     const int l7 = (src[-1+6*stride] + 2*src[-1+7*stride] + src[-1+7*stride] + 2) >> 2;
 
     __asm__ volatile (
-        "ldl $8, 7(%[srcA])                 \r\n"
-        "ldr $8, 0(%[srcA])                 \r\n"
-        "ldl $9, 7(%[src0])                 \r\n"
-        "ldr $9, 0(%[src0])                 \r\n"
-        "ldl $10, 7(%[src1])                \r\n"
-        "ldr $10, 0(%[src1])                \r\n"
-        "dmtc1 $8, $f2                      \r\n"
-        "dmtc1 $9, $f4                      \r\n"
-        "dmtc1 $10, $f6                     \r\n"
-        "dmtc1 $0, $f0                      \r\n"
-        "punpcklbh $f8, $f2, $f0            \r\n"
-        "punpckhbh $f10, $f2, $f0           \r\n"
-        "punpcklbh $f12, $f4, $f0           \r\n"
-        "punpckhbh $f14, $f4, $f0           \r\n"
-        "punpcklbh $f16, $f6, $f0           \r\n"
-        "punpckhbh $f18, $f6, $f0           \r\n"
-        "daddiu $8, $0, 3                   \r\n"
-        "dmtc1 $8, $f20                     \r\n"
-        "pshufh $f28, $f10, $f20            \r\n"
-        "pshufh $f30, $f18, $f20            \r\n"
-        "pinsrh_3 $f10, $f10, $f30          \r\n"
-        "pinsrh_3 $f18, $f18, $f28          \r\n"
-        "bnez %[has_topleft], 1f            \r\n"
-        "pinsrh_0 $f8, $f8, $f12            \r\n"
-        "1:                                 \r\n"
-        "bnez %[has_topright], 2f           \r\n"
-        "pshufh $f30, $f14, $f20            \r\n"
-        "pinsrh_3 $f10, $f10, $f30          \r\n"
-        "2:                                 \r\n"
-        "daddiu $8, $0, 2                   \r\n"
-        "dmtc1 $8, $f20                     \r\n"
-        "pshufh $f22, $f20, $f0             \r\n"
-        "pmullh $f12, $f12, $f22            \r\n"
-        "pmullh $f14, $f14, $f22            \r\n"
-        "paddh $f8, $f8, $f12               \r\n"
-        "paddh $f10, $f10, $f14             \r\n"
-        "paddh $f8, $f8, $f16               \r\n"
-        "paddh $f10, $f10, $f18             \r\n"
-        "paddh $f8, $f8, $f22               \r\n"
-        "paddh $f10, $f10, $f22             \r\n"
-        "psrah $f8, $f8, $f20               \r\n"
-        "psrah $f10, $f10, $f20             \r\n"
-        "packushb $f4, $f8, $f10            \r\n"
-        "biadd $f2, $f4                     \r\n"
-        "mfc1 %[dc2], $f2                   \r\n"
-        : [dc2]"=r"(dc2)
-        : [srcA]"r"(src-stride-1),[src0]"r"(src-stride),
-          [src1]"r"(src-stride+1),[has_topleft]"r"(has_topleft),
-          [has_topright]"r"(has_topright)
-        : "$8","$9","$10","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20","$f22"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp4],   0x07(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[srcA])                           \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[src0])                           \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src0])                           \n\t"
+        "gsldlc1    %[ftmp6],   0x07(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp6],   0x00(%[src1])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[src0])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x03                                    \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp11],  %[ftmp6],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp12],  %[ftmp6],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp3],   %[ftmp8],       %[ftmp1]                \n\t"
+        "pshufh     %[ftmp13],  %[ftmp12],      %[ftmp1]                \n\t"
+        "pinsrh_3   %[ftmp8],   %[ftmp8],       %[ftmp13]               \n\t"
+        "pinsrh_3   %[ftmp12],  %[ftmp12],      %[ftmp3]                \n\t"
+        "bnez       %[has_topleft],             1f                      \n\t"
+        "pinsrh_0   %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+
+        "1:                                                             \n\t"
+        "bnez       %[has_topright],            2f                      \n\t"
+        "pshufh     %[ftmp13],  %[ftmp10],      %[ftmp1]                \n\t"
+        "pinsrh_3   %[ftmp8],   %[ftmp8],       %[ftmp13]               \n\t"
+
+        "2:                                                             \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp2]                \n\t"
+        "pmullh     %[ftmp10],  %[ftmp10],      %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp12]               \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "psrah      %[ftmp8],   %[ftmp8],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp5],   %[ftmp7],       %[ftmp8]                \n\t"
+        "biadd      %[ftmp4],   %[ftmp5]                                \n\t"
+        "mfc1       %[dc2],     %[ftmp4]                                \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [ftmp12]"=&f"(ftmp[12]),          [ftmp13]"=&f"(ftmp[13]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [dc2]"=r"(dc2)
+        : [srcA]"r"((mips_reg)(src-stride-1)),
+          [src0]"r"((mips_reg)(src-stride)),
+          [src1]"r"((mips_reg)(src-stride+1)),
+          [has_topleft]"r"(has_topleft),    [has_topright]"r"(has_topright)
+        : "memory"
     );
 
     dc1 = l0+l1+l2+l3+l4+l5+l6+l7;
     dc = ((dc1+dc2+8)>>4)*0x01010101U;
 
     __asm__ volatile (
-        "dli $8, 8                          \r\n"
-        "1:                                 \r\n"
-        "punpcklwd $f2, %[dc], %[dc]        \r\n"
-        "gssdlc1 $f2, 7(%[src])             \r\n"
-        "gssdrc1 $f2, 0(%[src])             \r\n"
-        "daddu %[src], %[src], %[stride]    \r\n"
-        "daddi $8, $8, -1                   \r\n"
-        "bnez $8, 1b                        \r\n"
-        : [src]"+&r"(src)
-        : [dc]"f"(dc),[stride]"r"(stride)
-        : "$8","$f2"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "punpcklwd  %[ftmp0],   %[dc],          %[dc]                   \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdxc1    %[ftmp0],   0x00(%[src],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+#endif
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdxc1    %[ftmp0],   0x00(%[src],    %[stride])              \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64),
+          [src]"+&r"(src)
+        : [dc]"f"(dc),                      [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
 
 void ff_pred8x8l_vertical_8_mmi(uint8_t *src, int has_topleft,
         int has_topright, ptrdiff_t stride)
 {
+    double ftmp[12];
+    mips_reg tmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "ldl $8, 7(%[srcA])                 \r\n"
-        "ldr $8, 0(%[srcA])                 \r\n"
-        "ldl $9, 7(%[src0])                 \r\n"
-        "ldr $9, 0(%[src0])                 \r\n"
-        "ldl $10, 7(%[src1])                \r\n"
-        "ldr $10, 0(%[src1])                \r\n"
-        "dmtc1 $8, $f2                      \r\n"
-        "dmtc1 $9, $f4                      \r\n"
-        "dmtc1 $10, $f6                     \r\n"
-        "dmtc1 $0, $f0                      \r\n"
-        "punpcklbh $f8, $f2, $f0            \r\n"
-        "punpckhbh $f10, $f2, $f0           \r\n"
-        "punpcklbh $f12, $f4, $f0           \r\n"
-        "punpckhbh $f14, $f4, $f0           \r\n"
-        "punpcklbh $f16, $f6, $f0           \r\n"
-        "punpckhbh $f18, $f6, $f0           \r\n"
-        "bnez %[has_topleft], 1f            \r\n"
-        "pinsrh_0 $f8, $f8, $f12            \r\n"
-        "1:                                 \r\n"
-        "bnez %[has_topright], 2f           \r\n"
-        "pinsrh_3 $f18, $f18, $f14          \r\n"
-        "2:                                 \r\n"
-        "daddiu $8, $0, 2                   \r\n"
-        "dmtc1 $8, $f20                     \r\n"
-        "pshufh $f22, $f20, $f0             \r\n"
-        "pmullh $f12, $f12, $f22            \r\n"
-        "pmullh $f14, $f14, $f22            \r\n"
-        "paddh $f8, $f8, $f12               \r\n"
-        "paddh $f10, $f10, $f14             \r\n"
-        "paddh $f8, $f8, $f16               \r\n"
-        "paddh $f10, $f10, $f18             \r\n"
-        "paddh $f8, $f8, $f22               \r\n"
-        "paddh $f10, $f10, $f22             \r\n"
-        "psrah $f8, $f8, $f20               \r\n"
-        "psrah $f10, $f10, $f20             \r\n"
-        "packushb $f4, $f8, $f10            \r\n"
-        "sdc1 $f4, 0(%[src])                \r\n"
-        : [src]"=r"(src)
-        : [srcA]"r"(src-stride-1),[src0]"r"(src-stride),
-          [src1]"r"(src-stride+1),[has_topleft]"r"(has_topleft),
-          [has_topright]"r"(has_topright)
-        : "$8","$9","$10","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20","$f22"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp3],   0x07(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[srcA])                           \n\t"
+        "gsldlc1    %[ftmp4],   0x07(%[src0])                           \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[src0])                           \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src1])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x00(%[src0])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "punpcklbh  %[ftmp6],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp8],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp10],  %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp11],  %[ftmp5],       %[ftmp0]                \n\t"
+        "bnez       %[has_topleft],             1f                      \n\t"
+        "pinsrh_0   %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+
+        "1:                                                             \n\t"
+        "bnez       %[has_topright],            2f                      \n\t"
+        "pinsrh_3   %[ftmp11],  %[ftmp11],      %[ftmp9]                \n\t"
+
+        "2:                                                             \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp10]               \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp6],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp4],   0x00(%[src])                            \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=r"(all64),
+          [src]"=r"(src)
+        : [srcA]"r"((mips_reg)(src-stride-1)),
+          [src0]"r"((mips_reg)(src-stride)),
+          [src1]"r"((mips_reg)(src-stride+1)),
+          [has_topleft]"r"(has_topleft),    [has_topright]"r"(has_topright)
+        : "memory"
     );
 
     __asm__ volatile (
-        "dli $8, 7                          \r\n"
-        "gsldlc1 $f2, 7(%[src])             \r\n"
-        "gsldrc1 $f2, 0(%[src])             \r\n"
-        "dadd %[src], %[src], %[stride]     \r\n"
-        "1:                                 \r\n"
-        "gssdlc1 $f2, 7(%[src])             \r\n"
-        "gssdrc1 $f2, 0(%[src])             \r\n"
-        "daddu %[src], %[src], %[stride]    \r\n"
-        "daddi $8, $8, -1                   \r\n"
-        "bnez $8, 1b                        \r\n"
-        : [src]"+&r"(src)
-        : [stride]"r"(stride)
-        : "$8","$f2"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [src]"+&r"(src)
+        : [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
 
@@ -335,446 +536,640 @@ void ff_pred4x4_dc_8_mmi(uint8_t *src, const uint8_t *topright,
     const int dc = (src[-stride] + src[1-stride] + src[2-stride]
                  + src[3-stride] + src[-1+0*stride] + src[-1+1*stride]
                  + src[-1+2*stride] + src[-1+3*stride] + 4) >>3;
+    uint64_t tmp[2];
+    mips_reg addr[2];
 
     __asm__ volatile (
-        "daddu $2, %[dc], $0                \r\n"
-        "dmul $3, $2, %[ff_pb_1]            \r\n"
-        "xor $4, $4, $4                     \r\n"
-        "gsswx $3, 0(%[src],$4)             \r\n"
-        "daddu $4, %[stride]                \r\n"
-        "gsswx $3, 0(%[src],$4)             \r\n"
-        "daddu $4, %[stride]                \r\n"
-        "gsswx $3, 0(%[src],$4)             \r\n"
-        "daddu $4, %[stride]                \r\n"
-        "gsswx $3, 0(%[src],$4)             \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[dc]"r"(dc),[ff_pb_1]"r"(ff_pb_1)
-        : "$2","$3","$4"
+        PTR_ADDU   "%[tmp0],    %[dc],          $0                      \n\t"
+        "dmul       %[tmp1],    %[tmp0],        %[ff_pb_1]              \n\t"
+        "xor        %[addr0],   %[addr0],       %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswx      %[tmp1],    0x00(%[src],    %[addr0])               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "gsswx      %[tmp1],    0x00(%[src],    %[addr0])               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "gsswx      %[tmp1],    0x00(%[src],    %[addr0])               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "gsswx      %[tmp1],    0x00(%[src],    %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[src],         %[addr0]                \n\t"
+        "sw         %[tmp1],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         %[addr0]                \n\t"
+        "sw         %[tmp1],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         %[addr0]                \n\t"
+        "sw         %[tmp1],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         %[addr0]                \n\t"
+        "sw         %[tmp1],    0x00(%[addr1])                          \n\t"
+#endif
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride),
+          [dc]"r"(dc),                      [ff_pb_1]"r"(ff_pb_1)
+        : "memory"
     );
 }
 
 void ff_pred8x8_vertical_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    uint64_t tmp[2];
+    mips_reg addr[2];
+
     __asm__ volatile (
-        "dsubu $2, %[src], %[stride]        \r\n"
-        "daddu $3, %[src], $0               \r\n"
-        "ldl $4, 7($2)                      \r\n"
-        "ldr $4, 0($2)                      \r\n"
-        "dli $5, 0x8                        \r\n"
-        "1:                                 \r\n"
-        "sdl $4, 7($3)                      \r\n"
-        "sdr $4, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddiu $5, -1                      \r\n"
-        "bnez $5, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride)
-        : "$2","$3","$4","$5"
+        PTR_SUBU   "%[addr0],   %[src],         %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         $0                      \n\t"
+        "ldl        %[tmp0],    0x07(%[addr0])                          \n\t"
+        "ldr        %[tmp0],    0x00(%[addr0])                          \n\t"
+        "dli        %[tmp1],    0x04                                    \n\t"
+        "1:                                                             \n\t"
+        "sdl        %[tmp0],    0x07(%[addr1])                          \n\t"
+        "sdr        %[tmp0],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[stride]                               \n\t"
+        "sdl        %[tmp0],    0x07(%[addr1])                          \n\t"
+        "sdr        %[tmp0],    0x00(%[addr1])                          \n\t"
+        "daddi      %[tmp1],    -0x01                                   \n\t"
+        PTR_ADDU   "%[addr1],   %[stride]                               \n\t"
+        "bnez       %[tmp1],    1b                                      \n\t"
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
 
 void ff_pred8x8_horizontal_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    uint64_t tmp[3];
+    mips_reg addr[2];
+
     __asm__ volatile (
-        "daddiu $2, %[src], -1              \r\n"
-        "daddu $3, %[src], $0               \r\n"
-        "dli $6, 0x8                        \r\n"
-        "1:                                 \r\n"
-        "lbu $4, 0($2)                      \r\n"
-        "dmul $5, $4, %[ff_pb_1]            \r\n"
-        "sdl $5, 7($3)                      \r\n"
-        "sdr $5, 0($3)                      \r\n"
-        "daddu $2, %[stride]                \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddiu $6, -1                      \r\n"
-        "bnez $6, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[ff_pb_1]"r"(ff_pb_1)
-        : "$2","$3","$4","$5","$6"
+        PTR_ADDI   "%[addr0],   %[src],         -0x01                   \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         $0                      \n\t"
+        "dli        %[tmp0],    0x04                                    \n\t"
+        "1:                                                             \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp2],    %[tmp1],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp2],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp2],    %[tmp1],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp2],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr1])                          \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride),
+          [ff_pb_1]"r"(ff_pb_1)
+        : "memory"
     );
 }
 
-static void ff_pred16x16_plane_compat_8_mmi(uint8_t *src, ptrdiff_t stride,
-        const int svq3, const int rv40)
+void ff_pred8x8_top_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    double ftmp[4];
+    uint64_t tmp[1];
+    mips_reg addr[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "negu $2, %[stride]                 \r\n"
-        "daddu $3, %[src], $2               \r\n"
-        "xor $f8, $f8, $f8                  \r\n"
-        "gslwlc1 $f0, 2($3)                 \r\n"
-        "gslwrc1 $f0, -1($3)                \r\n"
-        "gslwlc1 $f2, 6($3)                 \r\n"
-        "gslwrc1 $f2, 3($3)                 \r\n"
-        "gslwlc1 $f4, 11($3)                \r\n"
-        "gslwrc1 $f4, 8($3)                 \r\n"
-        "gslwlc1 $f6, 15($3)                \r\n"
-        "gslwrc1 $f6, 12($3)                \r\n"
-        "punpcklbh $f0, $f0, $f8            \r\n"
-        "punpcklbh $f2, $f2, $f8            \r\n"
-        "punpcklbh $f4, $f4, $f8            \r\n"
-        "punpcklbh $f6, $f6, $f8            \r\n"
-        "dmtc1 %[ff_pw_m8tom5], $f20        \r\n"
-        "dmtc1 %[ff_pw_m4tom1], $f22        \r\n"
-        "dmtc1 %[ff_pw_1to4], $f24          \r\n"
-        "dmtc1 %[ff_pw_5to8], $f26          \r\n"
-        "pmullh $f0, $f0, $f20              \r\n"
-        "pmullh $f2, $f2, $f22              \r\n"
-        "pmullh $f4, $f4, $f24              \r\n"
-        "pmullh $f6, $f6, $f26              \r\n"
-        "paddsh $f0, $f0, $f4               \r\n"
-        "paddsh $f2, $f2, $f6               \r\n"
-        "paddsh $f0, $f0, $f2               \r\n"
-        "dli $4, 0xE                        \r\n"
-        "dmtc1 $4, $f28                     \r\n"
-        "pshufh $f2, $f0, $f28              \r\n"
-        "paddsh $f0, $f0, $f2               \r\n"
-        "dli $4, 0x1                        \r\n"
-        "dmtc1 $4, $f30                     \r\n"
-        "pshufh $f2, $f0, $f30              \r\n"
-        "paddsh $f10, $f0, $f2              \r\n"
-        "daddiu $3, %[src], -1              \r\n"
-        "daddu $3, $2                       \r\n"
-        "lbu $4, 0($3)                      \r\n"
-        "lbu $8, 16($3)                     \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $5, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $6, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $7, 0($3)                      \r\n"
-        "dsll $5, 16                        \r\n"
-        "dsll $6, 32                        \r\n"
-        "dsll $7, 48                        \r\n"
-        "or $6, $7                          \r\n"
-        "or $4, $5                          \r\n"
-        "or $4, $6                          \r\n"
-        "dmtc1 $4, $f0                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $4, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $5, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $6, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $7, 0($3)                      \r\n"
-        "dsll $5, 16                        \r\n"
-        "dsll $6, 32                        \r\n"
-        "dsll $7, 48                        \r\n"
-        "or $6, $7                          \r\n"
-        "or $4, $5                          \r\n"
-        "or $4, $6                          \r\n"
-        "dmtc1 $4, $f2                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $4, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $5, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $6, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $7, 0($3)                      \r\n"
-        "dsll $5, 16                        \r\n"
-        "dsll $6, 32                        \r\n"
-        "dsll $7, 48                        \r\n"
-        "or $6, $7                          \r\n"
-        "or $4, $5                          \r\n"
-        "or $4, $6                          \r\n"
-        "dmtc1 $4, $f4                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $4, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $5, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $6, 0($3)                      \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "lbu $7, 0($3)                      \r\n"
-        "daddu $8, $7                       \r\n"
-        "daddiu $8, 1                       \r\n"
-        "dsll $8, 4                         \r\n"
-        "dsll $5, 16                        \r\n"
-        "dsll $6, 32                        \r\n"
-        "dsll $7, 48                        \r\n"
-        "or $6, $7                          \r\n"
-        "or $4, $5                          \r\n"
-        "or $4, $6                          \r\n"
-        "dmtc1 $4, $f6                      \r\n"
-        "pmullh $f0, $f0, $f20              \r\n"
-        "pmullh $f2, $f2, $f22              \r\n"
-        "pmullh $f4, $f4, $f24              \r\n"
-        "pmullh $f6, $f6, $f26              \r\n"
-        "paddsh $f0, $f0, $f4               \r\n"
-        "paddsh $f2, $f2, $f6               \r\n"
-        "paddsh $f0, $f0, $f2               \r\n"
-        "pshufh $f2, $f0, $f28              \r\n"
-        "paddsh $f0, $f0, $f2               \r\n"
-        "pshufh $f2, $f0, $f30              \r\n"
-        "paddsh $f12, $f0, $f2              \r\n"
-        "dmfc1 $2, $f10                     \r\n"
-        "dsll $2, 48                        \r\n"
-        "dsra $2, 48                        \r\n"
-        "dmfc1 $3, $f12                     \r\n"
-        "dsll $3, 48                        \r\n"
-        "dsra $3, 48                        \r\n"
-        "beqz %[svq3], 1f                   \r\n"
-        "dli $4, 4                          \r\n"
-        "ddiv $2, $4                        \r\n"
-        "ddiv $3, $4                        \r\n"
-        "dli $4, 5                          \r\n"
-        "dmul $2, $4                        \r\n"
-        "dmul $3, $4                        \r\n"
-        "dli $4, 16                         \r\n"
-        "ddiv $2, $4                        \r\n"
-        "ddiv $3, $4                        \r\n"
-        "daddu $4, $2, $0                   \r\n"
-        "daddu $2, $3, $0                   \r\n"
-        "daddu $3, $4, $0                   \r\n"
-        "b 2f                               \r\n"
-        "1:                                 \r\n"
-        "beqz %[rv40], 1f                   \r\n"
-        "dsra $4, $2, 2                     \r\n"
-        "daddu $2, $4                       \r\n"
-        "dsra $4, $3, 2                     \r\n"
-        "daddu $3, $4                       \r\n"
-        "dsra $2, 4                         \r\n"
-        "dsra $3, 4                         \r\n"
-        "b 2f                               \r\n"
-        "1:                                 \r\n"
-        "dli $4, 5                          \r\n"
-        "dmul $2, $4                        \r\n"
-        "dmul $3, $4                        \r\n"
-        "daddiu $2, 32                      \r\n"
-        "daddiu $3, 32                      \r\n"
-        "dsra $2, 6                         \r\n"
-        "dsra $3, 6                         \r\n"
-        "2:                                 \r\n"
-        "daddu $5, $2, $3                   \r\n"
-        "dli $4, 7                          \r\n"
-        "dmul $5, $4                        \r\n"
-        "dsubu $8, $5                       \r\n"
-        "dmtc1 $0, $f8                      \r\n"
-        "dmtc1 $2, $f0                      \r\n"
-        "pshufh $f0, $f0, $f8               \r\n"
-        "dmtc1 $3, $f10                     \r\n"
-        "pshufh $f10, $f10, $f8             \r\n"
-        "dmtc1 $8, $f12                     \r\n"
-        "pshufh $f12, $f12, $f8             \r\n"
-        "dli $4, 5                          \r\n"
-        "dmtc1 $4, $f14                     \r\n"
-        "pmullh $f2, %[ff_pw_0to3], $f0     \r\n"
-        "pmullh $f4, %[ff_pw_4to7], $f0     \r\n"
-        "pmullh $f6, %[ff_pw_8tob], $f0     \r\n"
-        "pmullh $f8, %[ff_pw_ctof], $f0     \r\n"
-        "daddu $3, %[src], $0               \r\n"
-        "dli $2, 16                         \r\n"
-        "1:                                 \r\n"
-        "paddsh $f16, $f2, $f12             \r\n"
-        "psrah $f16, $f16, $f14             \r\n"
-        "paddsh $f18, $f4, $f12             \r\n"
-        "psrah $f18, $f18, $f14             \r\n"
-        "packushb $f20, $f16, $f18          \r\n"
-        "gssdlc1 $f20, 7($3)                \r\n"
-        "gssdrc1 $f20, 0($3)                \r\n"
-        "paddsh $f16, $f6, $f12             \r\n"
-        "psrah $f16, $f16, $f14             \r\n"
-        "paddsh $f18, $f8, $f12             \r\n"
-        "psrah $f18, $f18, $f14             \r\n"
-        "packushb $f20, $f16, $f18          \r\n"
-        "gssdlc1 $f20, 15($3)               \r\n"
-        "gssdrc1 $f20, 8($3)                \r\n"
-        "paddsh $f12, $f12, $f10            \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddiu $2, -1                      \r\n"
-        "bnez $2, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[svq3]"r"(svq3),[rv40]"r"(rv40),
-          [ff_pw_m8tom5]"r"(ff_pw_m8tom5),[ff_pw_m4tom1]"r"(ff_pw_m4tom1),
-          [ff_pw_1to4]"r"(ff_pw_1to4),[ff_pw_5to8]"r"(ff_pw_5to8),
-          [ff_pw_0to3]"f"(ff_pw_0to3),[ff_pw_4to7]"f"(ff_pw_4to7),
-          [ff_pw_8tob]"f"(ff_pw_8tob),[ff_pw_ctof]"f"(ff_pw_ctof)
-        : "$2","$3","$4","$5","$6","$7","$8","$f0","$f2","$f4","$f6","$f8",
-          "$f10","$f12","$f14","$f16","$f18","$f20","$f22","$f24","$f26",
-          "$f28","$f30"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        PTR_SUBU   "%[addr0],   %[src],         %[stride]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+#endif
+        "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]                \n\t"
+        "biadd      %[ftmp2],   %[ftmp2]                                \n\t"
+        "biadd      %[ftmp3],   %[ftmp3]                                \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "paddush    %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "paddush    %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "psrlh      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp2],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64),
+          [src]"+&r"(src)
+        : [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
 
-void ff_pred16x16_plane_svq3_8_mmi(uint8_t *src, ptrdiff_t stride)
+void ff_pred8x8_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
-    ff_pred16x16_plane_compat_8_mmi(src, stride, 1, 0);
-}
+    double ftmp[5];
+    mips_reg addr[7];
 
-void ff_pred16x16_plane_rv40_8_mmi(uint8_t *src, ptrdiff_t stride)
-{
-    ff_pred16x16_plane_compat_8_mmi(src, stride, 0, 1);
+    __asm__ volatile (
+        "negu       %[addr0],   %[stride]                               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[src]                  \n\t"
+        PTR_ADDIU  "%[addr1],   %[addr0],       0x04                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr3],   $0,             %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr0],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr4],   $0,             %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr1],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr3],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr0],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr1],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr3],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr0],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr1],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr3],   %[addr3],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr0],   0x01                                    \n\t"
+        "lbu        %[addr2],   0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr4],   %[addr4],       %[addr2]                \n\t"
+        PTR_ADDIU  "%[addr1],   0x01                                    \n\t"
+        "dli        %[addr2],  -0x01                                    \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[src]                  \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr5],   $0,             %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr5],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr5],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr5],   %[addr5],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr6],   $0,             %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr6],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr6],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr2],   %[addr2],       %[stride]               \n\t"
+        "lbu        %[addr1],   0x00(%[addr2])                          \n\t"
+        PTR_ADDU   "%[addr6],   %[addr6],       %[addr1]                \n\t"
+        PTR_ADDU   "%[addr3],   %[addr3],       %[addr5]                \n\t"
+        PTR_ADDIU  "%[addr3],   %[addr3],       0x04                    \n\t"
+        PTR_ADDIU  "%[addr4],   %[addr4],       0x02                    \n\t"
+        PTR_ADDIU  "%[addr1],   %[addr6],       0x02                    \n\t"
+        PTR_ADDU   "%[addr2],   %[addr4],       %[addr1]                \n\t"
+        PTR_SRL    "%[addr3],   0x03                                    \n\t"
+        PTR_SRL    "%[addr4],   0x02                                    \n\t"
+        PTR_SRL    "%[addr1],   0x02                                    \n\t"
+        PTR_SRL    "%[addr2],   0x03                                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dmtc1      %[addr3],   %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "dmtc1      %[addr4],   %[ftmp2]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "dmtc1      %[addr1],   %[ftmp3]                                \n\t"
+        "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "dmtc1      %[addr2],   %[ftmp4]                                \n\t"
+        "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp3],       %[ftmp4]                \n\t"
+        PTR_ADDU   "%[addr0],   $0,             %[src]                  \n\t"
+        "sdc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp1],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride)
+        : "memory"
+    );
 }
 
-void ff_pred16x16_plane_h264_8_mmi(uint8_t *src, ptrdiff_t stride)
+void ff_pred8x16_vertical_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
-    ff_pred16x16_plane_compat_8_mmi(src, stride, 0, 0);
-}
+    double ftmp[1];
+    uint64_t tmp[1];
+    uint64_t all64;
 
-void ff_pred8x8_top_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
-{
     __asm__ volatile (
-        "dli $2, 2                          \r\n"
-        "xor $f0, $f0, $f0                  \r\n"
-        "xor $f2, $f2, $f2                  \r\n"
-        "xor $f30, $f30, $f30               \r\n"
-        "negu $3, %[stride]                 \r\n"
-        "daddu $3, $3, %[src]               \r\n"
-        "gsldlc1 $f4, 7($3)                 \r\n"
-        "gsldrc1 $f4, 0($3)                 \r\n"
-        "punpcklbh $f0, $f4, $f30           \r\n"
-        "punpckhbh $f2, $f4, $f30           \r\n"
-        "biadd $f0, $f0                     \r\n"
-        "biadd $f2, $f2                     \r\n"
-        "pshufh $f0, $f0, $f30              \r\n"
-        "pshufh $f2, $f2, $f30              \r\n"
-        "dmtc1 $2, $f4                      \r\n"
-        "pshufh $f4, $f4, $f30              \r\n"
-        "paddush $f0, $f0, $f4              \r\n"
-        "paddush $f2, $f2, $f4              \r\n"
-        "dmtc1 $2, $f4                      \r\n"
-        "psrlh $f0, $f0, $f4                \r\n"
-        "psrlh $f2, $f2, $f4                \r\n"
-        "packushb $f4, $f0, $f2             \r\n"
-        "dli $2, 8                          \r\n"
-        "1:                                 \r\n"
-        "gssdlc1 $f4, 7(%[src])             \r\n"
-        "gssdrc1 $f4, 0(%[src])             \r\n"
-        "daddu %[src], %0, %[stride]        \r\n"
-        "daddiu $2, $2, -1                  \r\n"
-        "bnez $2, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride)
-        : "$2","$3","$f0","$f2","$f4","$f30"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[srcA])                           \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[srcA])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[srcA])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+#endif
+        "dli        %[tmp0],    0x04                                    \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[src])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [src]"+&r"(src)
+        : [stride]"r"((mips_reg)stride),    [srcA]"r"((mips_reg)(src-stride))
+        : "memory"
     );
 }
 
-void ff_pred8x8_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
+void ff_pred8x16_horizontal_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
+    uint64_t tmp[3];
+    mips_reg addr[2];
+
     __asm__ volatile (
-        "negu $2, %[stride]                 \r\n"
-        "daddu $2, $2, %[src]               \r\n"
-        "daddiu $5, $2, 4                   \r\n"
-        "lbu $6, 0($2)                      \r\n"
-        "daddu $3, $0, $6                   \r\n"
-        "daddiu $2, 1                       \r\n"
-        "lbu $6, 0($5)                      \r\n"
-        "daddu $4, $0, $6                   \r\n"
-        "daddiu $5, 1                       \r\n"
-        "lbu $6, 0($2)                      \r\n"
-        "daddu $3, $3, $6                   \r\n"
-        "daddiu $2, 1                       \r\n"
-        "lbu $6, 0($5)                      \r\n"
-        "daddu $4, $4, $6                   \r\n"
-        "daddiu $5, 1                       \r\n"
-        "lbu $6, 0($2)                      \r\n"
-        "daddu $3, $3, $6                   \r\n"
-        "daddiu $2, 1                       \r\n"
-        "lbu $6, 0($5)                      \r\n"
-        "daddu $4, $4, $6                   \r\n"
-        "daddiu $5, 1                       \r\n"
-        "lbu $6, 0($2)                      \r\n"
-        "daddu $3, $3, $6                   \r\n"
-        "daddiu $2, 1                       \r\n"
-        "lbu $6, 0($5)                      \r\n"
-        "daddu $4, $4, $6                   \r\n"
-        "daddiu $5, 1                       \r\n"
-        "dli $6, -1                         \r\n"
-        "daddu $6, $6, %[src]               \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $7, $0, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $7, $7, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $7, $7, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $7, $7, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $8, $0, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $8, $8, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $8, $8, $5                   \r\n"
-        "daddu $6, $6, %[stride]            \r\n"
-        "lbu $5, 0($6)                      \r\n"
-        "daddu $8, $8, $5                   \r\n"
-        "daddu $3, $3, $7                   \r\n"
-        "daddiu $3, $3, 4                   \r\n"
-        "daddiu $4, $4, 2                   \r\n"
-        "daddiu $5, $8, 2                   \r\n"
-        "daddu $6, $4, $5                   \r\n"
-        "dsrl $3, 3                         \r\n"
-        "dsrl $4, 2                         \r\n"
-        "dsrl $5, 2                         \r\n"
-        "dsrl $6, 3                         \r\n"
-        "xor $f30, $f30, $f30               \r\n"
-        "dmtc1 $3, $f0                      \r\n"
-        "pshufh $f0, $f0, $f30              \r\n"
-        "dmtc1 $4, $f2                      \r\n"
-        "pshufh $f2, $f2, $f30              \r\n"
-        "dmtc1 $5, $f4                      \r\n"
-        "pshufh $f4, $f4, $f30              \r\n"
-        "dmtc1 $6, $f6                      \r\n"
-        "pshufh $f6, $f6, $f30              \r\n"
-        "packushb $f0, $f0, $f2             \r\n"
-        "packushb $f2, $f4, $f6             \r\n"
-        "daddu $2, $0, %[src]               \r\n"
-        "sdc1 $f0, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f0, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f0, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f0, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f2, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f2, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f2, 0($2)                    \r\n"
-        "daddu $2, $2, %[stride]            \r\n"
-        "sdc1 $f2, 0($2)                    \r\n"
-        ::[src]"r"(src),[stride]"r"(stride)
-        : "$2","$3","$4","$5","$6","$7","$8","$f0","$f2","$f4","$f6","$f30"
+        PTR_ADDI   "%[addr0],   %[src],         -0x01                   \n\t"
+        PTR_ADDU   "%[addr1],   %[src],         $0                      \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "1:                                                             \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp2],    %[tmp1],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp2],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr1])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "lbu        %[tmp1],    0x00(%[addr0])                          \n\t"
+        "dmul       %[tmp2],    %[tmp1],        %[ff_pb_1]              \n\t"
+        "swl        %[tmp2],    0x07(%[addr1])                          \n\t"
+        "swr        %[tmp2],    0x00(%[addr1])                          \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr1],   %[addr1],       %[stride]               \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
+        : [src]"r"((mips_reg)src),          [stride]"r"((mips_reg)stride),
+          [ff_pb_1]"r"(ff_pb_1)
+        : "memory"
     );
 }
 
-void ff_pred8x16_vertical_8_mmi(uint8_t *src, ptrdiff_t stride)
+static inline void pred16x16_plane_compat_mmi(uint8_t *src, int stride,
+        const int svq3, const int rv40)
 {
-    __asm__ volatile (
-        "gsldlc1 $f2, 7(%[srcA])            \r\n"
-        "gsldrc1 $f2, 0(%[srcA])            \r\n"
-        "dli $8, 16                         \r\n"
-        "1:                                 \r\n"
-        "gssdlc1 $f2, 7(%[src])             \r\n"
-        "gssdrc1 $f2, 0(%[src])             \r\n"
-        "daddu %[src], %[src], %[stride]    \r\n"
-        "daddi $8, $8, -1                   \r\n"
-        "bnez $8, 1b                        \r\n"
-        : [src]"+&r"(src)
-        : [stride]"r"(stride),[srcA]"r"(src-stride)
-        : "$8","$f2"
+    double ftmp[11];
+    uint64_t tmp[7];
+    mips_reg addr[1];
+
+    __asm__ volatile(
+        PTR_SUBU   "%[addr0],   %[src],         %[stride]               \n\t"
+        "dli        %[tmp2],    0x20                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp4]                                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x06(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp0],   -0x01(%[addr0])                         \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[tmp0],    -0x01(%[addr0])                         \n\t"
+        "dmtc1      %[tmp0],    %[ftmp0]                                \n\t"
+        "uld        %[tmp0],    0x08(%[addr0])                          \n\t"
+        "dmtc1      %[tmp0],    %[ftmp2]                                \n\t"
+#endif
+        "dsrl       %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "dsrl       %[ftmp3],   %[ftmp2],       %[ftmp4]                \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pmullh     %[ftmp0],   %[ftmp0],       %[ff_pw_m8tom5]         \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[ff_pw_m4tom1]         \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ff_pw_1to4]           \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ff_pw_5to8]           \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "dli        %[tmp2],    0x0e                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp4]                                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "dli        %[tmp2],    0x01                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp4]                                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp5],   %[ftmp0],       %[ftmp1]                \n\t"
+
+        PTR_ADDIU  "%[addr0],   %[src],         -0x01                   \n\t"
+        PTR_SUBU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp2],    0x00(%[addr0])                          \n\t"
+        "lbu        %[tmp6],    0x10(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp3],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp4],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp5],    0x00(%[addr0])                          \n\t"
+        "dsll       %[tmp3],    %[tmp3],        0x10                    \n\t"
+        "dsll       %[tmp4],    %[tmp4],        0x20                    \n\t"
+        "dsll       %[tmp5],    %[tmp5],        0x30                    \n\t"
+        "or         %[tmp4],    %[tmp4],        %[tmp5]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp3]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp4]                 \n\t"
+        "dmtc1      %[tmp2],    %[ftmp0]                                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp2],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp3],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp4],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp5],    0x00(%[addr0])                          \n\t"
+        "dsll       %[tmp3],    %[tmp3],        0x10                    \n\t"
+        "dsll       %[tmp4],    %[tmp4],        0x20                    \n\t"
+        "dsll       %[tmp5],    %[tmp5],        0x30                    \n\t"
+        "or         %[tmp4],    %[tmp4],        %[tmp5]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp3]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp4]                 \n\t"
+        "dmtc1      %[tmp2],    %[ftmp1]                                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp2],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp3],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp4],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp5],    0x00(%[addr0])                          \n\t"
+        "dsll       %[tmp3],    %[tmp3],        0x10                    \n\t"
+        "dsll       %[tmp4],    %[tmp4],        0x20                    \n\t"
+        "dsll       %[tmp5],    %[tmp5],        0x30                    \n\t"
+        "or         %[tmp4],    %[tmp4],        %[tmp5]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp3]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp4]                 \n\t"
+        "dmtc1      %[tmp2],    %[ftmp2]                                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp2],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp3],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp4],    0x00(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "lbu        %[tmp5],    0x00(%[addr0])                          \n\t"
+        "daddu      %[tmp6],    %[tmp6],        %[tmp5]                 \n\t"
+        "daddiu     %[tmp6],    %[tmp6],        0x01                    \n\t"
+        "dsll       %[tmp6],    %[tmp6],        0x04                    \n\t"
+
+        "dsll       %[tmp3],    %[tmp3],        0x10                    \n\t"
+        "dsll       %[tmp4],    %[tmp4],        0x20                    \n\t"
+        "dsll       %[tmp5],    %[tmp5],        0x30                    \n\t"
+        "or         %[tmp4],    %[tmp4],        %[tmp5]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp3]                 \n\t"
+        "or         %[tmp2],    %[tmp2],        %[tmp4]                 \n\t"
+        "dmtc1      %[tmp2],    %[ftmp3]                                \n\t"
+
+        "pmullh     %[ftmp0],   %[ftmp0],       %[ff_pw_m8tom5]         \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[ff_pw_m4tom1]         \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ff_pw_1to4]           \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ff_pw_5to8]           \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "dli        %[tmp2],    0x0e                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp4]                                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+
+        "dli        %[tmp2],    0x01                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp4]                                \n\t"
+        "pshufh     %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp6],   %[ftmp0],       %[ftmp1]                \n\t"
+
+        "dmfc1      %[tmp0],    %[ftmp5]                                \n\t"
+        "dsll       %[tmp0],    %[tmp0],        0x30                    \n\t"
+        "dsra       %[tmp0],    %[tmp0],        0x30                    \n\t"
+        "dmfc1      %[tmp1],    %[ftmp6]                                \n\t"
+        "dsll       %[tmp1],    %[tmp1],        0x30                    \n\t"
+        "dsra       %[tmp1],    %[tmp1],        0x30                    \n\t"
+
+        "beqz       %[svq3],    1f                                      \n\t"
+        "dli        %[tmp2],    0x04                                    \n\t"
+        "ddiv       %[tmp0],    %[tmp0],        %[tmp2]                 \n\t"
+        "ddiv       %[tmp1],    %[tmp1],        %[tmp2]                 \n\t"
+        "dli        %[tmp2],    0x05                                    \n\t"
+        "dmul       %[tmp0],    %[tmp0],        %[tmp2]                 \n\t"
+        "dmul       %[tmp1],    %[tmp1],        %[tmp2]                 \n\t"
+        "dli        %[tmp2],    0x10                                    \n\t"
+        "ddiv       %[tmp0],    %[tmp0],        %[tmp2]                 \n\t"
+        "ddiv       %[tmp1],    %[tmp1],        %[tmp2]                 \n\t"
+        "daddu      %[tmp2],    %[tmp0],        $0                      \n\t"
+        "daddu      %[tmp0],    %[tmp1],        $0                      \n\t"
+        "daddu      %[tmp1],    %[tmp2],        $0                      \n\t"
+        "b          2f                                                  \n\t"
+
+        "1:                                                             \n\t"
+        "beqz       %[rv40],    1f                                      \n\t"
+        "dsra       %[tmp2],    %[tmp0],        0x02                    \n\t"
+        "daddu      %[tmp0],    %[tmp0],        %[tmp2]                 \n\t"
+        "dsra       %[tmp2],    %[tmp1],        0x02                    \n\t"
+        "daddu      %[tmp1],    %[tmp1],        %[tmp2]                 \n\t"
+        "dsra       %[tmp0],    %[tmp0],        0x04                    \n\t"
+        "dsra       %[tmp1],    %[tmp1],        0x04                    \n\t"
+        "b          2f                                                  \n\t"
+
+        "1:                                                             \n\t"
+        "dli        %[tmp2],    0x05                                    \n\t"
+        "dmul       %[tmp0],    %[tmp0],        %[tmp2]                 \n\t"
+        "dmul       %[tmp1],    %[tmp1],        %[tmp2]                 \n\t"
+        "daddiu     %[tmp0],    %[tmp0],        0x20                    \n\t"
+        "daddiu     %[tmp1],    %[tmp1],        0x20                    \n\t"
+        "dsra       %[tmp0],    %[tmp0],        0x06                    \n\t"
+        "dsra       %[tmp1],    %[tmp1],        0x06                    \n\t"
+
+        "2:                                                             \n\t"
+        "daddu      %[tmp3],    %[tmp0],        %[tmp1]                 \n\t"
+        "dli        %[tmp2],    0x07                                    \n\t"
+        "dmul       %[tmp3],    %[tmp3],        %[tmp2]                 \n\t"
+        "dsubu      %[tmp6],    %[tmp6],        %[tmp3]                 \n\t"
+
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "dmtc1      %[tmp0],    %[ftmp0]                                \n\t"
+        "pshufh     %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "dmtc1      %[tmp1],    %[ftmp5]                                \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "dmtc1      %[tmp6],    %[ftmp6]                                \n\t"
+        "pshufh     %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "dli        %[tmp2],    0x05                                    \n\t"
+        "dmtc1      %[tmp2],    %[ftmp7]                                \n\t"
+        "pmullh     %[ftmp1],   %[ff_pw_0to3],  %[ftmp0]                \n\t"
+        "dmtc1      %[ff_pw_4to7],              %[ftmp2]                \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "dmtc1      %[ff_pw_8tob],              %[ftmp3]                \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "dmtc1      %[ff_pw_ctof],              %[ftmp4]                \n\t"
+        "pmullh     %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+
+        "dli        %[tmp0],    0x10                                    \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         $0                      \n\t"
+        "1:                                                             \n\t"
+        "paddsh     %[ftmp8],   %[ftmp1],       %[ftmp6]                \n\t"
+        "psrah      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp2],       %[ftmp6]                \n\t"
+        "psrah      %[ftmp9],   %[ftmp9],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp8],       %[ftmp9]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[tmp2],    %[ftmp0]                                \n\t"
+        "usd        %[tmp2],    0x00(%[addr0])                          \n\t"
+#endif
+
+        "paddsh     %[ftmp8],   %[ftmp3],       %[ftmp6]                \n\t"
+        "psrah      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp4],       %[ftmp6]                \n\t"
+        "psrah      %[ftmp9],   %[ftmp9],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp8],       %[ftmp9]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x0f(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[tmp2],    %[ftmp0]                                \n\t"
+        "usd        %[tmp2],    0x08(%[addr0])                          \n\t"
+#endif
+
+        "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
+        "daddiu     %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),              [tmp3]"=&r"(tmp[3]),
+          [tmp4]"=&r"(tmp[4]),              [tmp5]"=&r"(tmp[5]),
+          [tmp6]"=&r"(tmp[6]),
+          [addr0]"=&r"(addr[0])
+        : [src]"r"(src),                    [stride]"r"((mips_reg)stride),
+          [svq3]"r"(svq3),                  [rv40]"r"(rv40),
+          [ff_pw_m8tom5]"f"(ff_pw_m8tom5),  [ff_pw_m4tom1]"f"(ff_pw_m4tom1),
+          [ff_pw_1to4]"f"(ff_pw_1to4),      [ff_pw_5to8]"f"(ff_pw_5to8),
+          [ff_pw_0to3]"f"(ff_pw_0to3),      [ff_pw_4to7]"r"(ff_pw_4to7),
+          [ff_pw_8tob]"r"(ff_pw_8tob),      [ff_pw_ctof]"r"(ff_pw_ctof)
+        : "memory"
     );
 }
 
-void ff_pred8x16_horizontal_8_mmi(uint8_t *src, ptrdiff_t stride)
+void ff_pred16x16_plane_h264_8_mmi(uint8_t *src, ptrdiff_t stride)
 {
-    __asm__ volatile (
-        "daddiu $2, %[src], -1              \r\n"
-        "daddu $3, %[src], $0               \r\n"
-        "dli $6, 0x10                       \r\n"
-        "1:                                 \r\n"
-        "lbu $4, 0($2)                      \r\n"
-        "dmul $5, $4, %[ff_pb_1]            \r\n"
-        "sdl $5, 7($3)                      \r\n"
-        "sdr $5, 0($3)                      \r\n"
-        "daddu $2, %[stride]                \r\n"
-        "daddu $3, %[stride]                \r\n"
-        "daddiu $6, -1                      \r\n"
-        "bnez $6, 1b                        \r\n"
-        ::[src]"r"(src),[stride]"r"(stride),[ff_pb_1]"r"(ff_pb_1)
-        : "$2","$3","$4","$5","$6"
-    );
+    pred16x16_plane_compat_mmi(src, stride, 0, 0);
+}
+
+void ff_pred16x16_plane_svq3_8_mmi(uint8_t *src, ptrdiff_t stride)
+{
+    pred16x16_plane_compat_mmi(src, stride, 1, 0);
+}
+
+void ff_pred16x16_plane_rv40_8_mmi(uint8_t *src, ptrdiff_t stride)
+{
+    pred16x16_plane_compat_mmi(src, stride, 0, 1);
 }
diff --git a/libavcodec/mips/h264qpel_mmi.c b/libavcodec/mips/h264qpel_mmi.c
index e04a2d5..0257f16 100644
--- a/libavcodec/mips/h264qpel_mmi.c
+++ b/libavcodec/mips/h264qpel_mmi.c
@@ -22,421 +22,276 @@
  */
 
 #include "h264dsp_mips.h"
+#include "hpeldsp_mips.h"
 #include "libavcodec/bit_depth_template.c"
+#include "libavutil/mips/asmdefs.h"
 
 static inline void copy_block4_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride, int h)
 {
+    double ftmp[1];
+    int low32;
+
     __asm__ volatile (
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 3(%[src])                 \r\n"
-        "gslwrc1 $f2, 0(%[src])                 \r\n"
-        "gsswlc1 $f2, 3(%[dst])                 \r\n"
-        "gsswrc1 $f2, 0(%[dst])                 \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),[h]"r"(h)
-        : "$f2"
+        "1:                                                             \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp0],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp0]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+#endif
+        "addi       %[h],       %[h],           -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [h]"+&r"(h),
+          [low32]"=&r"(low32)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride)
+        : "memory"
     );
 }
 
 static inline void copy_block8_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride, int h)
 {
+    double ftmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[src])                 \r\n"
-        "gsldrc1 $f2, 0(%[src])                 \r\n"
-        "gssdlc1 $f2, 7(%[dst])                 \r\n"
-        "gssdrc1 $f2, 0(%[dst])                 \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),[h]"r"(h)
-        : "$f2"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+        "gssdlc1    %[ftmp0],   0x07(%[dst])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "usd        %[all64],   0x00(%[dst])                            \n\t"
+#endif
+        "addi       %[h],       %[h],           -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [all64]"=&r"(all64),
+          [h]"+&r"(h)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride)
+        : "memory"
     );
 }
 
 static inline void copy_block16_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride, int h)
 {
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[src])                 \r\n"
-        "gsldrc1 $f2, 0(%[src])                 \r\n"
-        "gsldlc1 $f4, 15(%[src])                \r\n"
-        "gsldrc1 $f4, 8(%[src])                 \r\n"
-        "gssdlc1 $f2, 7(%[dst])                 \r\n"
-        "gssdrc1 $f2, 0(%[dst])                 \r\n"
-        "gssdlc1 $f4, 15(%[dst])                \r\n"
-        "gssdrc1 $f4, 8(%[dst])                 \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),[h]"r"(h)
-        : "$f2","$f4"
-    );
-}
+    double ftmp[1];
+    uint64_t tmp[1];
+    uint64_t all64;
 
-#define op_put(a, b) a = b
-#define op_avg(a, b) a = rnd_avg_pixel4(a, b)
-static inline void put_pixels4_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
     __asm__ volatile (
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 3(%[pixels])              \r\n"
-        "gslwrc1 $f2, 0(%[pixels])              \r\n"
-        "gsswlc1 $f2, 3(%[block])               \r\n"
-        "gsswrc1 $f2, 0(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+#endif
+        "ldl        %[tmp0],    0x0f(%[src])                            \n\t"
+        "ldr        %[tmp0],    0x08(%[src])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[dst])                            \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[dst])                            \n\t"
+#endif
+        "sdl        %[tmp0],    0x0f(%[dst])                            \n\t"
+        "sdr        %[tmp0],    0x08(%[dst])                            \n\t"
+        "addi       %[h],       %[h],           -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),
+          [tmp0]"=&r"(tmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [all64]"=&r"(all64),
+          [h]"+&r"(h)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride)
+        : "memory"
     );
 }
 
-static inline void put_pixels8_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[pixels])              \r\n"
-        "gsldrc1 $f2, 0(%[pixels])              \r\n"
-        "gssdlc1 $f2, 7(%[block])               \r\n"
-        "gssdrc1 $f2, 0(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2"
-    );
-}
-
-static inline void put_pixels16_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[pixels])              \r\n"
-        "gsldrc1 $f2, 0(%[pixels])              \r\n"
-        "gsldlc1 $f4, 15(%[pixels])             \r\n"
-        "gsldrc1 $f4, 8(%[pixels])              \r\n"
-        "gssdlc1 $f2, 7(%[block])               \r\n"
-        "gssdrc1 $f2, 0(%[block])               \r\n"
-        "gssdlc1 $f4, 15(%[block])              \r\n"
-        "gssdrc1 $f4, 8(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2","$f4"
-    );
-}
-
-static inline void avg_pixels4_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 3(%[pixels])              \r\n"
-        "gslwrc1 $f2, 0(%[pixels])              \r\n"
-        "gslwlc1 $f4, 3(%[block])               \r\n"
-        "gslwrc1 $f4, 0(%[block])               \r\n"
-        "pavgb $f2, $f2, $f4                    \r\n"
-        "gsswlc1 $f2, 3(%[block])               \r\n"
-        "gsswrc1 $f2, 0(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2","$f4"
-    );
-}
-
-static inline void avg_pixels8_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[block])               \r\n"
-        "gsldrc1 $f2, 0(%[block])               \r\n"
-        "gsldlc1 $f4, 7(%[pixels])              \r\n"
-        "gsldrc1 $f4, 0(%[pixels])              \r\n"
-        "pavgb $f2, $f2, $f4                    \r\n"
-        "gssdlc1 $f2, 7(%[block])               \r\n"
-        "gssdrc1 $f2, 0(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2","$f4"
-    );
-}
-
-static inline void avg_pixels16_mmi(uint8_t *block, const uint8_t *pixels,
-        ptrdiff_t line_size, int h)
-{
-    __asm__ volatile (
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 7(%[block])               \r\n"
-        "gsldrc1 $f2, 0(%[block])               \r\n"
-        "gsldlc1 $f4, 15(%[block])              \r\n"
-        "gsldrc1 $f4, 8(%[block])               \r\n"
-        "gsldlc1 $f6, 7(%[pixels])              \r\n"
-        "gsldrc1 $f6, 0(%[pixels])              \r\n"
-        "gsldlc1 $f8, 15(%[pixels])             \r\n"
-        "gsldrc1 $f8, 8(%[pixels])              \r\n"
-        "pavgb $f2, $f2, $f6                    \r\n"
-        "pavgb $f4, $f4, $f8                    \r\n"
-        "gssdlc1 $f2, 7(%[block])               \r\n"
-        "gssdrc1 $f2, 0(%[block])               \r\n"
-        "gssdlc1 $f4, 15(%[block])              \r\n"
-        "gssdrc1 $f4, 8(%[block])               \r\n"
-        "dadd %[pixels], %[pixels], %[line_size]\r\n"
-        "dadd %[block], %[block], %[line_size]  \r\n"
-        "daddi %[h], %[h], -1                   \r\n"
-        "bnez %[h], 1b                          \r\n"
-        : [block]"+&r"(block),[pixels]"+&r"(pixels)
-        : [line_size]"r"(line_size),[h]"r"(h)
-        : "$f2","$f4","$f6","$f8"
-    );
-}
-
-static inline void put_pixels4_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_put(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-    }
-}
-
-static inline void put_pixels8_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_put(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 4]);
-        b = AV_RN4P(&src2[i * src_stride2 + 4]);
-        op_put(*((pixel4 *) &dst[i * dst_stride + 4]), rnd_avg_pixel4(a, b));
-    }
-}
-
-static inline void put_pixels16_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_put(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 4]);
-        b = AV_RN4P(&src2[i * src_stride2 + 4]);
-        op_put(*((pixel4 *) &dst[i * dst_stride + 4]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 8]);
-        b = AV_RN4P(&src2[i * src_stride2 + 8]);
-        op_put(*((pixel4 *) &dst[i * dst_stride + 8]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 12]);
-        b = AV_RN4P(&src2[i * src_stride2 + 12]);
-        op_put(*((pixel4 *) &dst[i * dst_stride + 12]), rnd_avg_pixel4(a, b));
-    }
-}
-
-static inline void avg_pixels4_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-    }
-}
-
-static inline void avg_pixels8_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 4]);
-        b = AV_RN4P(&src2[i * src_stride2 + 4]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride + 4]), rnd_avg_pixel4(a, b));
-    }
-}
-
-static inline void avg_pixels16_l2_mmi(uint8_t *dst, const uint8_t *src1,
-        const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
-        int h)
-{
-    int i;
-    for (i = 0; i < h; i++) {
-        pixel4 a, b;
-        a = AV_RN4P(&src1[i * src_stride1]);
-        b = AV_RN4P(&src2[i * src_stride2]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 4]);
-        b = AV_RN4P(&src2[i * src_stride2 + 4]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride + 4]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 8]);
-        b = AV_RN4P(&src2[i * src_stride2 + 8]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride + 8]), rnd_avg_pixel4(a, b));
-        a = AV_RN4P(&src1[i * src_stride1 + 12]);
-        b = AV_RN4P(&src2[i * src_stride2 + 12]);
-        op_avg(*((pixel4 *) &dst[i * dst_stride + 12]), rnd_avg_pixel4(a, b));
-
-    }
-}
-#undef op_put
-#undef op_avg
-
 #define op2_avg(a, b)  a = (((a)+CLIP(((b) + 512)>>10)+1)>>1)
 #define op2_put(a, b)  a = CLIP(((b) + 512)>>10)
 static void put_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[10];
+    uint64_t tmp[1];
+    int low32;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 4                              \r\n"
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 1(%[src])                 \r\n"
-        "gslwrc1 $f2, -2(%[src])                \r\n"
-        "gslwlc1 $f4, 2(%[src])                 \r\n"
-        "gslwrc1 $f4, -1(%[src])                \r\n"
-        "gslwlc1 $f6, 3(%[src])                 \r\n"
-        "gslwrc1 $f6, 0(%[src])                 \r\n"
-        "gslwlc1 $f8, 4(%[src])                 \r\n"
-        "gslwrc1 $f8, 1(%[src])                 \r\n"
-        "gslwlc1 $f10, 5(%[src])                \r\n"
-        "gslwrc1 $f10, 2(%[src])                \r\n"
-        "gslwlc1 $f12, 6(%[src])                \r\n"
-        "gslwrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-        "paddsh $f18, $f18, %[ff_pw_16]         \r\n"
-        "psrah $f18, $f18, %[ff_pw_5]           \r\n"
-        "packushb $f18, $f18, $f0               \r\n"
-        "gsswlc1 $f18, 3(%[dst])                \r\n"
-        "gsswrc1 $f18, 0(%[dst])                \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5),[ff_pw_16]"f"(ff_pw_16)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x04                                    \n\t"
+        "1:                                                             \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   -0x01(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x01(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x02(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "ulw        %[low32],   0x03(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp1],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp8],   %[ftmp8],       %[ff_pw_5]              \n\t"
+        "psubsh     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp9],       %[ff_pw_16]             \n\t"
+        "psrah      %[ftmp9],   %[ftmp9],       %[ff_pw_5]              \n\t"
+        "packushb   %[ftmp9],   %[ftmp9],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp9],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp9],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp9]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [low32]"=&r"(low32)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
+          [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
 }
 
 static void put_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[11];
+    uint64_t tmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 8                              \r\n"
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 5(%[src])                 \r\n"
-        "gsldrc1 $f2, -2(%[src])                \r\n"
-        "gsldlc1 $f4, 6(%[src])                 \r\n"
-        "gsldrc1 $f4, -1(%[src])                \r\n"
-        "gsldlc1 $f6, 7(%[src])                 \r\n"
-        "gsldrc1 $f6, 0(%[src])                 \r\n"
-        "gsldlc1 $f8, 8(%[src])                 \r\n"
-        "gsldrc1 $f8, 1(%[src])                 \r\n"
-        "gsldlc1 $f10, 9(%[src])                \r\n"
-        "gsldrc1 $f10, 2(%[src])                \r\n"
-        "gsldlc1 $f12, 10(%[src])               \r\n"
-        "gsldrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f14, $f6, $f0               \r\n"
-        "punpckhbh $f16, $f6, $f0               \r\n"
-        "punpcklbh $f18, $f8, $f0               \r\n"
-        "punpckhbh $f20, $f8, $f0               \r\n"
-        "paddsh $f6, $f14, $f18                 \r\n"
-        "paddsh $f8, $f16, $f20                 \r\n"
-        "pmullh $f6, $f6, %[ff_pw_20]           \r\n"
-        "pmullh $f8, $f8, %[ff_pw_20]           \r\n"
-        "punpcklbh $f14, $f4, $f0               \r\n"
-        "punpckhbh $f16, $f4, $f0               \r\n"
-        "punpcklbh $f18, $f10, $f0              \r\n"
-        "punpckhbh $f20, $f10, $f0              \r\n"
-        "paddsh $f4, $f14, $f18                 \r\n"
-        "paddsh $f10, $f16, $f20                \r\n"
-        "pmullh $f4, $f4, %[ff_pw_5]            \r\n"
-        "pmullh $f10, $f10, %[ff_pw_5]          \r\n"
-        "punpcklbh $f14, $f2, $f0               \r\n"
-        "punpckhbh $f16, $f2, $f0               \r\n"
-        "punpcklbh $f18, $f12, $f0              \r\n"
-        "punpckhbh $f20, $f12, $f0              \r\n"
-        "paddsh $f2, $f14, $f18                 \r\n"
-        "paddsh $f12, $f16, $f20                \r\n"
-        "psubsh $f6, $f6, $f4                   \r\n"
-        "psubsh $f8, $f8, $f10                  \r\n"
-        "paddsh $f6, $f6, $f2                   \r\n"
-        "paddsh $f8, $f8, $f12                  \r\n"
-        "paddsh $f6, $f6, %[ff_pw_16]           \r\n"
-        "paddsh $f8, $f8, %[ff_pw_16]           \r\n"
-        "psrah $f6, $f6, %[ff_pw_5]             \r\n"
-        "psrah $f8, $f8, %[ff_pw_5]             \r\n"
-        "packushb $f18, $f6, $f8                \r\n"
-        "sdc1 $f18, 0(%[dst])                   \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5),[ff_pw_16]"f"(ff_pw_16)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x05(%[src])                            \n\t"
+        "gsldrc1    %[ftmp1],   -0x02(%[src])                           \n\t"
+        "gsldlc1    %[ftmp2],   0x06(%[src])                            \n\t"
+        "gsldrc1    %[ftmp2],   -0x01(%[src])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[src])                            \n\t"
+        "gsldlc1    %[ftmp4],   0x08(%[src])                            \n\t"
+        "gsldrc1    %[ftmp4],   0x01(%[src])                            \n\t"
+        "gsldlc1    %[ftmp5],   0x09(%[src])                            \n\t"
+        "gsldrc1    %[ftmp5],   0x02(%[src])                            \n\t"
+        "gsldlc1    %[ftmp6],   0x0a(%[src])                            \n\t"
+        "gsldrc1    %[ftmp6],   0x03(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   -0x02(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   -0x01(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x01(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x02(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x03(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "punpcklbh  %[ftmp7],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp4],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp4],   %[ftmp4],       %[ff_pw_20]             \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp5],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp2],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp5],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ff_pw_5]              \n\t"
+        "pmullh     %[ftmp5],   %[ftmp5],       %[ff_pw_5]              \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp6],   %[ftmp8],       %[ftmp10]               \n\t"
+        "psubsh     %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubsh     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ff_pw_16]             \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ff_pw_16]             \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ff_pw_5]              \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ff_pw_5]              \n\t"
+        "packushb   %[ftmp9],   %[ftmp3],       %[ftmp4]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp9],   0x07(%[dst])                            \n\t"
+        "gssdrc1    %[ftmp9],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp9]                                \n\t"
+        "usd        %[all64],   0x00(%[dst])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src]"+&r"(src)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
+          [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
 }
 
@@ -454,116 +309,162 @@ static void put_h264_qpel16_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
 static void avg_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[11];
+    uint64_t tmp[1];
+    int low32;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 4                              \r\n"
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 1(%[src])                 \r\n"
-        "gslwrc1 $f2, -2(%[src])                \r\n"
-        "gslwlc1 $f4, 2(%[src])                 \r\n"
-        "gslwrc1 $f4, -1(%[src])                \r\n"
-        "gslwlc1 $f6, 3(%[src])                 \r\n"
-        "gslwrc1 $f6, 0(%[src])                 \r\n"
-        "gslwlc1 $f8, 4(%[src])                 \r\n"
-        "gslwrc1 $f8, 1(%[src])                 \r\n"
-        "gslwlc1 $f10, 5(%[src])                \r\n"
-        "gslwrc1 $f10, 2(%[src])                \r\n"
-        "gslwlc1 $f12, 6(%[src])                \r\n"
-        "gslwrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-        "paddsh $f18, $f18, %[ff_pw_16]         \r\n"
-        "psrah $f18, $f18, %[ff_pw_5]           \r\n"
-        "packushb $f18, $f18, $f0               \r\n"
-        "lwc1 $f20, 0(%[dst])                   \r\n"
-        "pavgb $f18, $f18, $f20                 \r\n"
-        "gsswlc1 $f18, 3(%[dst])                \r\n"
-        "gsswrc1 $f18, 0(%[dst])                \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5),[ff_pw_16]"f"(ff_pw_16)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x04                                    \n\t"
+        "1:                                                             \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   -0x01(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x01(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x02(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "ulw        %[low32],   0x03(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp1],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp8],   %[ftmp8],       %[ff_pw_5]              \n\t"
+        "psubsh     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp9],       %[ff_pw_16]             \n\t"
+        "psrah      %[ftmp9],   %[ftmp9],       %[ff_pw_5]              \n\t"
+        "packushb   %[ftmp9],   %[ftmp9],       %[ftmp0]                \n\t"
+        "lwc1       %[ftmp10],  0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp9],   %[ftmp9],       %[ftmp10]               \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp9],   0x03(%[dst])                            \n\t"
+        "gsswrc1    %[ftmp9],   0x00(%[dst])                            \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp9]                                \n\t"
+        "usw        %[low32],   0x00(%[dst])                            \n\t"
+#endif
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [tmp0]"=&r"(tmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [low32]"=&r"(low32)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
+          [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
 }
 
 static void avg_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[11];
+    uint64_t tmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 8                              \r\n"
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 5(%[src])                 \r\n"
-        "gsldrc1 $f2, -2(%[src])                \r\n"
-        "gsldlc1 $f4, 6(%[src])                 \r\n"
-        "gsldrc1 $f4, -1(%[src])                \r\n"
-        "gsldlc1 $f6, 7(%[src])                 \r\n"
-        "gsldrc1 $f6, 0(%[src])                 \r\n"
-        "gsldlc1 $f8, 8(%[src])                 \r\n"
-        "gsldrc1 $f8, 1(%[src])                 \r\n"
-        "gsldlc1 $f10, 9(%[src])                \r\n"
-        "gsldrc1 $f10, 2(%[src])                \r\n"
-        "gsldlc1 $f12, 10(%[src])               \r\n"
-        "gsldrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f14, $f6, $f0               \r\n"
-        "punpckhbh $f16, $f6, $f0               \r\n"
-        "punpcklbh $f18, $f8, $f0               \r\n"
-        "punpckhbh $f20, $f8, $f0               \r\n"
-        "paddsh $f6, $f14, $f18                 \r\n"
-        "paddsh $f8, $f16, $f20                 \r\n"
-        "pmullh $f6, $f6, %[ff_pw_20]           \r\n"
-        "pmullh $f8, $f8, %[ff_pw_20]           \r\n"
-        "punpcklbh $f14, $f4, $f0               \r\n"
-        "punpckhbh $f16, $f4, $f0               \r\n"
-        "punpcklbh $f18, $f10, $f0              \r\n"
-        "punpckhbh $f20, $f10, $f0              \r\n"
-        "paddsh $f4, $f14, $f18                 \r\n"
-        "paddsh $f10, $f16, $f20                \r\n"
-        "pmullh $f4, $f4, %[ff_pw_5]            \r\n"
-        "pmullh $f10, $f10, %[ff_pw_5]          \r\n"
-        "punpcklbh $f14, $f2, $f0               \r\n"
-        "punpckhbh $f16, $f2, $f0               \r\n"
-        "punpcklbh $f18, $f12, $f0              \r\n"
-        "punpckhbh $f20, $f12, $f0              \r\n"
-        "paddsh $f2, $f14, $f18                 \r\n"
-        "paddsh $f12, $f16, $f20                \r\n"
-        "psubsh $f6, $f6, $f4                   \r\n"
-        "psubsh $f8, $f8, $f10                  \r\n"
-        "paddsh $f6, $f6, $f2                   \r\n"
-        "paddsh $f8, $f8, $f12                  \r\n"
-        "paddsh $f6, $f6, %[ff_pw_16]           \r\n"
-        "paddsh $f8, $f8, %[ff_pw_16]           \r\n"
-        "psrah $f6, $f6, %[ff_pw_5]             \r\n"
-        "psrah $f8, $f8, %[ff_pw_5]             \r\n"
-        "packushb $f18, $f6, $f8                \r\n"
-        "ldc1 $f20, 0(%[dst])                   \r\n"
-        "pavgb $f18, $f18, $f20                 \r\n"
-        "sdc1 $f18, 0(%[dst])                   \r\n"
-        "dadd %[dst], %[dst], %[dstStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [dst]"+&r"(dst),[src]"+&r"(src)
-        : [dstStride]"r"(dstStride),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5),[ff_pw_16]"f"(ff_pw_16)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18","$f20"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x08                                    \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x05(%[src])                            \n\t"
+        "gsldrc1    %[ftmp1],   -0x02(%[src])                           \n\t"
+        "gsldlc1    %[ftmp2],   0x06(%[src])                            \n\t"
+        "gsldrc1    %[ftmp2],   -0x01(%[src])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[src])                            \n\t"
+        "gsldlc1    %[ftmp4],   0x08(%[src])                            \n\t"
+        "gsldrc1    %[ftmp4],   0x01(%[src])                            \n\t"
+        "gsldlc1    %[ftmp5],   0x09(%[src])                            \n\t"
+        "gsldrc1    %[ftmp5],   0x02(%[src])                            \n\t"
+        "gsldlc1    %[ftmp6],   0x0a(%[src])                            \n\t"
+        "gsldrc1    %[ftmp6],   0x03(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   -0x02(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   -0x01(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x01(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x02(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x03(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "punpcklbh  %[ftmp7],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp4],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp4],   %[ftmp4],       %[ff_pw_20]             \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp5],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp2],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp5],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ff_pw_5]              \n\t"
+        "pmullh     %[ftmp5],   %[ftmp5],       %[ff_pw_5]              \n\t"
+        "punpcklbh  %[ftmp7],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp8],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp9],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp10],  %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp1],   %[ftmp7],       %[ftmp9]                \n\t"
+        "paddsh     %[ftmp6],   %[ftmp8],       %[ftmp10]               \n\t"
+        "psubsh     %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "psubsh     %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ff_pw_16]             \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ff_pw_16]             \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ff_pw_5]              \n\t"
+        "psrah      %[ftmp4],   %[ftmp4],       %[ff_pw_5]              \n\t"
+        "packushb   %[ftmp9],   %[ftmp3],       %[ftmp4]                \n\t"
+        "ldc1       %[ftmp10],  0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp9],   %[ftmp9],       %[ftmp10]               \n\t"
+        "sdc1       %[ftmp9],   0x00(%[dst])                            \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src]"+&r"(src)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
+          [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
 }
 
@@ -581,342 +482,438 @@ static void avg_h264_qpel16_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
 static void put_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[12];
+    uint64_t tmp[1];
+    int low32;
+
+    src -= 2 * srcStride;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "gslwlc1 $f2, 3(%[srcB])                \r\n"
-        "gslwrc1 $f2, 0(%[srcB])                \r\n"
-        "gslwlc1 $f4, 3(%[srcA])                \r\n"
-        "gslwrc1 $f4, 0(%[srcA])                \r\n"
-        "gslwlc1 $f6, 3(%[src0])                \r\n"
-        "gslwrc1 $f6, 0(%[src0])                \r\n"
-        "gslwlc1 $f8, 3(%[src1])                \r\n"
-        "gslwrc1 $f8, 0(%[src1])                \r\n"
-        "gslwlc1 $f10, 3(%[src2])               \r\n"
-        "gslwrc1 $f10, 0(%[src2])               \r\n"
-        "gslwlc1 $f12, 3(%[src3])               \r\n"
-        "gslwrc1 $f12, 0(%[src3])               \r\n"
-        "gslwlc1 $f14, 3(%[src4])               \r\n"
-        "gslwrc1 $f14, 0(%[src4])               \r\n"
-        "gslwlc1 $f16, 3(%[src5])               \r\n"
-        "gslwrc1 $f16, 0(%[src5])               \r\n"
-        "gslwlc1 $f18, 3(%[src6])               \r\n"
-        "gslwrc1 $f18, 0(%[src6])               \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "punpcklbh $f14, $f14, $f0              \r\n"
-        "punpcklbh $f16, $f16, $f0              \r\n"
-        "punpcklbh $f18, $f18, $f0              \r\n"
-        "paddsh $f20, $f6, $f8                  \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f4, $f10                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f24, $f20, $f22                \r\n"
-        "paddsh $f24, $f24, $f2                 \r\n"
-        "paddsh $f24, $f24, $f12                \r\n"
-        "paddsh $f20, $f8, $f10                 \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f6, $f12                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f26, $f20, $f22                \r\n"
-        "paddsh $f26, $f26, $f4                 \r\n"
-        "paddsh $f26, $f26, $f14                \r\n"
-        "paddsh $f20, $f10, $f12                \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f8, $f14                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f28, $f20, $f22                \r\n"
-        "paddsh $f28, $f28, $f6                 \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f20, $f12, $f14                \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f10, $f16                \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f30, $f20, $f22                \r\n"
-        "paddsh $f30, $f30, $f8                 \r\n"
-        "paddsh $f30, $f30, $f18                \r\n"
-        "paddsh $f24, $f24, %[ff_pw_16]         \r\n"
-        "paddsh $f26, $f26, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "paddsh $f30, $f30, %[ff_pw_16]         \r\n"
-        "psrah $f24, $f24, %[ff_pw_5]           \r\n"
-        "psrah $f26, $f26, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "psrah $f30, $f30, %[ff_pw_5]           \r\n"
-        "packushb $f24, $f24, $f0               \r\n"
-        "packushb $f26, $f26, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "packushb $f30, $f30, $f0               \r\n"
-        "swc1 $f24, 0(%[dst0])                  \r\n"
-        "swc1 $f26, 0(%[dst1])                  \r\n"
-        "swc1 $f28, 0(%[dst2])                  \r\n"
-        "swc1 $f30, 0(%[dst3])                  \r\n"
-        ::[dst0]"r"(dst),               [dst1]"r"(dst+dstStride),
-          [dst2]"r"(dst+2*dstStride),   [dst3]"r"(dst+3*dstStride),
-          [srcB]"r"(src-2*srcStride),   [srcA]"r"(src-srcStride),
-          [src0]"r"(src),               [src1]"r"(src+srcStride),
-          [src2]"r"(src+2*srcStride),   [src3]"r"(src+3*srcStride),
-          [src4]"r"(src+4*srcStride),   [src5]"r"(src+5*srcStride),
-          [src6]"r"(src+6*srcStride),   [ff_pw_20]"f"(ff_pw_20),
-          [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
-        : "$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16","$f18",
-          "$f20","$f22","$f24","$f26","$f28","$f30"
+        ".set       push                                                \n\t"
+        ".set       noreorder                                           \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "mtc1       %[tmp0],    %[ftmp10]                               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "dli        %[tmp0],    0x05                                    \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "mtc1       %[tmp0],    %[ftmp11]                               \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "paddh      %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psllh      %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "swc1       %[ftmp7],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "paddh      %[ftmp7],   %[ftmp4],       %[ftmp5]                \n\t"
+        "psllh      %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "swc1       %[ftmp7],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "paddh      %[ftmp7],   %[ftmp5],       %[ftmp6]                \n\t"
+        "psllh      %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "swc1       %[ftmp7],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "paddh      %[ftmp7],   %[ftmp6],       %[ftmp1]                \n\t"
+        "psllh      %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "psrah      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "packushb   %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "swc1       %[ftmp7],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        ".set       pop                                                 \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [tmp0]"=&r"(tmp[0]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [low32]"=&r"(low32)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
 }
 
 static void put_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
-    __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "gsldlc1 $f2, 7(%[srcB])                \r\n"
-        "gsldrc1 $f2, 0(%[srcB])                \r\n"
-        "gsldlc1 $f4, 7(%[srcA])                \r\n"
-        "gsldrc1 $f4, 0(%[srcA])                \r\n"
-        "gsldlc1 $f6, 7(%[src0])                \r\n"
-        "gsldrc1 $f6, 0(%[src0])                \r\n"
-        "gsldlc1 $f8, 7(%[src1])                \r\n"
-        "gsldrc1 $f8, 0(%[src1])                \r\n"
-        "gsldlc1 $f10, 7(%[src2])               \r\n"
-        "gsldrc1 $f10, 0(%[src2])               \r\n"
-        "gsldlc1 $f12, 7(%[src3])               \r\n"
-        "gsldrc1 $f12, 0(%[src3])               \r\n"
-        "gsldlc1 $f14, 7(%[src4])               \r\n"
-        "gsldrc1 $f14, 0(%[src4])               \r\n"
-        "gsldlc1 $f16, 7(%[src5])               \r\n"
-        "gsldrc1 $f16, 0(%[src5])               \r\n"
-        "gsldlc1 $f18, 7(%[src6])               \r\n"
-        "gsldrc1 $f18, 0(%[src6])               \r\n"
-        "gsldlc1 $f20, 7(%[src7])               \r\n"
-        "gsldrc1 $f20, 0(%[src7])               \r\n"
-        "gsldlc1 $f22, 7(%[src8])               \r\n"
-        "gsldrc1 $f22, 0(%[src8])               \r\n"
-        "gsldlc1 $f24, 7(%[src9])               \r\n"
-        "gsldrc1 $f24, 0(%[src9])               \r\n"
-        "gsldlc1 $f26, 7(%[src10])              \r\n"
-        "gsldrc1 $f26, 0(%[src10])              \r\n"
-        "punpcklbh $f1, $f2, $f0                \r\n"
-        "punpckhbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f3, $f4, $f0                \r\n"
-        "punpckhbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f5, $f6, $f0                \r\n"
-        "punpckhbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f7, $f8, $f0                \r\n"
-        "punpckhbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f9, $f10, $f0               \r\n"
-        "punpckhbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f11, $f12, $f0              \r\n"
-        "punpckhbh $f12, $f12, $f0              \r\n"
-        "punpcklbh $f13, $f14, $f0              \r\n"
-        "punpckhbh $f14, $f14, $f0              \r\n"
-        "punpcklbh $f15, $f16, $f0              \r\n"
-        "punpckhbh $f16, $f16, $f0              \r\n"
-        "punpcklbh $f17, $f18, $f0              \r\n"
-        "punpckhbh $f18, $f18, $f0              \r\n"
-        "punpcklbh $f19, $f20, $f0              \r\n"
-        "punpckhbh $f20, $f20, $f0              \r\n"
-        "punpcklbh $f21, $f22, $f0              \r\n"
-        "punpckhbh $f22, $f22, $f0              \r\n"
-        "punpcklbh $f23, $f24, $f0              \r\n"
-        "punpckhbh $f24, $f24, $f0              \r\n"
-        "punpcklbh $f25, $f26, $f0              \r\n"
-        "punpckhbh $f26, $f26, $f0              \r\n"
-        "paddsh $f27, $f5, $f7                  \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f6, $f8                  \r\n"//src0+src1
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f3                 \r\n"
-        "psubsh $f28, $f28, $f4                 \r\n"
-        "psubsh $f27, $f27, $f9                 \r\n"
-        "psubsh $f28, $f28, $f10                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f1                 \r\n"
-        "paddsh $f28, $f28, $f2                 \r\n"
-        "paddsh $f27, $f27, $f11                \r\n"
-        "paddsh $f28, $f28, $f12                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f2, $f27, $f28              \r\n"
-        "sdc1 $f2, 0(%[dst0])                   \r\n"
-        "paddsh $f27, $f7, $f9                  \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f8, $f10                 \r\n"//src1+src2
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f5                 \r\n"
-        "psubsh $f28, $f28, $f6                 \r\n"
-        "psubsh $f27, $f27, $f11                \r\n"
-        "psubsh $f28, $f28, $f12                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f3                 \r\n"
-        "paddsh $f28, $f28, $f4                 \r\n"
-        "paddsh $f27, $f27, $f13                \r\n"
-        "paddsh $f28, $f28, $f14                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f4, $f27, $f28              \r\n"
-        "sdc1 $f4, 0(%[dst1])                   \r\n"
-        "paddsh $f27, $f9, $f11                 \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f10, $f12                \r\n"//src2+src3
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f7                 \r\n"
-        "psubsh $f28, $f28, $f8                 \r\n"
-        "psubsh $f27, $f27, $f13                \r\n"
-        "psubsh $f28, $f28, $f14                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f5                 \r\n"
-        "paddsh $f28, $f28, $f6                 \r\n"
-        "paddsh $f27, $f27, $f15                \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f6, $f27, $f28              \r\n"
-        "sdc1 $f6, 0(%[dst2])                   \r\n"
-        "paddsh $f27, $f11, $f13                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f12, $f14                \r\n"//src3+src4
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f9                 \r\n"
-        "psubsh $f28, $f28, $f10                \r\n"
-        "psubsh $f27, $f27, $f15                \r\n"
-        "psubsh $f28, $f28, $f16                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f7                 \r\n"
-        "paddsh $f28, $f28, $f8                 \r\n"
-        "paddsh $f27, $f27, $f17                \r\n"
-        "paddsh $f28, $f28, $f18                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f8, $f27, $f28              \r\n"
-        "sdc1 $f8, 0(%[dst3])                   \r\n"
-        "paddsh $f27, $f13, $f15                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f14, $f16                \r\n"//src4+src5
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f11                \r\n"
-        "psubsh $f28, $f28, $f12                \r\n"
-        "psubsh $f27, $f27, $f17                \r\n"
-        "psubsh $f28, $f28, $f18                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f9                 \r\n"
-        "paddsh $f28, $f28, $f10                \r\n"
-        "paddsh $f27, $f27, $f19                \r\n"
-        "paddsh $f28, $f28, $f20                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f10, $f27, $f28             \r\n"
-        "sdc1 $f10, 0(%[dst4])                  \r\n"
-
-        "paddsh $f27, $f15, $f17                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f16, $f18                \r\n"//src5+src6
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f13                \r\n"
-        "psubsh $f28, $f28, $f14                \r\n"
-        "psubsh $f27, $f27, $f19                \r\n"
-        "psubsh $f28, $f28, $f20                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f11                \r\n"
-        "paddsh $f28, $f28, $f12                \r\n"
-        "paddsh $f27, $f27, $f21                \r\n"
-        "paddsh $f28, $f28, $f22                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f12, $f27, $f28             \r\n"
-        "sdc1 $f12, 0(%[dst5])                  \r\n"
-        "paddsh $f27, $f17, $f19                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f18, $f20                \r\n"//src6+src7
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f15                \r\n"
-        "psubsh $f28, $f28, $f16                \r\n"
-        "psubsh $f27, $f27, $f21                \r\n"
-        "psubsh $f28, $f28, $f22                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f13                \r\n"
-        "paddsh $f28, $f28, $f14                \r\n"
-        "paddsh $f27, $f27, $f23                \r\n"
-        "paddsh $f28, $f28, $f24                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f14, $f27, $f28             \r\n"
-        "sdc1 $f14, 0(%[dst6])                  \r\n"
-        "paddsh $f27, $f19, $f21                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f20, $f22                \r\n"//src7+src8
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f17                \r\n"
-        "psubsh $f28, $f28, $f18                \r\n"
-        "psubsh $f27, $f27, $f23                \r\n"
-        "psubsh $f28, $f28, $f24                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f15                \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f27, $f27, $f25                \r\n"
-        "paddsh $f28, $f28, $f26                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f16, $f27, $f28             \r\n"
-        "sdc1 $f16, 0(%[dst7])                  \r\n"
-        ::[dst0]"r"(dst),               [dst1]"r"(dst+dstStride),
-          [dst2]"r"(dst+2*dstStride),   [dst3]"r"(dst+3*dstStride),
-          [dst4]"r"(dst+4*dstStride),   [dst5]"r"(dst+5*dstStride),
-          [dst6]"r"(dst+6*dstStride),   [dst7]"r"(dst+7*dstStride),
-          [srcB]"r"(src-2*srcStride),   [srcA]"r"(src-srcStride),
-          [src0]"r"(src),               [src1]"r"(src+srcStride),
-          [src2]"r"(src+2*srcStride),   [src3]"r"(src+3*srcStride),
-          [src4]"r"(src+4*srcStride),   [src5]"r"(src+5*srcStride),
-          [src6]"r"(src+6*srcStride),   [src7]"r"(src+7*srcStride),
-          [src8]"r"(src+8*srcStride),   [src9]"r"(src+9*srcStride),
-          [src10]"r"(src+10*srcStride), [ff_pw_4]"f"(ff_pw_4),
-          [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
-        : "$f0","$f1","$f2","$f3","$f4","$f5","$f6","$f7","$f8","$f9","$f10",
-          "$f11","$f12","$f13","$f14","$f15","$f16","$f17","$f18","$f19",
-          "$f20","$f21","$f22","$f23","$f24","$f25","$f26","$f27","$f28"
-    );
+    int w = 2;
+    int h = 8;
+    double ftmp[10];
+    uint64_t tmp[1];
+    int low32;
+
+    src -= 2 * srcStride;
+
+    while (w--) {
+        __asm__ volatile (
+            ".set       push                                            \n\t"
+            ".set       noreorder                                       \n\t"
+            "dli        %[tmp0],    0x02                                \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp8]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "dli        %[tmp0],    0x05                                \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3] ,  %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "bne        %[h],       0x10,           2f                  \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "uld        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "2:                                                         \n\t"
+            ".set       pop                                             \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [src]"+&r"(src),              [dst]"+&r"(dst),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [dstStride]"r"((mips_reg)dstStride),
+              [srcStride]"r"((mips_reg)srcStride),
+              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+            : "memory"
+        );
+
+        src += 4 - (h + 5) * srcStride;
+        dst += 4 - h * dstStride;
+    }
 }
 
 static void put_h264_qpel16_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
@@ -933,365 +930,466 @@ static void put_h264_qpel16_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
 static void avg_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    double ftmp[10];
+    uint64_t tmp[1];
+
+    src -= 2 * srcStride;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "gslwlc1 $f2, 3(%[srcB])                \r\n"
-        "gslwrc1 $f2, 0(%[srcB])                \r\n"
-        "gslwlc1 $f4, 3(%[srcA])                \r\n"
-        "gslwrc1 $f4, 0(%[srcA])                \r\n"
-        "gslwlc1 $f6, 3(%[src0])                \r\n"
-        "gslwrc1 $f6, 0(%[src0])                \r\n"
-        "gslwlc1 $f8, 3(%[src1])                \r\n"
-        "gslwrc1 $f8, 0(%[src1])                \r\n"
-        "gslwlc1 $f10, 3(%[src2])               \r\n"
-        "gslwrc1 $f10, 0(%[src2])               \r\n"
-        "gslwlc1 $f12, 3(%[src3])               \r\n"
-        "gslwrc1 $f12, 0(%[src3])               \r\n"
-        "gslwlc1 $f14, 3(%[src4])               \r\n"
-        "gslwrc1 $f14, 0(%[src4])               \r\n"
-        "gslwlc1 $f16, 3(%[src5])               \r\n"
-        "gslwrc1 $f16, 0(%[src5])               \r\n"
-        "gslwlc1 $f18, 3(%[src6])               \r\n"
-        "gslwrc1 $f18, 0(%[src6])               \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "punpcklbh $f14, $f14, $f0              \r\n"
-        "punpcklbh $f16, $f16, $f0              \r\n"
-        "punpcklbh $f18, $f18, $f0              \r\n"
-        "paddsh $f20, $f6, $f8                  \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f4, $f10                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f24, $f20, $f22                \r\n"
-        "paddsh $f24, $f24, $f2                 \r\n"
-        "paddsh $f24, $f24, $f12                \r\n"
-        "paddsh $f20, $f8, $f10                 \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f6, $f12                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f26, $f20, $f22                \r\n"
-        "paddsh $f26, $f26, $f4                 \r\n"
-        "paddsh $f26, $f26, $f14                \r\n"
-        "paddsh $f20, $f10, $f12                \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f8, $f14                 \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f28, $f20, $f22                \r\n"
-        "paddsh $f28, $f28, $f6                 \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f20, $f12, $f14                \r\n"
-        "pmullh $f20, $f20, %[ff_pw_20]         \r\n"
-        "paddsh $f22, $f10, $f16                \r\n"
-        "pmullh $f22, $f22, %[ff_pw_5]          \r\n"
-        "psubsh $f30, $f20, $f22                \r\n"
-        "paddsh $f30, $f30, $f8                 \r\n"
-        "paddsh $f30, $f30, $f18                \r\n"
-        "paddsh $f24, $f24, %[ff_pw_16]         \r\n"
-        "paddsh $f26, $f26, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "paddsh $f30, $f30, %[ff_pw_16]         \r\n"
-        "psrah $f24, $f24, %[ff_pw_5]           \r\n"
-        "psrah $f26, $f26, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "psrah $f30, $f30, %[ff_pw_5]           \r\n"
-        "packushb $f24, $f24, $f0               \r\n"
-        "packushb $f26, $f26, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "packushb $f30, $f30, $f0               \r\n"
-        "lwc1 $f2, 0(%[dst0])                   \r\n"
-        "lwc1 $f4, 0(%[dst1])                   \r\n"
-        "lwc1 $f6, 0(%[dst2])                   \r\n"
-        "lwc1 $f8, 0(%[dst3])                   \r\n"
-        "pavgb $f24, $f2, $f24                  \r\n"
-        "pavgb $f26, $f4, $f26                  \r\n"
-        "pavgb $f28, $f6, $f28                  \r\n"
-        "pavgb $f30, $f8, $f30                  \r\n"
-        "swc1 $f24, 0(%[dst0])                  \r\n"
-        "swc1 $f26, 0(%[dst1])                  \r\n"
-        "swc1 $f28, 0(%[dst2])                  \r\n"
-        "swc1 $f30, 0(%[dst3])                  \r\n"
-        ::[dst0]"r"(dst),               [dst1]"r"(dst+dstStride),
-          [dst2]"r"(dst+2*dstStride),   [dst3]"r"(dst+3*dstStride),
-          [srcB]"r"(src-2*srcStride),   [srcA]"r"(src-srcStride),
-          [src0]"r"(src),               [src1]"r"(src+srcStride),
-          [src2]"r"(src+2*srcStride),   [src3]"r"(src+3*srcStride),
-          [src4]"r"(src+4*srcStride),   [src5]"r"(src+5*srcStride),
-          [src6]"r"(src+6*srcStride),   [ff_pw_20]"f"(ff_pw_20),
+        ".set       push                                                \n\t"
+        ".set       noreorder                                           \n\t"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
+        "dli        %[tmp0],    0x05                                    \n\t"
+        "lwc1       %[ftmp0],   0x00(%[src])                            \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "lwc1       %[ftmp1],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "lwc1       %[ftmp2],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "lwc1       %[ftmp3],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "lwc1       %[ftmp4],   0x00(%[src])                            \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "lwc1       %[ftmp5],   0x00(%[src])                            \n\t"
+        "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "lwc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "swc1       %[ftmp6],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "lwc1       %[ftmp0],   0x00(%[src])                            \n\t"
+        "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "lwc1       %[ftmp1],   0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "swc1       %[ftmp6],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "lwc1       %[ftmp1],   0x00(%[src])                            \n\t"
+        "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]                \n\t"
+        "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "lwc1       %[ftmp2],   0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "swc1       %[ftmp6],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        "lwc1       %[ftmp2],   0x00(%[src])                            \n\t"
+        "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]                \n\t"
+        "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]              \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]             \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "lwc1       %[ftmp3],   0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "swc1       %[ftmp6],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        ".set       pop                                                 \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),
+          [src]"+&r"(src),              [dst]"+&r"(dst)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [srcStride]"r"((mips_reg)srcStride),
           [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
-        : "$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16","$f18",
-          "$f20","$f22","$f24","$f26","$f28","$f30"
+        : "memory"
     );
 }
 
 static void avg_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
-    __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "gsldlc1 $f2, 7(%[srcB])                \r\n"
-        "gsldrc1 $f2, 0(%[srcB])                \r\n"
-        "gsldlc1 $f4, 7(%[srcA])                \r\n"
-        "gsldrc1 $f4, 0(%[srcA])                \r\n"
-        "gsldlc1 $f6, 7(%[src0])                \r\n"
-        "gsldrc1 $f6, 0(%[src0])                \r\n"
-        "gsldlc1 $f8, 7(%[src1])                \r\n"
-        "gsldrc1 $f8, 0(%[src1])                \r\n"
-        "gsldlc1 $f10, 7(%[src2])               \r\n"
-        "gsldrc1 $f10, 0(%[src2])               \r\n"
-        "gsldlc1 $f12, 7(%[src3])               \r\n"
-        "gsldrc1 $f12, 0(%[src3])               \r\n"
-        "gsldlc1 $f14, 7(%[src4])               \r\n"
-        "gsldrc1 $f14, 0(%[src4])               \r\n"
-        "gsldlc1 $f16, 7(%[src5])               \r\n"
-        "gsldrc1 $f16, 0(%[src5])               \r\n"
-        "gsldlc1 $f18, 7(%[src6])               \r\n"
-        "gsldrc1 $f18, 0(%[src6])               \r\n"
-        "gsldlc1 $f20, 7(%[src7])               \r\n"
-        "gsldrc1 $f20, 0(%[src7])               \r\n"
-        "gsldlc1 $f22, 7(%[src8])               \r\n"
-        "gsldrc1 $f22, 0(%[src8])               \r\n"
-        "gsldlc1 $f24, 7(%[src9])               \r\n"
-        "gsldrc1 $f24, 0(%[src9])               \r\n"
-        "gsldlc1 $f26, 7(%[src10])              \r\n"
-        "gsldrc1 $f26, 0(%[src10])              \r\n"
-        "punpcklbh $f1, $f2, $f0                \r\n"
-        "punpckhbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f3, $f4, $f0                \r\n"
-        "punpckhbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f5, $f6, $f0                \r\n"
-        "punpckhbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f7, $f8, $f0                \r\n"
-        "punpckhbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f9, $f10, $f0               \r\n"
-        "punpckhbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f11, $f12, $f0              \r\n"
-        "punpckhbh $f12, $f12, $f0              \r\n"
-        "punpcklbh $f13, $f14, $f0              \r\n"
-        "punpckhbh $f14, $f14, $f0              \r\n"
-        "punpcklbh $f15, $f16, $f0              \r\n"
-        "punpckhbh $f16, $f16, $f0              \r\n"
-        "punpcklbh $f17, $f18, $f0              \r\n"
-        "punpckhbh $f18, $f18, $f0              \r\n"
-        "punpcklbh $f19, $f20, $f0              \r\n"
-        "punpckhbh $f20, $f20, $f0              \r\n"
-        "punpcklbh $f21, $f22, $f0              \r\n"
-        "punpckhbh $f22, $f22, $f0              \r\n"
-        "punpcklbh $f23, $f24, $f0              \r\n"
-        "punpckhbh $f24, $f24, $f0              \r\n"
-        "punpcklbh $f25, $f26, $f0              \r\n"
-        "punpckhbh $f26, $f26, $f0              \r\n"
-        "paddsh $f27, $f5, $f7                  \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f6, $f8                  \r\n"//src0+src1
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f3                 \r\n"
-        "psubsh $f28, $f28, $f4                 \r\n"
-        "psubsh $f27, $f27, $f9                 \r\n"
-        "psubsh $f28, $f28, $f10                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f1                 \r\n"
-        "paddsh $f28, $f28, $f2                 \r\n"
-        "paddsh $f27, $f27, $f11                \r\n"
-        "paddsh $f28, $f28, $f12                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f2, $f27, $f28              \r\n"
-        "ldc1 $f28, 0(%[dst0])                  \r\n"
-        "pavgb $f2, $f2, $f28                   \r\n"
-        "sdc1 $f2, 0(%[dst0])                   \r\n"
-        "paddsh $f27, $f7, $f9                  \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f8, $f10                 \r\n"//src1+src2
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f5                 \r\n"
-        "psubsh $f28, $f28, $f6                 \r\n"
-        "psubsh $f27, $f27, $f11                \r\n"
-        "psubsh $f28, $f28, $f12                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f3                 \r\n"
-        "paddsh $f28, $f28, $f4                 \r\n"
-        "paddsh $f27, $f27, $f13                \r\n"
-        "paddsh $f28, $f28, $f14                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f4, $f27, $f28              \r\n"
-        "ldc1 $f28, 0(%[dst1])                  \r\n"
-        "pavgb $f4, $f4, $f28                   \r\n"
-        "sdc1 $f4, 0(%[dst1])                   \r\n"
-        "paddsh $f27, $f9, $f11                 \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f10, $f12                \r\n"//src2+src3
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f7                 \r\n"
-        "psubsh $f28, $f28, $f8                 \r\n"
-        "psubsh $f27, $f27, $f13                \r\n"
-        "psubsh $f28, $f28, $f14                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f5                 \r\n"
-        "paddsh $f28, $f28, $f6                 \r\n"
-        "paddsh $f27, $f27, $f15                \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f6, $f27, $f28              \r\n"
-        "ldc1 $f28, 0(%[dst2])                  \r\n"
-        "pavgb $f6, $f6, $f28                   \r\n"
-        "sdc1 $f6, 0(%[dst2])                   \r\n"
-        "paddsh $f27, $f11, $f13                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f12, $f14                \r\n"//src3+src4
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f9                 \r\n"
-        "psubsh $f28, $f28, $f10                \r\n"
-        "psubsh $f27, $f27, $f15                \r\n"
-        "psubsh $f28, $f28, $f16                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f7                 \r\n"
-        "paddsh $f28, $f28, $f8                 \r\n"
-        "paddsh $f27, $f27, $f17                \r\n"
-        "paddsh $f28, $f28, $f18                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f8, $f27, $f28              \r\n"
-        "ldc1 $f28, 0(%[dst3])                  \r\n"
-        "pavgb $f8, $f8, $f28                   \r\n"
-        "sdc1 $f8, 0(%[dst3])                   \r\n"
-        "paddsh $f27, $f13, $f15                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f14, $f16                \r\n"//src4+src5
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f11                \r\n"
-        "psubsh $f28, $f28, $f12                \r\n"
-        "psubsh $f27, $f27, $f17                \r\n"
-        "psubsh $f28, $f28, $f18                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f9                 \r\n"
-        "paddsh $f28, $f28, $f10                \r\n"
-        "paddsh $f27, $f27, $f19                \r\n"
-        "paddsh $f28, $f28, $f20                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f10, $f27, $f28             \r\n"
-        "ldc1 $f28, 0(%[dst4])                  \r\n"
-        "pavgb $f10, $f10, $f28                 \r\n"
-        "sdc1 $f10, 0(%[dst4])                  \r\n"
-        "paddsh $f27, $f15, $f17                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f16, $f18                \r\n"//src5+src6
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f13                \r\n"
-        "psubsh $f28, $f28, $f14                \r\n"
-        "psubsh $f27, $f27, $f19                \r\n"
-        "psubsh $f28, $f28, $f20                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f11                \r\n"
-        "paddsh $f28, $f28, $f12                \r\n"
-        "paddsh $f27, $f27, $f21                \r\n"
-        "paddsh $f28, $f28, $f22                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f12, $f27, $f28             \r\n"
-        "ldc1 $f28, 0(%[dst5])                  \r\n"
-        "pavgb $f12, $f12, $f28                 \r\n"
-        "sdc1 $f12, 0(%[dst5])                  \r\n"
-        "paddsh $f27, $f17, $f19                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f18, $f20                \r\n"//src6+src7
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f15                \r\n"
-        "psubsh $f28, $f28, $f16                \r\n"
-        "psubsh $f27, $f27, $f21                \r\n"
-        "psubsh $f28, $f28, $f22                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f13                \r\n"
-        "paddsh $f28, $f28, $f14                \r\n"
-        "paddsh $f27, $f27, $f23                \r\n"
-        "paddsh $f28, $f28, $f24                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f14, $f27, $f28             \r\n"
-        "ldc1 $f28, 0(%[dst6])                  \r\n"
-        "pavgb $f14, $f14, $f28                 \r\n"
-        "sdc1 $f14, 0(%[dst6])                  \r\n"
-        "paddsh $f27, $f19, $f21                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_4]          \r\n"
-        "paddsh $f28, $f20, $f22                \r\n"//src7+src8
-        "pmullh $f28, $f28, %[ff_pw_4]          \r\n"
-        "psubsh $f27, $f27, $f17                \r\n"
-        "psubsh $f28, $f28, $f18                \r\n"
-        "psubsh $f27, $f27, $f23                \r\n"
-        "psubsh $f28, $f28, $f24                \r\n"
-        "pmullh $f27, $f27, %[ff_pw_5]          \r\n"
-        "pmullh $f28, $f28, %[ff_pw_5]          \r\n"
-        "paddsh $f27, $f27, $f15                \r\n"
-        "paddsh $f28, $f28, $f16                \r\n"
-        "paddsh $f27, $f27, $f25                \r\n"
-        "paddsh $f28, $f28, $f26                \r\n"
-        "paddsh $f27, $f27, %[ff_pw_16]         \r\n"
-        "paddsh $f28, $f28, %[ff_pw_16]         \r\n"
-        "psrah $f27, $f27, %[ff_pw_5]           \r\n"
-        "psrah $f28, $f28, %[ff_pw_5]           \r\n"
-        "packushb $f27, $f27, $f0               \r\n"
-        "packushb $f28, $f28, $f0               \r\n"
-        "punpcklwd $f16, $f27, $f28             \r\n"
-        "ldc1 $f28, 0(%[dst7])                  \r\n"
-        "pavgb $f16, $f16, $f28                 \r\n"
-        "sdc1 $f16, 0(%[dst7])                  \r\n"
-        ::[dst0]"r"(dst),               [dst1]"r"(dst+dstStride),
-          [dst2]"r"(dst+2*dstStride),   [dst3]"r"(dst+3*dstStride),
-          [dst4]"r"(dst+4*dstStride),   [dst5]"r"(dst+5*dstStride),
-          [dst6]"r"(dst+6*dstStride),   [dst7]"r"(dst+7*dstStride),
-          [srcB]"r"(src-2*srcStride),   [srcA]"r"(src-srcStride),
-          [src0]"r"(src),               [src1]"r"(src+srcStride),
-          [src2]"r"(src+2*srcStride),   [src3]"r"(src+3*srcStride),
-          [src4]"r"(src+4*srcStride),   [src5]"r"(src+5*srcStride),
-          [src6]"r"(src+6*srcStride),   [src7]"r"(src+7*srcStride),
-          [src8]"r"(src+8*srcStride),   [src9]"r"(src+9*srcStride),
-          [src10]"r"(src+10*srcStride), [ff_pw_4]"f"(ff_pw_4),
-          [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
-        : "$f0","$f1","$f2","$f3","$f4","$f5","$f6","$f7","$f8","$f9","$f10",
-          "$f11","$f12","$f13","$f14","$f15","$f16","$f17","$f18","$f19",
-          "$f20","$f21","$f22","$f23","$f24","$f25","$f26","$f27","$f28"
-    );
+    int w = 2;
+    int h = 8;
+    double ftmp[10];
+    uint64_t tmp[1];
+    int low32;
+
+    src -= 2 * srcStride;
+
+    while (w--) {
+        __asm__ volatile (
+            ".set       push                                            \n\t"
+            ".set       noreorder                                       \n\t"
+            "dli        %[tmp0],    0x02                                \n\t"
+            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            "dli        %[tmp0],    0x05                                \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp8]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp3],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp4],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp5],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "bne        %[h],       0x10,           2f                  \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp3],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp4],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp5],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp1],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp2],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psrah      %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]            \n\t"
+            "lwc1       %[ftmp3],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "swc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "2:                                                         \n\t"
+            ".set       pop                                             \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [src]"+&r"(src),              [dst]"+&r"(dst),
+              [h]"+&r"(h),
+              [low32]"=&r"(low32)
+            : [dstStride]"r"((mips_reg)dstStride),
+              [srcStride]"r"((mips_reg)srcStride),
+              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+            : "memory"
+        );
+
+        src += 4 - (h + 5) * srcStride;
+        dst += 4 - h * dstStride;
+    }
 }
 
 static void avg_h264_qpel16_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
@@ -1308,53 +1406,67 @@ static void avg_h264_qpel16_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
 static void put_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    INIT_CLIP
     int i;
     int16_t _tmp[36];
     int16_t *tmp = _tmp;
+    double ftmp[10];
+    uint64_t tmp0;
+    int low32;
+
     src -= 2*srcStride;
+
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 9                              \r\n"
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 1(%[src])                 \r\n"
-        "gslwrc1 $f2, -2(%[src])                \r\n"
-        "gslwlc1 $f4, 2(%[src])                 \r\n"
-        "gslwrc1 $f4, -1(%[src])                \r\n"
-        "gslwlc1 $f6, 3(%[src])                 \r\n"
-        "gslwrc1 $f6, 0(%[src])                 \r\n"
-        "gslwlc1 $f8, 4(%[src])                 \r\n"
-        "gslwrc1 $f8, 1(%[src])                 \r\n"
-        "gslwlc1 $f10, 5(%[src])                \r\n"
-        "gslwrc1 $f10, 2(%[src])                \r\n"
-        "gslwlc1 $f12, 6(%[src])                \r\n"
-        "gslwrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-        "sdc1 $f18, 0(%[tmp])                   \r\n"
-        "dadd %[tmp], %[tmp], %[tmpStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [tmp]"+&r"(tmp),[src]"+&r"(src)
-        : [tmpStride]"r"(8),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16","$f18"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x09                                    \n\t"
+        "1:                                                             \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   -0x01(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x01(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x02(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "ulw        %[low32],   0x03(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp1],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp8],   %[ftmp8],       %[ff_pw_5]              \n\t"
+        "psubsh     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp7],       %[ftmp9]                \n\t"
+        "sdc1       %[ftmp9],   0x00(%[tmp])                            \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[tmp],     %[tmp],         %[tmpStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp0),
+          [tmp]"+&r"(tmp),                  [src]"+&r"(src),
+          [low32]"=&r"(low32)
+        : [tmpStride]"r"(8),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5)
+        : "memory"
     );
 
     tmp -= 28;
 
-    for(i=0; i<4; i++) {
+    for (i=0; i<4; i++) {
         const int16_t tmpB= tmp[-8];
         const int16_t tmpA= tmp[-4];
         const int16_t tmp0= tmp[ 0];
@@ -1373,161 +1485,665 @@ static void put_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     }
 }
 
-static void put_h264_qpel8_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
-        int dstStride, int srcStride)
-{
-    int16_t _tmp[104];
-    int16_t *tmp = _tmp;
-    int i;
-    src -= 2*srcStride;
+static void put_h264_qpel8or16_hv1_lowpass_mmi(int16_t *tmp,
+        const uint8_t *src, ptrdiff_t tmpStride, ptrdiff_t srcStride, int size)
+{
+    int w = (size + 8) >> 2;
+    double ftmp[11];
+    uint64_t tmp0;
+    int low32;
+
+    src -= 2 * srcStride + 2;
+
+    while (w--) {
+        __asm__ volatile (
+            "dli        %[tmp0],    0x02                                \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp10]                           \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "sdc1       %[ftmp6],   0x00(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "sdc1       %[ftmp6],   0x30(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "sdc1       %[ftmp6],   0x60(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "sdc1       %[ftmp6],   0x90(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "sdc1       %[ftmp6],   0xc0(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "sdc1       %[ftmp6],   0xf0(%[tmp])                        \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "sdc1       %[ftmp6],   0x120(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "sdc1       %[ftmp6],   0x150(%[tmp])                       \n\t"
+            "bne        %[size],    0x10,           2f                  \n\t"
+
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "sdc1       %[ftmp6],   0x180(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "sdc1       %[ftmp6],   0x1b0(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp3]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "sdc1       %[ftmp6],   0x1e0(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp4]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "sdc1       %[ftmp6],   0x210(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp5]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "sdc1       %[ftmp6],   0x240(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp0]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp5]            \n\t"
+            "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "sdc1       %[ftmp6],   0x270(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp4],       %[ftmp5]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp1]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp2]            \n\t"
+            "sdc1       %[ftmp6],   0x2a0(%[tmp])                       \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "paddh      %[ftmp6],   %[ftmp5],       %[ftmp0]            \n\t"
+            "psllh      %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]         \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ff_pw_5]          \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "sdc1       %[ftmp6],   0x2d0(%[tmp])                       \n\t"
+            "2:                                                         \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),
+              [tmp0]"=&r"(tmp0),
+              [src]"+&r"(src),
+              [low32]"=&r"(low32)
+            : [tmp]"r"(tmp),                [size]"r"(size),
+              [srcStride]"r"((mips_reg)srcStride),
+              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+            : "memory"
+        );
+
+        tmp += 4;
+        src += 4 - (size + 5) * srcStride;
+    }
+}
+
+static void put_h264_qpel8or16_hv2_lowpass_mmi(uint8_t *dst,
+        int16_t *tmp, ptrdiff_t dstStride, ptrdiff_t tmpStride, int size)
+{
+    int w = size >> 4;
+    double ftmp[10];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    do {
+        int h = size;
+
+        __asm__ volatile (
+            "dli        %[tmp0],    0x02                                \n\t"
+            "mtc1       %[tmp0],    %[ftmp8]                            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            "1:                                                         \n\t"
+            "ldc1       %[ftmp0],   0x00(%[tmp])                        \n\t"
+            "ldc1       %[ftmp3],   0x08(%[tmp])                        \n\t"
+            "ldc1       %[ftmp6],   0x10(%[tmp])                        \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp4],   0x11(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp4],   0x0a(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp5],   0x19(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp5],   0x12(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+            "uld        %[all64],   0x12(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp5]                            \n\t"
+#endif
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]            \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp2],   0x0b(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x04(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp6],   0x0d(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp6],   0x06(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp5],   0x13(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp5],   0x0c(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x15(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp7],   0x0e(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x04(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x06(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp6]                            \n\t"
+            "uld        %[all64],   0x0c(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp5]                            \n\t"
+            "uld        %[all64],   0x0e(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp6]            \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "psubh      %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp8]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp8]            \n\t"
+            "psubh      %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp3] ,  %[ftmp3],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp8]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp8]            \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp9]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp9]            \n\t"
+            "packushb   %[ftmp0],   %[ftmp0],       %[ftmp3]            \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp0],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp0],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp0]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            PTR_ADDIU  "%[tmp],     %[tmp],         0x30                \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64),
+              [tmp]"+&r"(tmp),              [dst]"+&r"(dst),
+              [h]"+&r"(h)
+            : [dstStride]"r"((mips_reg)dstStride)
+            : "memory"
+        );
+
+        tmp += 8 - size * 24;
+        dst += 8 - size * dstStride;
+    } while (w--);
+}
+
+static void put_h264_qpel8or16_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride, int size)
+{
+    put_h264_qpel8or16_hv1_lowpass_mmi(tmp, src, tmpStride, srcStride, size);
+    put_h264_qpel8or16_hv2_lowpass_mmi(dst, tmp, dstStride, tmpStride, size);
+}
+
+static void put_h264_qpel8_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride)
+{
+    put_h264_qpel8or16_hv_lowpass_mmi(dst, tmp, src, dstStride, tmpStride,
+            srcStride, 8);
+}
+
+static void put_h264_qpel16_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride)
+{
+    put_h264_qpel8or16_hv_lowpass_mmi(dst, tmp, src, dstStride, tmpStride,
+            srcStride, 16);
+}
+
+static void put_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
+        const uint8_t *src2, ptrdiff_t dstStride, ptrdiff_t src2Stride)
+{
+    int h = 8;
+    double ftmp[9];
+    uint64_t tmp[1];
+    int low32;
+    uint64_t all64;
 
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 13                             \r\n"
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 5(%[src])                 \r\n"
-        "gsldrc1 $f2, -2(%[src])                \r\n"
-        "gsldlc1 $f4, 6(%[src])                 \r\n"
-        "gsldrc1 $f4, -1(%[src])                \r\n"
-        "gsldlc1 $f6, 7(%[src])                 \r\n"
-        "gsldrc1 $f6, 0(%[src])                 \r\n"
-        "gsldlc1 $f8, 8(%[src])                 \r\n"
-        "gsldrc1 $f8, 1(%[src])                 \r\n"
-        "gsldlc1 $f10, 9(%[src])                \r\n"
-        "gsldrc1 $f10, 2(%[src])                \r\n"
-        "gsldlc1 $f12, 10(%[src])               \r\n"
-        "gsldrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f1, $f2, $f0                \r\n"
-        "punpcklbh $f3, $f4, $f0                \r\n"
-        "punpcklbh $f5, $f6, $f0                \r\n"
-        "punpcklbh $f7, $f8, $f0                \r\n"
-        "punpcklbh $f9, $f10, $f0               \r\n"
-        "punpcklbh $f11, $f12, $f0              \r\n"
-        "punpckhbh $f2, $f2, $f0                \r\n"
-        "punpckhbh $f4, $f4, $f0                \r\n"
-        "punpckhbh $f6, $f6, $f0                \r\n"
-        "punpckhbh $f8, $f8, $f0                \r\n"
-        "punpckhbh $f10, $f10, $f0              \r\n"
-        "punpckhbh $f12, $f12, $f0              \r\n"
-        "paddsh $f13, $f5, $f7                  \r\n"
-        "paddsh $f15, $f3, $f9                 \r\n"
-        "paddsh $f17, $f1, $f11                 \r\n"
-        "pmullh $f13, $f13, %[ff_pw_20]         \r\n"
-        "pmullh $f15, $f15, %[ff_pw_5]          \r\n"
-        "psubsh $f13, $f13, $f15                \r\n"
-        "paddsh $f17, $f13, $f17                \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-        "sdc1 $f17, 0(%[tmp])                   \r\n"
-        "sdc1 $f18, 8(%[tmp])                   \r\n"
-        "dadd %[tmp], %[tmp], %[tmpStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [tmp]"+&r"(tmp),[src]"+&r"(src)
-        : [tmpStride]"r"(16),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5)
-        : "$8","$f0","$f1","$f2","$f3","$f4","$f5","$f6","$f7","$f8","$f9",
-          "$f10","$f11","$f12","$f13","$f14","$f15","$f16","$f17","$f18"
+        "dli        %[tmp0],    0x02                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp7]                                \n\t"
+        "dli        %[tmp0],    0x05                                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        "gsldlc1    %[ftmp3],   0x08(%[src])                            \n\t"
+        "gsldrc1    %[ftmp3],   0x01(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x01(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp4],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psllh      %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "psllh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp3],   0x06(%[src])                            \n\t"
+        "gsldrc1    %[ftmp3],   -0x01(%[src])                           \n\t"
+        "gsldlc1    %[ftmp5],   0x09(%[src])                            \n\t"
+        "gsldrc1    %[ftmp5],   0x02(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   -0x01(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x02(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp4],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp6],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ff_pw_5]              \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[ff_pw_5]              \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x07(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_16]             \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]             \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp5],   0x07(%[src2])                           \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src2])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[dstStride]            \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        PTR_ADDU   "%[h],       %[h],           -0x01                   \n\t"
+        "sdc1       %[ftmp1],   0x00(%[dst])                            \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        PTR_ADDU   "%[src2],    %[src2],        %[src2Stride]           \n\t"
+        "bgtz       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),
+          [tmp0]"=&r"(tmp[0]),
+          [src]"+&r"(src),                  [dst]"+&r"(dst),
+          [src2]"+&r"(src2),                [h]"+&r"(h),
+          [low32]"=&r"(low32),              [all64]"=&r"(all64)
+        : [src2Stride]"r"((mips_reg)src2Stride),
+          [dstStride]"r"((mips_reg)dstStride),
+          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
-
-    tmp -= 88;
-
-    for(i=0; i<8; i++) {
-        const int tmpB= tmp[-16];
-        const int tmpA= tmp[ -8];
-        const int tmp0= tmp[  0];
-        const int tmp1= tmp[  8];
-        const int tmp2= tmp[ 16];
-        const int tmp3= tmp[ 24];
-        const int tmp4= tmp[ 32];
-        const int tmp5= tmp[ 40];
-        const int tmp6= tmp[ 48];
-        const int tmp7= tmp[ 56];
-        const int tmp8= tmp[ 64];
-        const int tmp9= tmp[ 72];
-        const int tmp10=tmp[ 80];
-        op2_put(dst[0*dstStride], (tmp0+tmp1)*20 - (tmpA+tmp2)*5 + (tmpB+tmp3));
-        op2_put(dst[1*dstStride], (tmp1+tmp2)*20 - (tmp0+tmp3)*5 + (tmpA+tmp4));
-        op2_put(dst[2*dstStride], (tmp2+tmp3)*20 - (tmp1+tmp4)*5 + (tmp0+tmp5));
-        op2_put(dst[3*dstStride], (tmp3+tmp4)*20 - (tmp2+tmp5)*5 + (tmp1+tmp6));
-        op2_put(dst[4*dstStride], (tmp4+tmp5)*20 - (tmp3+tmp6)*5 + (tmp2+tmp7));
-        op2_put(dst[5*dstStride], (tmp5+tmp6)*20 - (tmp4+tmp7)*5 + (tmp3+tmp8));
-        op2_put(dst[6*dstStride], (tmp6+tmp7)*20 - (tmp5+tmp8)*5 + (tmp4+tmp9));
-        op2_put(dst[7*dstStride], (tmp7+tmp8)*20 - (tmp6+tmp9)*5 + (tmp5+tmp10));
-        dst++;
-        tmp++;
-    }
 }
 
-static void put_h264_qpel16_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
-        int dstStride, int srcStride)
-{
-    put_h264_qpel8_hv_lowpass_mmi(dst, src, dstStride, srcStride);
-    put_h264_qpel8_hv_lowpass_mmi(dst+8, src+8, dstStride, srcStride);
-    src += 8*srcStride;
-    dst += 8*dstStride;
-    put_h264_qpel8_hv_lowpass_mmi(dst, src, dstStride, srcStride);
-    put_h264_qpel8_hv_lowpass_mmi(dst+8, src+8, dstStride, srcStride);
+static void put_pixels8_l2_shift5_mmi(uint8_t *dst, int16_t *src16,
+        const uint8_t *src8, ptrdiff_t dstStride, ptrdiff_t src8Stride, int h)
+{
+    double ftmp[7];
+    uint64_t tmp0;
+    mips_reg addr[1];
+    uint64_t all64;
+
+    do {
+        __asm__ volatile (
+            "dli        %[tmp0],    0x05                                \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp0],   0x07(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp0],   0x00(%[src16])                      \n\t"
+            "mtc1       %[tmp0],    %[ftmp6]                            \n\t"
+            "gsldlc1    %[ftmp1],   0x0f(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[src16])                      \n\t"
+            "gsldlc1    %[ftmp2],   0x37(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp2],   0x30(%[src16])                      \n\t"
+            "gsldlc1    %[ftmp3],   0x3f(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp3],   0x38(%[src16])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp0]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp6]                            \n\t"
+            "uld        %[all64],   0x08(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x30(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x38(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+#endif
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp2],   %[ftmp2],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "ldc1       %[ftmp5],   0x00(%[src8])                       \n\t"
+#if HAVE_LOONGSON3
+            "gsldxc1    %[ftmp4],   0x00(%[src8],   %[src8Stride])      \n\t"
+#elif HAVE_LOONGSON2
+            PTR_ADDU   "%[addr0],   %[src8],        %[src8Stride]       \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp5]            \n\t"
+            "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "sdc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+#if HAVE_LOONGSON3
+            "gssdxc1    %[ftmp2],   0x00(%[dst],    %[dstStride])       \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            PTR_ADDU   "%[addr0],   %[dst],         %[dstStride]        \n\t"
+            "usd        %[all64],   0x00(%[addr0])                      \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64)
+            : [src8]"r"(src8),              [src16]"r"(src16),
+              [dst]"r"(dst),
+              [src8Stride]"r"((mips_reg)src8Stride),
+              [dstStride]"r"((mips_reg)dstStride)
+            : "memory"
+        );
+
+        src8  += 2 * src8Stride;
+        src16 += 48;
+        dst   += 2 * dstStride;
+    } while (h -= 2);
+}
+
+static void put_h264_qpel16_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
+        const uint8_t *src2, ptrdiff_t dstStride, ptrdiff_t src2Stride)
+{
+    put_h264_qpel8_h_lowpass_l2_mmi(dst, src, src2, dstStride, src2Stride);
+    put_h264_qpel8_h_lowpass_l2_mmi(dst + 8, src + 8, src2 + 8, dstStride,
+            src2Stride);
+
+    src += 8 * dstStride;
+    dst += 8 * dstStride;
+    src2 += 8 * src2Stride;
+
+    put_h264_qpel8_h_lowpass_l2_mmi(dst, src, src2, dstStride, src2Stride);
+    put_h264_qpel8_h_lowpass_l2_mmi(dst + 8, src + 8, src2 + 8, dstStride,
+            src2Stride);
+}
+
+static void put_pixels16_l2_shift5_mmi(uint8_t *dst, int16_t *src16,
+        const uint8_t *src8, ptrdiff_t dstStride, ptrdiff_t src8Stride, int h)
+{
+    put_pixels8_l2_shift5_mmi(dst, src16, src8, dstStride, src8Stride, h);
+    put_pixels8_l2_shift5_mmi(dst + 8, src16 + 8, src8 + 8, dstStride,
+            src8Stride, h);
 }
 
 static void avg_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         int dstStride, int srcStride)
 {
+    INIT_CLIP
     int i;
     int16_t _tmp[36];
     int16_t *tmp = _tmp;
+    double ftmp[10];
+    uint64_t tmp0;
+    int low32;
+
     src -= 2*srcStride;
 
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 9                              \r\n"
-        "1:                                     \r\n"
-        "gslwlc1 $f2, 1(%[src])                 \r\n"
-        "gslwrc1 $f2, -2(%[src])                \r\n"
-        "gslwlc1 $f4, 2(%[src])                 \r\n"
-        "gslwrc1 $f4, -1(%[src])                \r\n"
-        "gslwlc1 $f6, 3(%[src])                 \r\n"
-        "gslwrc1 $f6, 0(%[src])                 \r\n"
-        "gslwlc1 $f8, 4(%[src])                 \r\n"
-        "gslwrc1 $f8, 1(%[src])                 \r\n"
-        "gslwlc1 $f10, 5(%[src])                \r\n"
-        "gslwrc1 $f10, 2(%[src])                \r\n"
-        "gslwlc1 $f12, 6(%[src])                \r\n"
-        "gslwrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f2, $f2, $f0                \r\n"
-        "punpcklbh $f4, $f4, $f0                \r\n"
-        "punpcklbh $f6, $f6, $f0                \r\n"
-        "punpcklbh $f8, $f8, $f0                \r\n"
-        "punpcklbh $f10, $f10, $f0              \r\n"
-        "punpcklbh $f12, $f12, $f0              \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-        "sdc1 $f18, 0(%[tmp])                   \r\n"
-        "dadd %[tmp], %[tmp], %[tmpStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [tmp]"+&r"(tmp),[src]"+&r"(src)
-        : [tmpStride]"r"(8),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16","$f18"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dli        %[tmp0],    0x09                                    \n\t"
+        "1:                                                             \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   -0x01(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x01(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x02(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "ulw        %[low32],   0x03(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddsh     %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
+        "paddsh     %[ftmp8],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp1],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ff_pw_20]             \n\t"
+        "pmullh     %[ftmp8],   %[ftmp8],       %[ff_pw_5]              \n\t"
+        "psubsh     %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "paddsh     %[ftmp9],   %[ftmp7],       %[ftmp9]                \n\t"
+        "sdc1       %[ftmp9],   0x00(%[tmp])                            \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[srcStride]            \n\t"
+        PTR_ADDU   "%[tmp],     %[tmp],         %[tmpStride]            \n\t"
+        "bnez       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp0),
+          [tmp]"+&r"(tmp),                  [src]"+&r"(src),
+          [low32]"=&r"(low32)
+        : [tmpStride]"r"(8),
+          [srcStride]"r"((mips_reg)srcStride),
+          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5)
+        : "memory"
     );
 
     tmp -= 28;
 
-    for(i=0; i<4; i++)
-    {
+    for (i=0; i<4; i++) {
         const int16_t tmpB= tmp[-8];
         const int16_t tmpA= tmp[-4];
         const int16_t tmp0= tmp[ 0];
@@ -1546,114 +2162,348 @@ static void avg_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     }
 }
 
-static void avg_h264_qpel8_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
-        int dstStride, int srcStride)
-{
-    int16_t _tmp[104];
-    int16_t *tmp = _tmp;
-    int i;
-    src -= 2*srcStride;
+static void avg_h264_qpel8or16_hv2_lowpass_mmi(uint8_t *dst,
+        int16_t *tmp, ptrdiff_t dstStride, ptrdiff_t tmpStride, int size)
+{
+    int w = size >> 4;
+    double ftmp[11];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    do {
+        int h = size;
+        __asm__ volatile (
+            "dli        %[tmp0],    0x02                                \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
+            "dli        %[tmp0],    0x06                                \n\t"
+            "mtc1       %[tmp0],    %[ftmp10]                           \n\t"
+            "1:                                                         \n\t"
+            "ldc1       %[ftmp0],   0x00(%[tmp])                        \n\t"
+            "ldc1       %[ftmp3],   0x08(%[tmp])                        \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp4],   0x11(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp4],   0x0a(%[tmp])                        \n\t"
+            "ldc1       %[ftmp7],   0x10(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp8],   0x19(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp8],   0x12(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+            "ldc1       %[ftmp7],   0x10(%[tmp])                        \n\t"
+            "uld        %[all64],   0x12(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp8]                            \n\t"
+#endif
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp4]            \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp8]            \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp7]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp2],   0x0b(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x04(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp5],   0x13(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp5],   0x0c(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp8],   0x15(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp8],   0x0e(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x04(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x0c(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp5]                            \n\t"
+            "uld        %[all64],   0x06(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+            "uld        %[all64],   0x0e(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp8]                            \n\t"
+#endif
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp8]            \n\t"
+            "psubh      %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp9]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp9]            \n\t"
+            "psubh      %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp9]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp9]            \n\t"
+            "paddh      %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]            \n\t"
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp10]           \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp10]           \n\t"
+            "packushb   %[ftmp0],   %[ftmp0],       %[ftmp3]            \n\t"
+            "ldc1       %[ftmp6],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp6]            \n\t"
+            "sdc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
+            PTR_ADDI   "%[tmp],     %[tmp],         0x30                \n\t"
+            PTR_ADDU   "%[dst],     %[dst],         %[dstStride]        \n\t"
+            "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64),
+              [tmp]"+&r"(tmp),              [dst]"+&r"(dst),
+              [h]"+&r"(h)
+            : [dstStride]"r"((mips_reg)dstStride)
+            : "memory"
+        );
+
+        tmp += 8 - size * 24;
+        dst += 8 - size * dstStride;
+    } while (w--);
+}
+
+static void avg_h264_qpel8or16_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride, int size)
+{
+    put_h264_qpel8or16_hv1_lowpass_mmi(tmp, src, tmpStride, srcStride, size);
+    avg_h264_qpel8or16_hv2_lowpass_mmi(dst, tmp, dstStride, tmpStride, size);
+}
+
+static void avg_h264_qpel8_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride)
+{
+    avg_h264_qpel8or16_hv_lowpass_mmi(dst, tmp, src, dstStride, tmpStride,
+            srcStride, 8);
+}
+
+static void avg_h264_qpel16_hv_lowpass_mmi(uint8_t *dst, int16_t *tmp,
+        const uint8_t *src, ptrdiff_t dstStride, ptrdiff_t tmpStride,
+        ptrdiff_t srcStride)
+{
+    avg_h264_qpel8or16_hv_lowpass_mmi(dst, tmp, src, dstStride, tmpStride,
+            srcStride, 16);
+}
+
+static void avg_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
+        const uint8_t *src2, ptrdiff_t dstStride, ptrdiff_t src2Stride)
+{
+    double ftmp[10];
+    uint64_t tmp[2];
+    int low32;
+    uint64_t all64;
 
     __asm__ volatile (
-        "xor $f0, $f0, $f0                      \r\n"
-        "dli $8, 13                             \r\n"
-        "1:                                     \r\n"
-        "gsldlc1 $f2, 5(%[src])                 \r\n"
-        "gsldrc1 $f2, -2(%[src])                \r\n"
-        "gsldlc1 $f4, 6(%[src])                 \r\n"
-        "gsldrc1 $f4, -1(%[src])                \r\n"
-        "gsldlc1 $f6, 7(%[src])                 \r\n"
-        "gsldrc1 $f6, 0(%[src])                 \r\n"
-        "gsldlc1 $f8, 8(%[src])                 \r\n"
-        "gsldrc1 $f8, 1(%[src])                 \r\n"
-        "gsldlc1 $f10, 9(%[src])                \r\n"
-        "gsldrc1 $f10, 2(%[src])                \r\n"
-        "gsldlc1 $f12, 10(%[src])               \r\n"
-        "gsldrc1 $f12, 3(%[src])                \r\n"
-        "punpcklbh $f1, $f2, $f0                \r\n"
-        "punpcklbh $f3, $f4, $f0                \r\n"
-        "punpcklbh $f5, $f6, $f0                \r\n"
-        "punpcklbh $f7, $f8, $f0                \r\n"
-        "punpcklbh $f9, $f10, $f0               \r\n"
-        "punpcklbh $f11, $f12, $f0              \r\n"
-        "punpckhbh $f2, $f2, $f0                \r\n"
-        "punpckhbh $f4, $f4, $f0                \r\n"
-        "punpckhbh $f6, $f6, $f0                \r\n"
-        "punpckhbh $f8, $f8, $f0                \r\n"
-        "punpckhbh $f10, $f10, $f0              \r\n"
-        "punpckhbh $f12, $f12, $f0              \r\n"
-        "paddsh $f13, $f5, $f7                  \r\n"
-        "paddsh $f15, $f3, $f9                 \r\n"
-        "paddsh $f17, $f1, $f11                 \r\n"
-        "pmullh $f13, $f13, %[ff_pw_20]         \r\n"
-        "pmullh $f15, $f15, %[ff_pw_5]          \r\n"
-        "psubsh $f13, $f13, $f15                \r\n"
-        "paddsh $f17, $f13, $f17                \r\n"
-        "paddsh $f14, $f6, $f8                  \r\n"
-        "paddsh $f16, $f4, $f10                 \r\n"
-        "paddsh $f18, $f2, $f12                 \r\n"
-        "pmullh $f14, $f14, %[ff_pw_20]         \r\n"
-        "pmullh $f16, $f16, %[ff_pw_5]          \r\n"
-        "psubsh $f14, $f14, $f16                \r\n"
-        "paddsh $f18, $f14, $f18                \r\n"
-
-        "sdc1 $f17, 0(%[tmp])                   \r\n"
-        "sdc1 $f18, 8(%[tmp])                   \r\n"
-        "dadd %[tmp], %[tmp], %[tmpStride]      \r\n"
-        "dadd %[src], %[src], %[srcStride]      \r\n"
-        "daddi $8, $8, -1                       \r\n"
-        "bnez $8, 1b                            \r\n"
-        : [tmp]"+&r"(tmp),[src]"+&r"(src)
-        : [tmpStride]"r"(16),[srcStride]"r"(srcStride),
-          [ff_pw_20]"f"(ff_pw_20),[ff_pw_5]"f"(ff_pw_5)
-        : "$8","$f0","$f1","$f2","$f3","$f4","$f5","$f6","$f7","$f8","$f9",
-          "$f10","$f11","$f12","$f13","$f14","$f15","$f16","$f17","$f18"
+        "dli        %[tmp1],    0x02                                    \n\t"
+        "ori        %[tmp0],    $0,             0x8                     \n\t"
+        "mtc1       %[tmp1],    %[ftmp7]                                \n\t"
+        "dli        %[tmp1],    0x05                                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[tmp1],    %[ftmp8]                                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[src])                            \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[src])                            \n\t"
+        "gsldlc1    %[ftmp2],   0x08(%[src])                            \n\t"
+        "gsldrc1    %[ftmp2],   0x01(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x01(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp4],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psllh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "psllh      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp2],   0x06(%[src])                            \n\t"
+        "gsldrc1    %[ftmp2],   -0x01(%[src])                           \n\t"
+        "gsldlc1    %[ftmp5],   0x09(%[src])                            \n\t"
+        "gsldrc1    %[ftmp5],   0x02(%[src])                            \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   -0x01(%[src])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x02(%[src])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "punpckhbh  %[ftmp4],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp6],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[ff_pw_5]              \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ff_pw_5]              \n\t"
+        "ulw        %[low32],   -0x02(%[src])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x07(%[src])                            \n\t"
+        "mtc1       %[low32],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_16]             \n\t"
+        "paddh      %[ftmp5],   %[ftmp5],       %[ff_pw_16]             \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp5],   0x07(%[src2])                           \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src2])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "ldc1       %[ftmp9],   0x00(%[dst])                            \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        PTR_ADDU   "%[src],     %[src],         %[dstStride]            \n\t"
+        "sdc1       %[ftmp1],   0x00(%[dst])                            \n\t"
+        "daddi      %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[dstStride]            \n\t"
+        PTR_ADDU   "%[src2],    %[src2],        %[src2Stride]           \n\t"
+        "bgtz       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [src2]"+&r"(src2),
+          [low32]"=&r"(low32)
+        : [dstStride]"r"((mips_reg)dstStride),
+          [src2Stride]"r"((mips_reg)src2Stride),
+          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+        : "memory"
     );
-
-    tmp -= 88;
-
-    for(i=0; i<8; i++) {
-        const int tmpB= tmp[-16];
-        const int tmpA= tmp[ -8];
-        const int tmp0= tmp[  0];
-        const int tmp1= tmp[  8];
-        const int tmp2= tmp[ 16];
-        const int tmp3= tmp[ 24];
-        const int tmp4= tmp[ 32];
-        const int tmp5= tmp[ 40];
-        const int tmp6= tmp[ 48];
-        const int tmp7= tmp[ 56];
-        const int tmp8= tmp[ 64];
-        const int tmp9= tmp[ 72];
-        const int tmp10=tmp[ 80];
-        op2_avg(dst[0*dstStride], (tmp0+tmp1)*20 - (tmpA+tmp2)*5 + (tmpB+tmp3));
-        op2_avg(dst[1*dstStride], (tmp1+tmp2)*20 - (tmp0+tmp3)*5 + (tmpA+tmp4));
-        op2_avg(dst[2*dstStride], (tmp2+tmp3)*20 - (tmp1+tmp4)*5 + (tmp0+tmp5));
-        op2_avg(dst[3*dstStride], (tmp3+tmp4)*20 - (tmp2+tmp5)*5 + (tmp1+tmp6));
-        op2_avg(dst[4*dstStride], (tmp4+tmp5)*20 - (tmp3+tmp6)*5 + (tmp2+tmp7));
-        op2_avg(dst[5*dstStride], (tmp5+tmp6)*20 - (tmp4+tmp7)*5 + (tmp3+tmp8));
-        op2_avg(dst[6*dstStride], (tmp6+tmp7)*20 - (tmp5+tmp8)*5 + (tmp4+tmp9));
-        op2_avg(dst[7*dstStride], (tmp7+tmp8)*20 - (tmp6+tmp9)*5 + (tmp5+tmp10));
-        dst++;
-        tmp++;
-    }
 }
 
-static void avg_h264_qpel16_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
-        int dstStride, int srcStride){
-    avg_h264_qpel8_hv_lowpass_mmi(dst, src, dstStride, srcStride);
-    avg_h264_qpel8_hv_lowpass_mmi(dst+8, src+8, dstStride, srcStride);
-    src += 8*srcStride;
-    dst += 8*dstStride;
-    avg_h264_qpel8_hv_lowpass_mmi(dst, src, dstStride, srcStride);
-    avg_h264_qpel8_hv_lowpass_mmi(dst+8, src+8, dstStride, srcStride);
+static void avg_h264_qpel16_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
+        const uint8_t *src2, ptrdiff_t dstStride, ptrdiff_t src2Stride)
+{
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst, src, src2, dstStride, src2Stride);
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst + 8, src + 8, src2 + 8, dstStride,
+            src2Stride);
+
+    src += 8 * dstStride;
+    dst += 8 * dstStride;
+    src2 += 8 * src2Stride;
+
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst, src, src2, dstStride, src2Stride);
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst + 8, src + 8, src2 + 8, dstStride,
+            src2Stride);
+}
+
+static void avg_pixels8_l2_shift5_mmi(uint8_t *dst, int16_t *src16,
+        const uint8_t *src8, ptrdiff_t dstStride, ptrdiff_t src8Stride, int b)
+{
+    double ftmp[8];
+    uint64_t tmp0;
+    mips_reg addr[1];
+    uint64_t all64;
+
+    do {
+        __asm__ volatile (
+            "dli        %[tmp0],    0x05                                \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp0],   0x07(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp0],   0x00(%[src16])                      \n\t"
+            "mtc1       %[tmp0],    %[ftmp6]                            \n\t"
+            "gsldlc1    %[ftmp1],   0x0f(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[src16])                      \n\t"
+            "gsldlc1    %[ftmp2],   0x37(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp2],   0x30(%[src16])                      \n\t"
+            "gsldlc1    %[ftmp3],   0x3f(%[src16])                      \n\t"
+            "gsldrc1    %[ftmp3],   0x38(%[src16])                      \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp0]                            \n\t"
+            "mtc1       %[tmp0],    %[ftmp6]                            \n\t"
+            "uld        %[all64],   0x08(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x30(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x38(%[src16])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+#endif
+            "psrah      %[ftmp0],   %[ftmp0],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp1],   %[ftmp1],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp2],   %[ftmp2],       %[ftmp6]            \n\t"
+            "psrah      %[ftmp3],   %[ftmp3],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp0],   %[ftmp0],       %[ftmp1]            \n\t"
+            "ldc1       %[ftmp4],   0x00(%[src8])                       \n\t"
+#if HAVE_LOONGSON3
+            "gsldxc1    %[ftmp5],   0x00(%[src8],   %[src8Stride])      \n\t"
+#elif HAVE_LOONGSON2
+            PTR_ADDU   "%[addr0],   %[src8],        %[src8Stride]       \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp5]                            \n\t"
+#endif
+            "packushb   %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp4]            \n\t"
+            "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp5]            \n\t"
+            "ldc1       %[ftmp7],   0x00(%[dst])                        \n\t"
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp7]            \n\t"
+            "sdc1       %[ftmp0],   0x00(%[dst])                        \n\t"
+#if HAVE_LOONGSON3
+            "gsldxc1    %[ftmp7],   0x00(%[dst],    %[dstStride])       \n\t"
+            "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "gssdxc1    %[ftmp2],   0x00(%[dst],    %[dstStride])       \n\t"
+#elif HAVE_LOONGSON2
+            PTR_ADDU   "%[addr0],   %[dst],         %[dstStride]        \n\t"
+            "uld        %[all64],   0x00(%[addr0])                      \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+            "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp7]            \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            PTR_ADDU   "%[addr0],   %[dst],         %[dstStride]        \n\t"
+            "usd        %[all64],   0x00(%[addr0])                      \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp0),
+              [addr0]"=&r"(addr[0]),
+              [all64]"=&r"(all64)
+            : [src8]"r"(src8),              [src16]"r"(src16),
+              [dst]"r"(dst),
+              [src8Stride]"r"((mips_reg)src8Stride),
+              [dstStride]"r"((mips_reg)dstStride)
+            : "memory"
+        );
+
+        src8  += 2 * src8Stride;
+        src16 += 48;
+        dst   += 2 * dstStride;
+    } while (b -= 2);
+}
+
+static void avg_pixels16_l2_shift5_mmi(uint8_t *dst, int16_t *src16,
+        const uint8_t *src8, ptrdiff_t dstStride, ptrdiff_t src8Stride, int b)
+{
+    avg_pixels8_l2_shift5_mmi(dst, src16, src8, dstStride, src8Stride, b);
+    avg_pixels8_l2_shift5_mmi(dst + 8, src16 + 8, src8 + 8, dstStride,
+            src8Stride, b);
 }
 
 //DEF_H264_MC_MMI(put_, 4)
 void ff_put_h264_qpel4_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    put_pixels4_mmi(dst, src, stride, 4);
+    ff_put_pixels4_8_mmi(dst, src, stride, 4);
 }
 
 void ff_put_h264_qpel4_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -1661,7 +2511,7 @@ void ff_put_h264_qpel4_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[16];
     put_h264_qpel4_h_lowpass_mmi(half, src, 4, stride);
-    put_pixels4_l2_mmi(dst, src, half, stride, stride, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, src, half, stride, stride, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -1675,7 +2525,7 @@ void ff_put_h264_qpel4_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[16];
     put_h264_qpel4_h_lowpass_mmi(half, src, 4, stride);
-    put_pixels4_l2_mmi(dst, src+1, half, stride, stride, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, src+1, half, stride, stride, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -1686,7 +2536,7 @@ void ff_put_h264_qpel4_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[16];
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(half, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, full_mid, half, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, full_mid, half, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -1706,7 +2556,7 @@ void ff_put_h264_qpel4_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[16];
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(half, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, full_mid+4, half, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, full_mid+4, half, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -1719,7 +2569,7 @@ void ff_put_h264_qpel4_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -1732,7 +2582,7 @@ void ff_put_h264_qpel4_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -1745,7 +2595,7 @@ void ff_put_h264_qpel4_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -1758,7 +2608,7 @@ void ff_put_h264_qpel4_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    put_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc22_mmi(uint8_t *dst, const uint8_t *src,
@@ -1774,7 +2624,7 @@ void ff_put_h264_qpel4_mc21_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t halfHV[16];
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    put_pixels4_l2_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc23_mmi(uint8_t *dst, const uint8_t *src,
@@ -1784,7 +2634,7 @@ void ff_put_h264_qpel4_mc23_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t halfHV[16];
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    put_pixels4_l2_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc12_mmi(uint8_t *dst, const uint8_t *src,
@@ -1797,7 +2647,7 @@ void ff_put_h264_qpel4_mc12_mmi(uint8_t *dst, const uint8_t *src,
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    put_pixels4_l2_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
 }
 
 void ff_put_h264_qpel4_mc32_mmi(uint8_t *dst, const uint8_t *src,
@@ -1810,14 +2660,14 @@ void ff_put_h264_qpel4_mc32_mmi(uint8_t *dst, const uint8_t *src,
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    put_pixels4_l2_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
+    ff_put_pixels4_l2_8_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
 }
 
 //DEF_H264_MC_MMI(avg_, 4)
 void ff_avg_h264_qpel4_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    avg_pixels4_mmi(dst, src, stride, 4);
+    ff_avg_pixels4_8_mmi(dst, src, stride, 4);
 }
 
 void ff_avg_h264_qpel4_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -1825,7 +2675,7 @@ void ff_avg_h264_qpel4_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[16];
     put_h264_qpel4_h_lowpass_mmi(half, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, src, half, stride, stride, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, src, half, stride, stride, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -1839,7 +2689,7 @@ void ff_avg_h264_qpel4_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[16];
     put_h264_qpel4_h_lowpass_mmi(half, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, src+1, half, stride, stride, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, src+1, half, stride, stride, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -1850,7 +2700,7 @@ void ff_avg_h264_qpel4_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[16];
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(half, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, full_mid, half, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, full_mid, half, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -1870,7 +2720,7 @@ void ff_avg_h264_qpel4_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[16];
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(half, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, full_mid+4, half, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, full_mid+4, half, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -1883,7 +2733,7 @@ void ff_avg_h264_qpel4_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -1896,7 +2746,7 @@ void ff_avg_h264_qpel4_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -1909,7 +2759,7 @@ void ff_avg_h264_qpel4_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -1922,7 +2772,7 @@ void ff_avg_h264_qpel4_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
-    avg_pixels4_l2_mmi(dst, halfH, halfV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc22_mmi(uint8_t *dst, const uint8_t *src,
@@ -1938,7 +2788,7 @@ void ff_avg_h264_qpel4_mc21_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t halfHV[16];
     put_h264_qpel4_h_lowpass_mmi(halfH, src, 4, stride);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc23_mmi(uint8_t *dst, const uint8_t *src,
@@ -1948,7 +2798,7 @@ void ff_avg_h264_qpel4_mc23_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t halfHV[16];
     put_h264_qpel4_h_lowpass_mmi(halfH, src + stride, 4, stride);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfH, halfHV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc12_mmi(uint8_t *dst, const uint8_t *src,
@@ -1961,7 +2811,7 @@ void ff_avg_h264_qpel4_mc12_mmi(uint8_t *dst, const uint8_t *src,
     copy_block4_mmi(full, src - stride*2, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
 }
 
 void ff_avg_h264_qpel4_mc32_mmi(uint8_t *dst, const uint8_t *src,
@@ -1974,14 +2824,14 @@ void ff_avg_h264_qpel4_mc32_mmi(uint8_t *dst, const uint8_t *src,
     copy_block4_mmi(full, src - stride*2 + 1, 4,  stride, 9);
     put_h264_qpel4_v_lowpass_mmi(halfV, full_mid, 4, 4);
     put_h264_qpel4_hv_lowpass_mmi(halfHV, src, 4, stride);
-    avg_pixels4_l2_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
+    ff_avg_pixels4_l2_8_mmi(dst, halfV, halfHV, stride, 4, 4, 4);
 }
 
 //DEF_H264_MC_MMI(put_, 8)
 void ff_put_h264_qpel8_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    put_pixels8_mmi(dst, src, stride, 8);
+    ff_put_pixels8_8_mmi(dst, src, stride, 8);
 }
 
 void ff_put_h264_qpel8_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -1989,7 +2839,7 @@ void ff_put_h264_qpel8_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
     put_h264_qpel8_h_lowpass_mmi(half, src, 8, stride);
-    put_pixels8_l2_mmi(dst, src, half, stride, stride, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, src, half, stride, stride, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -2003,7 +2853,7 @@ void ff_put_h264_qpel8_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
     put_h264_qpel8_h_lowpass_mmi(half, src, 8, stride);
-    put_pixels8_l2_mmi(dst, src+1, half, stride, stride, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, src+1, half, stride, stride, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -2014,7 +2864,7 @@ void ff_put_h264_qpel8_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[64];
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(half, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, full_mid, half, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, full_mid, half, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -2034,7 +2884,7 @@ void ff_put_h264_qpel8_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[64];
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(half, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, full_mid+8, half, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, full_mid+8, half, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -2047,7 +2897,7 @@ void ff_put_h264_qpel8_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -2060,7 +2910,7 @@ void ff_put_h264_qpel8_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
     copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -2073,7 +2923,7 @@ void ff_put_h264_qpel8_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -2086,66 +2936,66 @@ void ff_put_h264_qpel8_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
     copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_put_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc22_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    put_h264_qpel8_hv_lowpass_mmi(dst, src, stride, stride);
+    uint16_t __attribute__ ((aligned(8))) temp[192];
+
+    put_h264_qpel8_hv_lowpass_mmi(dst, temp, src, stride, 8, stride);
 }
 
 void ff_put_h264_qpel8_mc21_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[64];
-    uint8_t halfHV[64];
-    put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    put_pixels8_l2_mmi(dst, halfH, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    put_h264_qpel8_h_lowpass_l2_mmi(dst, src, halfHV, stride, 8);
 }
 
 void ff_put_h264_qpel8_mc23_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[64];
-    uint8_t halfHV[64];
-    put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    put_pixels8_l2_mmi(dst, halfH, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    put_h264_qpel8_h_lowpass_l2_mmi(dst, src + stride, halfHV, stride, 8);
 }
 
 void ff_put_h264_qpel8_mc12_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[104];
-    uint8_t * const full_mid= full + 16;
-    uint8_t halfV[64];
-    uint8_t halfHV[64];
-    copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
-    put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    put_pixels8_l2_mmi(dst, halfV, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    put_pixels8_l2_shift5_mmi(dst, halfV + 2, halfHV, stride, 8, 8);
 }
 
 void ff_put_h264_qpel8_mc32_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[104];
-    uint8_t * const full_mid= full + 16;
-    uint8_t halfV[64];
-    uint8_t halfHV[64];
-    copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
-    put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    put_pixels8_l2_mmi(dst, halfV, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    put_pixels8_l2_shift5_mmi(dst, halfV + 3, halfHV, stride, 8, 8);
 }
 
 //DEF_H264_MC_MMI(avg_, 8)
 void ff_avg_h264_qpel8_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    avg_pixels8_mmi(dst, src, stride, 8);
+    ff_avg_pixels8_8_mmi(dst, src, stride, 8);
 }
 
 void ff_avg_h264_qpel8_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -2153,7 +3003,7 @@ void ff_avg_h264_qpel8_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
     put_h264_qpel8_h_lowpass_mmi(half, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, src, half, stride, stride, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, src, half, stride, stride, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -2167,7 +3017,7 @@ void ff_avg_h264_qpel8_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
     put_h264_qpel8_h_lowpass_mmi(half, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, src+1, half, stride, stride, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, src+1, half, stride, stride, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -2178,7 +3028,7 @@ void ff_avg_h264_qpel8_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[64];
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(half, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, full_mid, half, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, full_mid, half, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -2198,7 +3048,7 @@ void ff_avg_h264_qpel8_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[64];
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(half, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, full_mid+8, half, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, full_mid+8, half, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -2211,7 +3061,7 @@ void ff_avg_h264_qpel8_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -2224,7 +3074,7 @@ void ff_avg_h264_qpel8_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
     copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -2237,7 +3087,7 @@ void ff_avg_h264_qpel8_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
     copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -2250,66 +3100,66 @@ void ff_avg_h264_qpel8_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
     copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
     put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    avg_pixels8_l2_mmi(dst, halfH, halfV, stride, 8, 8, 8);
+    ff_avg_pixels8_l2_8_mmi(dst, halfH, halfV, stride, 8, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc22_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    avg_h264_qpel8_hv_lowpass_mmi(dst, src, stride, stride);
+    uint16_t __attribute__ ((aligned(8))) temp[192];
+
+    avg_h264_qpel8_hv_lowpass_mmi(dst, temp, src, stride, 8, stride);
 }
 
 void ff_avg_h264_qpel8_mc21_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[64];
-    uint8_t halfHV[64];
-    put_h264_qpel8_h_lowpass_mmi(halfH, src, 8, stride);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, halfH, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst, src, halfHV, stride, 8);
 }
 
 void ff_avg_h264_qpel8_mc23_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[64];
-    uint8_t halfHV[64];
-    put_h264_qpel8_h_lowpass_mmi(halfH, src + stride, 8, stride);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, halfH, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    avg_h264_qpel8_h_lowpass_l2_mmi(dst, src + stride, halfHV, stride, 8);
 }
 
 void ff_avg_h264_qpel8_mc12_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[104];
-    uint8_t * const full_mid= full + 16;
-    uint8_t halfV[64];
-    uint8_t halfHV[64];
-    copy_block8_mmi(full, src - stride*2, 8,  stride, 13);
-    put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, halfV, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    avg_pixels8_l2_shift5_mmi(dst, halfV + 2, halfHV, stride, 8, 8);
 }
 
 void ff_avg_h264_qpel8_mc32_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[104];
-    uint8_t * const full_mid= full + 16;
-    uint8_t halfV[64];
-    uint8_t halfHV[64];
-    copy_block8_mmi(full, src - stride*2 + 1, 8,  stride, 13);
-    put_h264_qpel8_v_lowpass_mmi(halfV, full_mid, 8, 8);
-    put_h264_qpel8_hv_lowpass_mmi(halfHV, src, 8, stride);
-    avg_pixels8_l2_mmi(dst, halfV, halfHV, stride, 8, 8, 8);
+    uint8_t __attribute__ ((aligned(8))) temp[448];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 64);
+
+    put_h264_qpel8_hv_lowpass_mmi(halfHV, halfV, src, 8, 8, stride);
+    avg_pixels8_l2_shift5_mmi(dst, halfV + 3, halfHV, stride, 8, 8);
 }
 
 //DEF_H264_MC_MMI(put_, 16)
 void ff_put_h264_qpel16_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    put_pixels16_mmi(dst, src, stride, 16);
+    ff_put_pixels16_8_mmi(dst, src, stride, 16);
 }
 
 void ff_put_h264_qpel16_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -2317,7 +3167,7 @@ void ff_put_h264_qpel16_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[256];
     put_h264_qpel16_h_lowpass_mmi(half, src, 16, stride);
-    put_pixels16_l2_mmi(dst, src, half, stride, stride, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, src, half, stride, stride, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -2331,7 +3181,7 @@ void ff_put_h264_qpel16_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[256];
     put_h264_qpel16_h_lowpass_mmi(half, src, 16, stride);
-    put_pixels16_l2_mmi(dst, src+1, half, stride, stride, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, src+1, half, stride, stride, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -2342,7 +3192,7 @@ void ff_put_h264_qpel16_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[256];
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(half, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, full_mid, half, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, full_mid, half, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -2362,7 +3212,7 @@ void ff_put_h264_qpel16_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[256];
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(half, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, full_mid+16, half, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, full_mid+16, half, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -2375,7 +3225,7 @@ void ff_put_h264_qpel16_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -2388,7 +3238,7 @@ void ff_put_h264_qpel16_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
     copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -2401,7 +3251,7 @@ void ff_put_h264_qpel16_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -2414,66 +3264,66 @@ void ff_put_h264_qpel16_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
     copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_put_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc22_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    put_h264_qpel16_hv_lowpass_mmi(dst, src, stride, stride);
+    uint16_t __attribute__ ((aligned(8))) temp[384];
+
+    put_h264_qpel16_hv_lowpass_mmi(dst, temp, src, stride, 16, stride);
 }
 
 void ff_put_h264_qpel16_mc21_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[256];
-    uint8_t halfHV[256];
-    put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    put_pixels16_l2_mmi(dst, halfH, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    put_h264_qpel16_h_lowpass_l2_mmi(dst, src, halfHV, stride, 16);
 }
 
 void ff_put_h264_qpel16_mc23_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[256];
-    uint8_t halfHV[256];
-    put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    put_pixels16_l2_mmi(dst, halfH, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    put_h264_qpel16_h_lowpass_l2_mmi(dst, src + stride, halfHV, stride, 16);
 }
 
 void ff_put_h264_qpel16_mc12_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[336];
-    uint8_t * const full_mid= full + 32;
-    uint8_t halfV[256];
-    uint8_t halfHV[256];
-    copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
-    put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    put_pixels16_l2_mmi(dst, halfV, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    put_pixels16_l2_shift5_mmi(dst, halfV + 2, halfHV, stride, 16, 16);
 }
 
 void ff_put_h264_qpel16_mc32_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[336];
-    uint8_t * const full_mid= full + 32;
-    uint8_t halfV[256];
-    uint8_t halfHV[256];
-    copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
-    put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    put_pixels16_l2_mmi(dst, halfV, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    put_pixels16_l2_shift5_mmi(dst, halfV + 3, halfHV, stride, 16, 16);
 }
 
 //DEF_H264_MC_MMI(avg_, 16)
 void ff_avg_h264_qpel16_mc00_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    avg_pixels16_mmi(dst, src, stride, 16);
+    ff_avg_pixels16_8_mmi(dst, src, stride, 16);
 }
 
 void ff_avg_h264_qpel16_mc10_mmi(uint8_t *dst, const uint8_t *src,
@@ -2481,7 +3331,7 @@ void ff_avg_h264_qpel16_mc10_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[256];
     put_h264_qpel16_h_lowpass_mmi(half, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, src, half, stride, stride, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, src, half, stride, stride, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc20_mmi(uint8_t *dst, const uint8_t *src,
@@ -2495,7 +3345,7 @@ void ff_avg_h264_qpel16_mc30_mmi(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[256];
     put_h264_qpel16_h_lowpass_mmi(half, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, src+1, half, stride, stride, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, src+1, half, stride, stride, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc01_mmi(uint8_t *dst, const uint8_t *src,
@@ -2506,7 +3356,7 @@ void ff_avg_h264_qpel16_mc01_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[256];
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(half, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, full_mid, half, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, full_mid, half, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc02_mmi(uint8_t *dst, const uint8_t *src,
@@ -2526,7 +3376,7 @@ void ff_avg_h264_qpel16_mc03_mmi(uint8_t *dst, const uint8_t *src,
     uint8_t half[256];
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(half, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, full_mid+16, half, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, full_mid+16, half, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc11_mmi(uint8_t *dst, const uint8_t *src,
@@ -2539,7 +3389,7 @@ void ff_avg_h264_qpel16_mc11_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc31_mmi(uint8_t *dst, const uint8_t *src,
@@ -2552,7 +3402,7 @@ void ff_avg_h264_qpel16_mc31_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
     copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc13_mmi(uint8_t *dst, const uint8_t *src,
@@ -2565,7 +3415,7 @@ void ff_avg_h264_qpel16_mc13_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
     copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc33_mmi(uint8_t *dst, const uint8_t *src,
@@ -2578,59 +3428,59 @@ void ff_avg_h264_qpel16_mc33_mmi(uint8_t *dst, const uint8_t *src,
     put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
     copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
     put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    avg_pixels16_l2_mmi(dst, halfH, halfV, stride, 16, 16, 16);
+    ff_avg_pixels16_l2_8_mmi(dst, halfH, halfV, stride, 16, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc22_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    avg_h264_qpel16_hv_lowpass_mmi(dst, src, stride, stride);
+    uint16_t __attribute__ ((aligned(8))) temp[384];
+
+    avg_h264_qpel16_hv_lowpass_mmi(dst, temp, src, stride, 16, stride);
 }
 
 void ff_avg_h264_qpel16_mc21_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[256];
-    uint8_t halfHV[256];
-    put_h264_qpel16_h_lowpass_mmi(halfH, src, 16, stride);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, halfH, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    avg_h264_qpel16_h_lowpass_l2_mmi(dst, src, halfHV, stride, 16);
 }
 
 void ff_avg_h264_qpel16_mc23_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t halfH[256];
-    uint8_t halfHV[256];
-    put_h264_qpel16_h_lowpass_mmi(halfH, src + stride, 16, stride);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, halfH, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    avg_h264_qpel16_h_lowpass_l2_mmi(dst, src + stride, halfHV, stride, 16);
 }
 
 void ff_avg_h264_qpel16_mc12_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[336];
-    uint8_t * const full_mid= full + 32;
-    uint8_t halfV[256];
-    uint8_t halfHV[256];
-    copy_block16_mmi(full, src - stride*2, 16,  stride, 21);
-    put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, halfV, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    avg_pixels16_l2_shift5_mmi(dst, halfV + 2, halfHV, stride, 16, 16);
 }
 
 void ff_avg_h264_qpel16_mc32_mmi(uint8_t *dst, const uint8_t *src,
         ptrdiff_t stride)
 {
-    uint8_t full[336];
-    uint8_t * const full_mid= full + 32;
-    uint8_t halfV[256];
-    uint8_t halfHV[256];
-    copy_block16_mmi(full, src - stride*2 + 1, 16,  stride, 21);
-    put_h264_qpel16_v_lowpass_mmi(halfV, full_mid, 16, 16);
-    put_h264_qpel16_hv_lowpass_mmi(halfHV, src, 16, stride);
-    avg_pixels16_l2_mmi(dst, halfV, halfHV, stride, 16, 16, 16);
+    uint8_t __attribute__ ((aligned(8))) temp[1024];
+    uint8_t *const halfHV = temp;
+    int16_t *const halfV = (int16_t *) (temp + 256);
+
+    put_h264_qpel16_hv_lowpass_mmi(halfHV, halfV, src, 16, 16, stride);
+    avg_pixels16_l2_shift5_mmi(dst, halfV + 3, halfHV, stride, 16, 16);
 }
 
 #undef op2_avg
diff --git a/libavcodec/mips/hpeldsp_init_mips.c b/libavcodec/mips/hpeldsp_init_mips.c
index 82f2310..363a045 100644
--- a/libavcodec/mips/hpeldsp_init_mips.c
+++ b/libavcodec/mips/hpeldsp_init_mips.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2015 Parag Salasakar (Parag.Salasakar@imgtec.com)
+ * Copyright (c) 2016 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -65,9 +66,57 @@ static void ff_hpeldsp_init_msa(HpelDSPContext *c, int flags)
 }
 #endif  // #if HAVE_MSA
 
+#if HAVE_MMI
+static void ff_hpeldsp_init_mmi(HpelDSPContext *c, int flags)
+{
+    c->put_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+    c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_mmi;
+    c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_mmi;
+    c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_mmi;
+
+    c->put_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+    c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_mmi;
+    c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_mmi;
+    c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_mmi;
+
+    c->put_pixels_tab[2][0] = ff_put_pixels4_8_mmi;
+    c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_mmi;
+    c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_mmi;
+    c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_mmi;
+
+    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+    c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_mmi;
+    c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_mmi;
+    c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_mmi;
+
+    c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+    c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_mmi;
+    c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_mmi;
+    c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_mmi;
+
+    c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_mmi;
+    c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_mmi;
+    c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_mmi;
+    c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_mmi;
+
+    c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_mmi;
+    c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_mmi;
+    c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_mmi;
+    c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_mmi;
+
+    c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_mmi;
+    c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_mmi;
+    c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_mmi;
+    c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_mmi;
+}
+#endif  // #if HAVE_MMI
+
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags)
 {
 #if HAVE_MSA
     ff_hpeldsp_init_msa(c, flags);
 #endif  // #if HAVE_MSA
+#if HAVE_MMI
+    ff_hpeldsp_init_mmi(c, flags);
+#endif  // #if HAVE_MMI
 }
diff --git a/libavcodec/mips/hpeldsp_mips.h b/libavcodec/mips/hpeldsp_mips.h
index f4ab53e..f527c1d 100644
--- a/libavcodec/mips/hpeldsp_mips.h
+++ b/libavcodec/mips/hpeldsp_mips.h
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2015 Parag Salasakar (Parag.Salasakar@imgtec.com)
+ * Copyright (c) 2016 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -84,4 +85,90 @@ void ff_avg_pixels4_y2_msa(uint8_t *block, const uint8_t *pixels,
 void ff_avg_pixels4_xy2_msa(uint8_t *block, const uint8_t *pixels,
                             ptrdiff_t line_size, int32_t h);
 
+void ff_put_pixels16_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_put_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_put_pixels4_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_avg_pixels16_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_avg_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_avg_pixels4_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_put_no_rnd_pixels16_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+void ff_put_no_rnd_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h);
+
+void ff_put_pixels16_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels8_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels4_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels4_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels4_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_pixels4_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels16_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels8_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels4_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels4_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels4_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+void ff_avg_pixels4_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int32_t h);
+
 #endif  // #ifndef AVCODEC_MIPS_HPELDSP_MIPS_H
diff --git a/libavcodec/mips/hpeldsp_mmi.c b/libavcodec/mips/hpeldsp_mmi.c
new file mode 100644
index 0000000..6e0f9a5
--- /dev/null
+++ b/libavcodec/mips/hpeldsp_mmi.c
@@ -0,0 +1,1721 @@
+/*
+ * Loongson SIMD optimized qpeldsp
+ *
+ * Copyright (c) 2016 Loongson Technology Corporation Limited
+ * Copyright (c) 2016 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "hpeldsp_mips.h"
+#include "libavcodec/bit_depth_template.c"
+#include "libavutil/mips/asmdefs.h"
+#include "constants.h"
+
+void ff_put_pixels4_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[2];
+    mips_reg addr[3];
+    int low32;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr1],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[pixels])                         \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "swc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "usw        %[low32],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[pixels])                         \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "swc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "usw        %[low32],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),
+          [low32]"=&r"(low32),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+void ff_put_pixels8_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[2];
+    mips_reg addr[3];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr1],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),
+          [all64]"=&r"(all64),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+void ff_put_pixels16_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[4];
+    mips_reg addr[3];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr1],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[pixels])                         \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x08(%[addr0])                          \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+        "sdc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        "gssdxc1    %[ftmp3],   0x08(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x08(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+        "sdc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x08(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[pixels])                         \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x08(%[addr0])                          \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+        "sdc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        "gssdxc1    %[ftmp3],   0x08(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x08(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+        "sdc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        "usd        %[all64],   0x08(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr1]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr1]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),
+          [all64]"=&r"(all64),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+void ff_avg_pixels4_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[4];
+    mips_reg addr[4];
+    int low32;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[pixels])                         \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[block])                          \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[pixels])                         \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "ulw        %[low32],   0x00(%[block])                          \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "usw        %[low32],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [low32]"=&r"(low32),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+void ff_avg_pixels8_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[4];
+    mips_reg addr[4];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[block])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[block])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[block])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[block])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [all64]"=&r"(all64),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+void ff_avg_pixels16_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    double ftmp[8];
+    mips_reg addr[4];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[line_size],   %[line_size]            \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp4],   0x0f(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "gsldrc1    %[ftmp4],   0x08(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp5],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x08(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[block])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[block])                          \n\t"
+        "gsldlc1    %[ftmp6],   0x0f(%[block])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x08(%[block])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp7],   0x0f(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp7],   0x08(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x08(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x08(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+        "sdc1       %[ftmp4],   0x08(%[block])                          \n\t"
+        "gssdxc1    %[ftmp5],   0x08(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[block])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x08(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp4],   0x0f(%[pixels])                         \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "gsldrc1    %[ftmp4],   0x08(%[pixels])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp5],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x08(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[block])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[block])                          \n\t"
+        "gsldlc1    %[ftmp6],   0x0f(%[block])                          \n\t"
+        "gsldrc1    %[ftmp6],   0x08(%[block])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp7],   0x0f(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp7],   0x08(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x08(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[block],       %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x08(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[block])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[block],  %[line_size])           \n\t"
+        "sdc1       %[ftmp4],   0x08(%[block])                          \n\t"
+        "gssdxc1    %[ftmp5],   0x08(%[block],  %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr3],   %[block],       %[line_size]            \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr3])                          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[block])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x08(%[addr3])                          \n\t"
+#endif
+        PTR_ADDU   "%[pixels],  %[pixels],      %[addr2]                \n\t"
+        PTR_ADDU   "%[block],   %[block],       %[addr2]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [all64]"=&r"(all64),
+          [block]"+&r"(block),              [pixels]"+&r"(pixels),
+          [h]"+&r"(h)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+}
+
+inline void ff_put_pixels4_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[4];
+    mips_reg addr[6];
+    int low32;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "ulw        %[low32],   0x00(%[src1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   0x00(%[src2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "ulw        %[low32],   0x00(%[src1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   0x00(%[src2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "usw        %[low32],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [low32]"=&r"(low32),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+inline void ff_put_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[4];
+    mips_reg addr[6];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+inline void ff_put_pixels16_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[8];
+    mips_reg addr[6];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp4],   0x0f(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp4],   0x08(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp5],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x08(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp6],   0x0f(%[src2])                           \n\t"
+        "gsldrc1    %[ftmp6],   0x08(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp7],   0x0f(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp7],   0x08(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x08(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x08(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[dst])                            \n\t"
+        "gssdxc1    %[ftmp5],   0x08(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[dst])                            \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x08(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp4],   0x0f(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp4],   0x08(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp5],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x08(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp6],   0x0f(%[src2])                           \n\t"
+        "gsldrc1    %[ftmp6],   0x08(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp7],   0x0f(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp7],   0x08(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x08(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x08(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[dst])                            \n\t"
+        "gssdxc1    %[ftmp5],   0x08(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+        "sdc1       %[ftmp4],   0x08(%[dst])                            \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x08(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+inline void ff_avg_pixels4_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[6];
+    mips_reg addr[7];
+    int low32;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "ulw        %[low32],   0x00(%[src1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   0x00(%[src2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "ulw        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr5])                          \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr6],   %[dst],         %[dst_stride]           \n\t"
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr6])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "ulw        %[low32],   0x00(%[src1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp0]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr0])                          \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   0x00(%[src2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "ulw        %[low32],   0x00(%[addr1])                          \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "ulw        %[low32],   0x00(%[dst])                            \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "ulw        %[low32],   0x00(%[addr5])                          \n\t"
+        "mtc1       %[low32],   %[ftmp5]                                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "swc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gsswxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr6],   %[dst],         %[dst_stride]           \n\t"
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[addr6])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),
+          [low32]"=&r"(low32),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+inline void ff_avg_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[6];
+    mips_reg addr[7];
+    uint64_t all64;
+
+    __asm__ volatile (
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp4],   0x07(%[dst])                            \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[dst])                            \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[dst])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[dst],         %[dst_stride]           \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp4],   0x07(%[dst])                            \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[dst])                            \n\t"
+        "gsldlc1    %[ftmp5],   0x07(%[addr5])                          \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[addr5])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[dst])                            \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+        "uld        %[all64],   0x00(%[addr5])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+#endif
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr6],   %[dst],         %[dst_stride]           \n\t"
+        "usd        %[all64],   0x00(%[addr6])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [addr6]"=&r"(addr[6]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+inline void ff_avg_pixels16_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    ff_avg_pixels8_l2_8_mmi(dst, src1, src2, dst_stride, src_stride1,
+            src_stride2, h);
+    ff_avg_pixels8_l2_8_mmi(dst + 8, src1 + 8, src2 + 8, dst_stride,
+            src_stride1, src_stride2, h);
+}
+
+void ff_put_pixels4_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels4_l2_8_mmi(block, pixels, pixels + 1, line_size, line_size,
+            line_size, h);
+}
+
+void ff_put_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels8_l2_8_mmi(block, pixels, pixels + 1, line_size, line_size,
+            line_size, h);
+}
+
+void ff_put_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels16_l2_8_mmi(block, pixels, pixels + 1, line_size, line_size,
+            line_size, h);
+}
+
+void ff_avg_pixels4_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels4_l2_8_mmi(block, pixels, pixels + 1, line_size, line_size,
+            line_size, h);
+}
+
+void ff_avg_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels8_l2_8_mmi(block, pixels, pixels + 1, line_size, line_size,
+            line_size, h);
+}
+
+void ff_avg_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels8_x2_8_mmi(block, pixels, line_size, h);
+    ff_avg_pixels8_x2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
+
+inline void ff_put_no_rnd_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
+    const uint8_t *src2, int dst_stride, int src_stride1, int src_stride2,
+    int h)
+{
+    double ftmp[5];
+    mips_reg addr[6];
+    uint64_t all64;
+
+    __asm__ volatile (
+        "pcmpeqb    %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        PTR_ADDU   "%[addr2],   %[src_stride1], %[src_stride1]          \n\t"
+        PTR_ADDU   "%[addr3],   %[src_stride2], %[src_stride2]          \n\t"
+        PTR_ADDU   "%[addr4],   %[dst_stride],  %[dst_stride]           \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+        "gsldlc1    %[ftmp3],   0x07(%[addr1])                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+        "gsldrc1    %[ftmp3],   0x00(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr0],   %[src1],        %[src_stride1]          \n\t"
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
+#endif
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[dst])                            \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp1],   0x00(%[dst],    %[dst_stride])          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        PTR_ADDU   "%[addr5],   %[dst],         %[dst_stride]           \n\t"
+        "usd        %[all64],   0x00(%[addr5])                          \n\t"
+#endif
+        PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
+        PTR_ADDU   "%[dst],     %[dst],         %[addr4]                \n\t"
+
+        PTR_ADDI   "%[h],       %[h],           -0x04                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),            [addr3]"=&r"(addr[3]),
+          [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src1]"+&r"(src1),
+          [src2]"+&r"(src2),                [h]"+&r"(h)
+        : [dst_stride]"r"((mips_reg)dst_stride),
+          [src_stride1]"r"((mips_reg)src_stride1),
+          [src_stride2]"r"((mips_reg)src_stride2)
+        : "memory"
+    );
+}
+
+void ff_put_no_rnd_pixels8_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_no_rnd_pixels8_l2_8_mmi(block, pixels, pixels + 1, line_size,
+            line_size, line_size, h);
+}
+
+void ff_put_no_rnd_pixels16_x2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_no_rnd_pixels8_x2_8_mmi(block, pixels, line_size, h);
+    ff_put_no_rnd_pixels8_x2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
+
+void ff_put_pixels4_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels4_l2_8_mmi(block, pixels, pixels + line_size, line_size,
+            line_size, line_size, h);
+}
+
+void ff_put_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels8_l2_8_mmi(block, pixels, pixels + line_size, line_size,
+            line_size, line_size, h);
+}
+
+void ff_put_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels16_l2_8_mmi(block, pixels, pixels + line_size, line_size,
+            line_size, line_size, h);
+}
+
+void ff_avg_pixels4_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels4_l2_8_mmi(block, pixels, pixels + line_size, line_size,
+            line_size, line_size, h);
+}
+
+void ff_avg_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels8_l2_8_mmi(block, pixels, pixels + line_size, line_size,
+            line_size, line_size, h);
+}
+
+void ff_avg_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels8_y2_8_mmi(block, pixels, line_size, h);
+    ff_avg_pixels8_y2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
+
+void ff_put_no_rnd_pixels8_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_no_rnd_pixels8_l2_8_mmi(block, pixels, pixels + line_size,
+            line_size, line_size, line_size, h);
+}
+
+void ff_put_no_rnd_pixels16_y2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_no_rnd_pixels8_y2_8_mmi(block, pixels, line_size, h);
+    ff_put_no_rnd_pixels8_y2_8_mmi(block + 8 , pixels + 8, line_size, h);
+}
+
+void ff_put_pixels4_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    /* FIXME HIGH BIT DEPTH */
+    int i;
+    const uint32_t a = AV_RN32(pixels);
+    const uint32_t b = AV_RN32(pixels + 1);
+    uint32_t l0 = (a & 0x03030303UL) +
+                  (b & 0x03030303UL) +
+                       0x02020202UL;
+    uint32_t h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                  ((b & 0xFCFCFCFCUL) >> 2);
+    uint32_t l1, h1;
+
+    pixels += line_size;
+    for (i = 0; i < h; i += 2) {
+        uint32_t a = AV_RN32(pixels);
+        uint32_t b = AV_RN32(pixels + 1);
+        l1 = (a & 0x03030303UL) +
+             (b & 0x03030303UL);
+        h1 = ((a & 0xFCFCFCFCUL) >> 2) +
+             ((b & 0xFCFCFCFCUL) >> 2);
+        *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+        pixels += line_size;
+        block  += line_size;
+        a  = AV_RN32(pixels);
+        b  = AV_RN32(pixels + 1);
+        l0 = (a & 0x03030303UL) +
+             (b & 0x03030303UL) +
+                  0x02020202UL;
+        h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+             ((b & 0xFCFCFCFCUL) >> 2);
+        *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+        pixels += line_size;
+        block  += line_size;
+    }
+}
+
+void ff_put_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+#if 1
+    double ftmp[10];
+    mips_reg addr[3];
+    uint64_t all64;
+
+    __asm__ volatile (
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "dli        %[addr0],   0x0f                                    \n\t"
+        "pcmpeqw    %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "dmtc1      %[addr0],   %[ftmp8]                                \n\t"
+        "dli        %[addr0],   0x01                                    \n\t"
+        "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+        "dmtc1      %[addr0],   %[ftmp8]                                \n\t"
+        "psllh      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
+
+        "dli        %[addr0],   0x02                                    \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[pixels])                         \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[addr0],   %[ftmp9]                                \n\t"
+        "gsldlc1    %[ftmp4],   0x08(%[pixels])                         \n\t"
+        "gsldrc1    %[ftmp4],   0x01(%[pixels])                         \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        "dmtc1      %[addr0],   %[ftmp9]                                \n\t"
+        "uld        %[all64],   0x01(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        "mov.d      %[ftmp1],   %[ftmp0]                                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp4]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "paddush    %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddush    %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "xor        %[addr0],   %[addr0],       %[addr0]                \n\t"
+        PTR_ADDU   "%[pixels],  %[pixels],      %[line_size]            \n\t"
+        ".p2align   3                                                   \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr1],   %[pixels],      %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x08(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x01(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        "uld        %[all64],   0x01(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "mov.d      %[ftmp1],   %[ftmp0]                                \n\t"
+        "mov.d      %[ftmp3],   %[ftmp2]                                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "paddush    %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "paddush    %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "paddush    %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "paddush    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
+        "paddush    %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddush    %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp4],   0x00(%[block],  %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[addr0]                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr0],   %[addr0],       %[line_size]            \n\t"
+        PTR_ADDU   "%[addr1],   %[pixels],      %[addr0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp2],   0x07(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[addr1])                          \n\t"
+        "gsldlc1    %[ftmp4],   0x08(%[addr1])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x01(%[addr1])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x01(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+        "mov.d      %[ftmp3],   %[ftmp2]                                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp4]                                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "paddush    %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
+        "paddush    %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
+        "paddush    %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "paddush    %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "paddush    %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "psrlh      %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]                \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp0],   0x00(%[block],  %[addr0])               \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        PTR_ADDU   "%[addr2],   %[block],       %[addr0]                \n\t"
+        "usd        %[all64],   0x00(%[addr2])                          \n\t"
+#endif
+        PTR_ADDU   "%[addr0],   %[addr0],       %[line_size]            \n\t"
+        PTR_ADDU   "%[h],       %[h],           -0x02                   \n\t"
+        "bnez       %[h],       1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [addr2]"=&r"(addr[2]),
+          [all64]"=&r"(all64),
+          [h]"+&r"(h),                      [pixels]"+&r"(pixels)
+        : [block]"r"(block),                [line_size]"r"((mips_reg)line_size)
+        : "memory"
+    );
+#else
+    /* FIXME HIGH BIT DEPTH */
+    int j;
+
+    for (j = 0; j < 2; j++) {
+        int i;
+        const uint32_t a = AV_RN32(pixels);
+        const uint32_t b = AV_RN32(pixels + 1);
+        uint32_t l0 = (a & 0x03030303UL) +
+                      (b & 0x03030303UL) +
+                           0x02020202UL;
+        uint32_t h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                      ((b & 0xFCFCFCFCUL) >> 2);
+        uint32_t l1, h1;
+
+        pixels += line_size;
+        for (i = 0; i < h; i += 2) {
+            uint32_t a = AV_RN32(pixels);
+            uint32_t b = AV_RN32(pixels + 1);
+            l1 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL);
+            h1 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+            pixels += line_size;
+            block  += line_size;
+            a  = AV_RN32(pixels);
+            b  = AV_RN32(pixels + 1);
+            l0 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL) +
+                      0x02020202UL;
+            h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+            pixels += line_size;
+            block  += line_size;
+        }
+        pixels += 4 - line_size * (h + 1);
+        block  += 4 - line_size * h;
+    }
+#endif
+}
+
+void ff_put_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_pixels8_xy2_8_mmi(block, pixels, line_size, h);
+    ff_put_pixels8_xy2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
+
+void ff_avg_pixels4_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    /* FIXME HIGH BIT DEPTH */
+    int i;
+    const uint32_t a = AV_RN32(pixels);
+    const uint32_t b = AV_RN32(pixels + 1);
+    uint32_t l0 = (a & 0x03030303UL) +
+                  (b & 0x03030303UL) +
+                       0x02020202UL;
+    uint32_t h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                  ((b & 0xFCFCFCFCUL) >> 2);
+    uint32_t l1, h1;
+
+    pixels += line_size;
+    for (i = 0; i < h; i += 2) {
+        uint32_t a = AV_RN32(pixels);
+        uint32_t b = AV_RN32(pixels + 1);
+        l1 = (a & 0x03030303UL) +
+             (b & 0x03030303UL);
+        h1 = ((a & 0xFCFCFCFCUL) >> 2) +
+             ((b & 0xFCFCFCFCUL) >> 2);
+        *((uint32_t *) block) = rnd_avg32(*((uint32_t *) block), h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL));
+        pixels += line_size;
+        block  += line_size;
+        a  = AV_RN32(pixels);
+        b  = AV_RN32(pixels + 1);
+        l0 = (a & 0x03030303UL) +
+             (b & 0x03030303UL) +
+                  0x02020202UL;
+        h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+             ((b & 0xFCFCFCFCUL) >> 2);
+        *((uint32_t *) block) = rnd_avg32(*((uint32_t *) block), h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL));
+        pixels += line_size;
+        block  += line_size;
+    }
+}
+
+void ff_avg_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    /* FIXME HIGH BIT DEPTH */
+    int j;
+
+    for (j = 0; j < 2; j++) {
+        int i;
+        const uint32_t a = AV_RN32(pixels);
+        const uint32_t b = AV_RN32(pixels + 1);
+        uint32_t l0 = (a & 0x03030303UL) +
+                      (b & 0x03030303UL) +
+                           0x02020202UL;
+        uint32_t h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                      ((b & 0xFCFCFCFCUL) >> 2);
+        uint32_t l1, h1;
+
+        pixels += line_size;
+        for (i = 0; i < h; i += 2) {
+            uint32_t a = AV_RN32(pixels);
+            uint32_t b = AV_RN32(pixels + 1);
+            l1 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL);
+            h1 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = rnd_avg32(*((uint32_t *) block), h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL));
+            pixels += line_size;
+            block  += line_size;
+            a  = AV_RN32(pixels);
+            b  = AV_RN32(pixels + 1);
+            l0 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL) +
+                      0x02020202UL;
+            h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = rnd_avg32(*((uint32_t *) block), h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL));
+            pixels += line_size;
+            block  += line_size;
+        }
+        pixels += 4 - line_size * (h + 1);
+        block  += 4 - line_size * h;
+    }
+}
+
+void ff_avg_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_avg_pixels8_xy2_8_mmi(block, pixels, line_size, h);
+    ff_avg_pixels8_xy2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
+
+void ff_put_no_rnd_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    /* FIXME HIGH BIT DEPTH */
+    int j;
+
+    for (j = 0; j < 2; j++) {
+        int i;
+        const uint32_t a = AV_RN32(pixels);
+        const uint32_t b = AV_RN32(pixels + 1);
+        uint32_t l0 = (a & 0x03030303UL) +
+                      (b & 0x03030303UL) +
+                           0x01010101UL;
+        uint32_t h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                      ((b & 0xFCFCFCFCUL) >> 2);
+        uint32_t l1, h1;
+
+        pixels += line_size;
+        for (i = 0; i < h; i += 2) {
+            uint32_t a = AV_RN32(pixels);
+            uint32_t b = AV_RN32(pixels + 1);
+            l1 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL);
+            h1 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+            pixels += line_size;
+            block  += line_size;
+            a  = AV_RN32(pixels);
+            b  = AV_RN32(pixels + 1);
+            l0 = (a & 0x03030303UL) +
+                 (b & 0x03030303UL) +
+                      0x01010101UL;
+            h0 = ((a & 0xFCFCFCFCUL) >> 2) +
+                 ((b & 0xFCFCFCFCUL) >> 2);
+            *((uint32_t *) block) = h0 + h1 + (((l0 + l1) >> 2) & 0x0F0F0F0FUL);
+            pixels += line_size;
+            block  += line_size;
+        }
+        pixels += 4 - line_size * (h + 1);
+        block  += 4 - line_size * h;
+    }
+}
+
+void ff_put_no_rnd_pixels16_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
+    ptrdiff_t line_size, int h)
+{
+    ff_put_no_rnd_pixels8_xy2_8_mmi(block, pixels, line_size, h);
+    ff_put_no_rnd_pixels8_xy2_8_mmi(block + 8, pixels + 8, line_size, h);
+}
diff --git a/libavcodec/mips/idctdsp_mmi.c b/libavcodec/mips/idctdsp_mmi.c
index 25476f3..8e53fd4 100644
--- a/libavcodec/mips/idctdsp_mmi.c
+++ b/libavcodec/mips/idctdsp_mmi.c
@@ -23,63 +23,96 @@
 
 #include "idctdsp_mips.h"
 #include "constants.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_put_pixels_clamped_mmi(const int16_t *block,
         uint8_t *av_restrict pixels, ptrdiff_t line_size)
 {
-    const int16_t *p;
-    uint8_t *pix;
-
-    p = block;
-    pix = pixels;
+    double ftmp[8];
+    mips_reg addr[2];
+    uint64_t all64;
 
     __asm__ volatile (
-        "ldc1 $f0, 0+%3                 \r\n"
-        "ldc1 $f2, 8+%3                 \r\n"
-        "ldc1 $f4, 16+%3                \r\n"
-        "ldc1 $f6, 24+%3                \r\n"
-        "ldc1 $f8, 32+%3                \r\n"
-        "ldc1 $f10, 40+%3               \r\n"
-        "ldc1 $f12, 48+%3               \r\n"
-        "ldc1 $f14, 56+%3               \r\n"
-        "dadd $10, %0, %1               \r\n"
-        "packushb $f0, $f0, $f2         \r\n"
-        "packushb $f4, $f4, $f6         \r\n"
-        "packushb $f8, $f8, $f10        \r\n"
-        "packushb $f12, $f12, $f14      \r\n"
-        "sdc1 $f0, 0(%0)                \r\n"
-        "sdc1 $f4, 0($10)               \r\n"
-        "gssdxc1 $f8, 0($10, %1)        \r\n"
-        "gssdxc1 $f12, 0(%0, %2)        \r\n"
-        ::"r"(pix),"r"((int)line_size),
-          "r"((int)line_size*3),"m"(*p)
-        : "$10","memory"
+        "ldc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "ldc1       %[ftmp1],   0x08(%[block])                          \n\t"
+        "ldc1       %[ftmp2],   0x10(%[block])                          \n\t"
+        "ldc1       %[ftmp3],   0x18(%[block])                          \n\t"
+        "ldc1       %[ftmp4],   0x20(%[block])                          \n\t"
+        "ldc1       %[ftmp5],   0x28(%[block])                          \n\t"
+        "ldc1       %[ftmp6],   0x30(%[block])                          \n\t"
+        "ldc1       %[ftmp7],   0x38(%[block])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp4],   0x00(%[addr0],  %[line_size])           \n\t"
+        "gssdxc1    %[ftmp6],   0x00(%[pixels], %[line_sizex3])         \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[line_size]            \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[pixels],      %[line_sizex3]          \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [pixels]"+&r"(pixels)
+        : [line_size]"r"((mips_reg)line_size),
+          [line_sizex3]"r"((mips_reg)(line_size*3)),
+          [block]"r"(block)
+        : "memory"
     );
 
-    pix += line_size*4;
-    p += 32;
+    pixels += line_size*4;
+    block += 32;
 
     __asm__ volatile (
-        "ldc1 $f0, 0+%3                 \r\n"
-        "ldc1 $f2, 8+%3                 \r\n"
-        "ldc1 $f4, 16+%3                \r\n"
-        "ldc1 $f6, 24+%3                \r\n"
-        "ldc1 $f8, 32+%3                \r\n"
-        "ldc1 $f10, 40+%3               \r\n"
-        "ldc1 $f12, 48+%3               \r\n"
-        "ldc1 $f14, 56+%3               \r\n"
-        "dadd $10, %0, %1               \r\n"
-        "packushb $f0, $f0, $f2         \r\n"
-        "packushb $f4, $f4, $f6         \r\n"
-        "packushb $f8, $f8, $f10        \r\n"
-        "packushb $f12, $f12, $f14      \r\n"
-        "sdc1 $f0, 0(%0)                \r\n"
-        "sdc1 $f4, 0($10)               \r\n"
-        "gssdxc1 $f8, 0($10, %1)        \r\n"
-        "gssdxc1 $f12, 0(%0, %2)        \r\n"
-        ::"r"(pix),"r"((int)line_size),
-          "r"((int)line_size*3),"m"(*p)
-        : "$10","memory"
+        "ldc1       %[ftmp0],   0x00(%[block])                          \n\t"
+        "ldc1       %[ftmp1],   0x08(%[block])                          \n\t"
+        "ldc1       %[ftmp2],   0x10(%[block])                          \n\t"
+        "ldc1       %[ftmp3],   0x18(%[block])                          \n\t"
+        "ldc1       %[ftmp4],   0x20(%[block])                          \n\t"
+        "ldc1       %[ftmp5],   0x28(%[block])                          \n\t"
+        "ldc1       %[ftmp6],   0x30(%[block])                          \n\t"
+        "ldc1       %[ftmp7],   0x38(%[block])                          \n\t"
+        PTR_ADDU   "%[addr0],   %[pixels],      %[line_size]            \n\t"
+        "packushb   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "packushb   %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "sdc1       %[ftmp0],   0x00(%[pixels])                         \n\t"
+        "sdc1       %[ftmp2],   0x00(%[addr0])                          \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp4],   0x00(%[addr0],  %[line_size])           \n\t"
+        "gssdxc1    %[ftmp6],   0x00(%[pixels], %[line_sizex3])         \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp4]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[addr0],       %[line_size]            \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[pixels],      %[line_sizex3]          \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [pixels]"+&r"(pixels)
+        : [line_size]"r"((mips_reg)line_size),
+          [line_sizex3]"r"((mips_reg)(line_size*3)),
+          [block]"r"(block)
+        : "memory"
     );
 }
 
@@ -87,104 +120,154 @@ void ff_put_signed_pixels_clamped_mmi(const int16_t *block,
     uint8_t *av_restrict pixels, ptrdiff_t line_size)
 {
     int64_t line_skip = line_size;
-    int64_t line_skip3;
+    int64_t line_skip3 = 0;
+    double ftmp[5];
+    mips_reg addr[2];
+    uint64_t all64;
 
     __asm__ volatile (
-        "dmtc1 %4, $f0                  \n\t"
-        "daddu %1, %3, %3               \n\t"
-        "ldc1 $f2, 0(%2)                \n\t"
-        "ldc1 $f10, 8(%2)               \n\t"
-        "packsshb $f2, $f2, $f10        \n\t"
-        "ldc1 $f4, 16(%2)               \n\t"
-        "ldc1 $f10, 24(%2)              \n\t"
-        "packsshb $f4, $f4, $f10        \n\t"
-        "ldc1 $f6, 32(%2)               \n\t"
-        "ldc1 $f10, 40(%2)              \n\t"
-        "packsshb $f6, $f6, $f10        \n\t"
-        "ldc1 $f8, 48(%2)               \n\t"
-        "ldc1 $f10, 56(%2)              \n\t"
-        "packsshb $f8, $f8, $f10        \n\t"
-        "paddb $f2, $f2, $f0            \n\t"
-        "paddb $f4, $f4, $f0            \n\t"
-        "paddb $f6, $f6, $f0            \n\t"
-        "paddb $f8, $f8, $f0            \n\t"
-        "sdc1 $f2, 0(%0)                \n\t"
-        "gssdxc1 $f4, 0(%0, %3)         \n\t"
-        "gssdxc1 $f6, 0(%0, %1)         \n\t"
-        "daddu %1, %1, %3               \n\t"
-        "gssdxc1 $f8, 0(%0, %1)         \n\t"
-        "daddu $10, %1, %3              \n\t"
-        "daddu %0, %0, $10              \n\t"
-        "ldc1 $f2, 64(%2)               \n\t"
-        "ldc1 $f10, 8+64(%2)            \n\t"
-        "packsshb  $f2, $f2, $f10       \n\t"
-        "ldc1 $f4, 16+64(%2)            \n\t"
-        "ldc1 $f10, 24+64(%2)           \n\t"
-        "packsshb $f4, $f4, $f10        \n\t"
-        "ldc1 $f6, 32+64(%2)            \n\t"
-        "ldc1 $f10, 40+64(%2)           \n\t"
-        "packsshb $f6, $f6, $f10        \n\t"
-        "ldc1 $f8, 48+64(%2)            \n\t"
-        "ldc1 $f10, 56+64(%2)           \n\t"
-        "packsshb $f8, $f8, $f10        \n\t"
-        "paddb $f2, $f2, $f0            \n\t"
-        "paddb $f4, $f4, $f0            \n\t"
-        "paddb $f6, $f6, $f0            \n\t"
-        "paddb $f8, $f8, $f0            \n\t"
-        "sdc1 $f2, 0(%0)                \n\t"
-        "gssdxc1 $f4, 0(%0, %3)         \n\t"
-        "daddu $10, %3, %3              \n\t"
-        "gssdxc1 $f6, 0(%0, $10)        \n\t"
-        "gssdxc1 $f8, 0(%0, %1)         \n\t"
-        : "+&r"(pixels),"=&r"(line_skip3)
-        : "r"(block),"r"(line_skip),"r"(ff_pb_80)
-        : "$10","memory"
+        PTR_ADDU   "%[line_skip3],  %[line_skip],   %[line_skip]        \n\t"
+        "ldc1       %[ftmp1],       0x00(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x08(%[block])                      \n\t"
+        "packsshb   %[ftmp1],       %[ftmp1],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp2],       0x10(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x18(%[block])                      \n\t"
+        "packsshb   %[ftmp2],       %[ftmp2],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp3],       0x20(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x28(%[block])                      \n\t"
+        "packsshb   %[ftmp3],       %[ftmp3],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp4],       48(%[block])                        \n\t"
+        "ldc1       %[ftmp0],       56(%[block])                        \n\t"
+        "packsshb   %[ftmp4],       %[ftmp4],       %[ftmp0]            \n\t"
+        "paddb      %[ftmp1],       %[ftmp1],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp2],       %[ftmp2],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp3],       %[ftmp3],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp4],       %[ftmp4],       %[ff_pb_80]         \n\t"
+        "sdc1       %[ftmp1],       0x00(%[pixels])                     \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp2],       0x00(%[pixels], %[line_skip])       \n\t"
+        "gssdxc1    %[ftmp3],       0x00(%[pixels], %[line_skip3])      \n\t"
+        PTR_ADDU   "%[line_skip3],  %[line_skip3],  %[line_skip]        \n\t"
+        "gssdxc1    %[ftmp4],       0x00(%[pixels], %[line_skip3])      \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],       %[ftmp2]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[line_skip]        \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+        "dmfc1      %[all64],       %[ftmp3]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[line_skip3]       \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+        PTR_ADDU   "%[line_skip3],  %[line_skip3],  %[line_skip]        \n\t"
+        "dmfc1      %[all64],       %[ftmp4]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[line_skip3]       \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+#endif
+        PTR_ADDU   "%[addr0],       %[line_skip3],  %[line_skip]        \n\t"
+        PTR_ADDU   "%[pixels],      %[pixels],      %[addr0]            \n\t"
+        "ldc1       %[ftmp1],       0x40(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x48(%[block])                      \n\t"
+        "packsshb   %[ftmp1],       %[ftmp1],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp2],       0x50(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x58(%[block])                      \n\t"
+        "packsshb   %[ftmp2],       %[ftmp2],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp3],       0x60(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x68(%[block])                      \n\t"
+        "packsshb   %[ftmp3],       %[ftmp3],       %[ftmp0]            \n\t"
+        "ldc1       %[ftmp4],       0x70(%[block])                      \n\t"
+        "ldc1       %[ftmp0],       0x78(%[block])                      \n\t"
+        "packsshb   %[ftmp4],       %[ftmp4],       %[ftmp0]            \n\t"
+        "paddb      %[ftmp1],       %[ftmp1],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp2],       %[ftmp2],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp3],       %[ftmp3],       %[ff_pb_80]         \n\t"
+        "paddb      %[ftmp4],       %[ftmp4],       %[ff_pb_80]         \n\t"
+        "sdc1       %[ftmp1],       0x00(%[pixels])                     \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp2],       0x00(%[pixels], %[line_skip])       \n\t"
+        PTR_ADDU   "%[addr0],       %[line_skip],   %[line_skip]        \n\t"
+        "gssdxc1    %[ftmp3],       0x00(%[pixels], %[addr0])           \n\t"
+        "gssdxc1    %[ftmp4],       0x00(%[pixels], %[line_skip3])      \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],       %[ftmp2]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[line_skip]        \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+        PTR_ADDU   "%[addr0],       %[line_skip],   %[line_skip]        \n\t"
+        "dmfc1      %[all64],       %[ftmp3]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[addr0]            \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+        "dmfc1      %[all64],       %[ftmp4]                            \n\t"
+        PTR_ADDU   "%[addr1],       %[pixels],      %[line_skip3]       \n\t"
+        "usd        %[all64],       0x00(%[addr1])                      \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [pixels]"+&r"(pixels),            [line_skip3]"+&r"(line_skip3)
+        : [block]"r"(block),
+          [line_skip]"r"((mips_reg)line_skip),
+          [ff_pb_80]"f"(ff_pb_80)
+        : "memory"
     );
 }
 
 void ff_add_pixels_clamped_mmi(const int16_t *block,
         uint8_t *av_restrict pixels, ptrdiff_t line_size)
 {
-    const int16_t *p;
-    uint8_t *pix;
-    int i = 4;
-
-    p = block;
-    pix = pixels;
+    double ftmp[8];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    uint64_t all64;
 
     __asm__ volatile (
-        "xor $f14, $f14, $f14           \r\n"
-        ::
+        "li         %[tmp0],    0x04                                    \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "1:                                                             \n\t"
+        "ldc1       %[ftmp1],   0x00(%[block])                          \n\t"
+        "ldc1       %[ftmp2],   0x08(%[block])                          \n\t"
+        "ldc1       %[ftmp3],   0x10(%[block])                          \n\t"
+        "ldc1       %[ftmp4],   0x18(%[block])                          \n\t"
+        "ldc1       %[ftmp5],   0x00(%[pixels])                         \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp6],   0x00(%[pixels], %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[pixels],      %[line_size]            \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "mov.d      %[ftmp7],   %[ftmp5]                                \n\t"
+        "punpcklbh  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "mov.d      %[ftmp7],   %[ftmp6]                                \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp7]                \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp1],   0x00(%[pixels])                         \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp3],   0x00(%[pixels], %[line_size])           \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp3]                                \n\t"
+        PTR_ADDU   "%[addr1],   %[pixels],      %[line_size]            \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+#endif
+        "addi       %[tmp0],    %[tmp0],        -0x01                   \n\t"
+        PTR_ADDIU  "%[block],   %[block],       0x20                    \n\t"
+        PTR_ADDU   "%[pixels],  %[pixels],      %[line_size]            \n\t"
+        PTR_ADDU   "%[pixels],  %[pixels],      %[line_size]            \n\t"
+        "bnez       %[tmp0],    1b"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [pixels]"+&r"(pixels),            [block]"+&r"(block)
+        : [line_size]"r"((mips_reg)line_size)
+        : "memory"
     );
-
-    do {
-        __asm__ volatile (
-            "ldc1 $f0, 0+%2             \r\n"
-            "ldc1 $f2, 8+%2             \r\n"
-            "ldc1 $f4, 16+%2            \r\n"
-            "ldc1 $f6, 24+%2            \r\n"
-            "ldc1 $f8, %0               \r\n"
-            "ldc1 $f12, %1              \r\n"
-            "mov.d $f10, $f8            \r\n"
-            "punpcklbh $f8, $f8, $f14   \r\n"
-            "punpckhbh $f10, $f10, $f14 \r\n"
-            "paddsh $f0, $f0, $f8       \r\n"
-            "paddsh $f2, $f2, $f10      \r\n"
-            "mov.d $f10, $f12           \r\n"
-            "punpcklbh $f12, $f12, $f14 \r\n"
-            "punpckhbh $f10, $f10, $f14 \r\n"
-            "paddsh $f4, $f4, $f12      \r\n"
-            "paddsh $f6, $f6, $f10      \r\n"
-            "packushb $f0, $f0, $f2     \r\n"
-            "packushb $f4, $f4, $f6     \r\n"
-            "sdc1 $f0, %0               \r\n"
-            "sdc1 $f4, %1               \r\n"
-            : "+m"(*pix),"+m"(*(pix+line_size))
-            : "m"(*p)
-            : "memory"
-        );
-
-        pix += line_size*2;
-        p += 16;
-    } while (--i);
 }
diff --git a/libavcodec/mips/mpegvideo_mmi.c b/libavcodec/mips/mpegvideo_mmi.c
index 94781e6..138db47 100644
--- a/libavcodec/mips/mpegvideo_mmi.c
+++ b/libavcodec/mips/mpegvideo_mmi.c
@@ -23,11 +23,15 @@
  */
 
 #include "mpegvideo_mips.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
         int n, int qscale)
 {
     int64_t level, qmul, qadd, nCoeffs;
+    double ftmp[6];
+    mips_reg addr[1];
+    uint64_t all64;
 
     qmul = qscale << 1;
     av_assert2(s->block_last_index[n]>=0 || s->h263_aic);
@@ -49,48 +53,65 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
         nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
 
     __asm__ volatile (
-        "xor $f12, $f12, $f12           \r\n"
-        "lwc1 $f12, %1                  \n\r"
-        "xor $f10, $f10, $f10           \r\n"
-        "lwc1 $f10, %2                  \r\n"
-        "xor $f14, $f14, $f14           \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "packsswh $f10, $f10, $f10      \r\n"
-        "packsswh $f10, $f10, $f10      \r\n"
-        "psubh $f14, $f14, $f10         \r\n"
-        "xor $f8, $f8, $f8              \r\n"
-        ".p2align 4                     \r\n"
-        "1:                             \r\n"
-        "daddu $8, %0, %3               \r\n"
-        "gsldlc1 $f0, 7($8)             \r\n"
-        "gsldrc1 $f0, 0($8)             \r\n"
-        "gsldlc1 $f2, 15($8)            \r\n"
-        "gsldrc1 $f2, 8($8)             \r\n"
-        "mov.d $f4, $f0                 \r\n"
-        "mov.d $f6, $f2                 \r\n"
-        "pmullh $f0, $f0, $f12          \r\n"
-        "pmullh $f2, $f2, $f12          \r\n"
-        "pcmpgth $f4, $f4, $f8          \r\n"
-        "pcmpgth $f6, $f6, $f8          \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "paddh $f0, $f0, $f14           \r\n"
-        "paddh $f2, $f2, $f14           \r\n"
-        "xor $f4, $f4, $f0              \r\n"
-        "xor $f6, $f6, $f2              \r\n"
-        "pcmpeqh $f0, $f0, $f14         \r\n"
-        "pcmpeqh $f2, $f2, $f14         \r\n"
-        "pandn $f0, $f0, $f4            \r\n"
-        "pandn $f2, $f2, $f6            \r\n"
-        "gssdlc1 $f0, 7($8)             \r\n"
-        "gssdrc1 $f0, 0($8)             \r\n"
-        "gssdlc1 $f2, 15($8)            \r\n"
-        "gssdrc1 $f2, 8($8)             \r\n"
-        "addi %3, %3, 16                \r\n"
-        "blez %3, 1b                    \r\n"
-        ::"r"(block+nCoeffs),"m"(qmul),"m"(qadd),"r"(2*(-nCoeffs))
-        :"$8","memory"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
+        "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
+        "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
+        "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[qadd]                 \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        ".p2align   4                                                   \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[block],       %[nCoeffs]              \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "mov.d      %[ftmp3],   %[ftmp1]                                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[qmul]                 \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[qmul]                 \n\t"
+        "pcmpgth    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "pcmpeqh    %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "pcmpeqh    %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pandn      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pandn      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        PTR_ADDIU  "%[nCoeffs], %[nCoeffs],     0x10                    \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gssdlc1    %[ftmp2],   0x0f(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp2],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x08(%[addr0])                          \n\t"
+#endif
+        "blez       %[nCoeffs], 1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64)
+        : [block]"r"((mips_reg)(block+nCoeffs)),
+          [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
+          [qmul]"f"(qmul),                  [qadd]"f"(qadd)
+        : "memory"
     );
 
     block[0] = level;
@@ -100,6 +121,9 @@ void ff_dct_unquantize_h263_inter_mmi(MpegEncContext *s, int16_t *block,
         int n, int qscale)
 {
     int64_t qmul, qadd, nCoeffs;
+    double ftmp[6];
+    mips_reg addr[1];
+    uint64_t all64;
 
     qmul = qscale << 1;
     qadd = (qscale - 1) | 1;
@@ -107,48 +131,65 @@ void ff_dct_unquantize_h263_inter_mmi(MpegEncContext *s, int16_t *block,
     nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
 
     __asm__ volatile (
-        "xor $f12, $f12, $f12           \r\n"
-        "lwc1 $f12, %1                  \r\n"
-        "xor $f10, $f10, $f10           \r\n"
-        "lwc1 $f10, %2                  \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "xor $f14, $f14, $f14           \r\n"
-        "packsswh $f10, $f10, $f10      \r\n"
-        "packsswh $f10, $f10, $f10      \r\n"
-        "psubh $f14, $f14, $f10         \r\n"
-        "xor $f8, $f8, $f8              \r\n"
-        ".p2align 4                     \r\n"
-        "1:                             \r\n"
-        "daddu $8, %0, %3               \r\n"
-        "gsldlc1 $f0, 7($8)             \r\n"
-        "gsldrc1 $f0, 0($8)             \r\n"
-        "gsldlc1 $f2, 15($8)            \r\n"
-        "gsldrc1 $f2, 8($8)             \r\n"
-        "mov.d $f4, $f0                 \r\n"
-        "mov.d $f6, $f2                 \r\n"
-        "pmullh $f0, $f0, $f12          \r\n"
-        "pmullh $f2, $f2, $f12          \r\n"
-        "pcmpgth $f4, $f4, $f8          \r\n"
-        "pcmpgth $f6, $f6, $f8          \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "paddh $f0, $f0, $f14           \r\n"
-        "paddh $f2, $f2, $f14           \r\n"
-        "xor $f4, $f4, $f0              \r\n"
-        "xor $f6, $f6, $f2              \r\n"
-        "pcmpeqh $f0, $f0, $f14         \r\n"
-        "pcmpeqh $f2, $f2, $f14         \r\n"
-        "pandn $f0, $f0, $f4            \r\n"
-        "pandn $f2, $f2, $f6            \r\n"
-        "gssdlc1 $f0, 7($8)             \r\n"
-        "gssdrc1 $f0, 0($8)             \r\n"
-        "gssdlc1 $f2, 15($8)            \r\n"
-        "gssdrc1 $f2, 8($8)             \r\n"
-        "addi %3, %3, 16                \r\n"
-        "blez %3, 1b                    \r\n"
-        ::"r"(block+nCoeffs),"m"(qmul),"m"(qadd),"r"(2*(-nCoeffs))
-        : "$8","memory"
+        "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
+        "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
+        "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[qadd]                 \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        ".p2align   4                                                   \n\t"
+        "1:                                                             \n\t"
+        PTR_ADDU   "%[addr0],   %[block],       %[nCoeffs]              \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[addr0])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr0])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "mov.d      %[ftmp3],   %[ftmp1]                                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[qmul]                 \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[qmul]                 \n\t"
+        "pcmpgth    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
+        "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "pcmpeqh    %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "pcmpeqh    %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pandn      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pandn      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        PTR_ADDIU  "%[nCoeffs], %[nCoeffs],     0x10                    \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp1],   0x07(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[addr0])                          \n\t"
+        "gssdlc1    %[ftmp2],   0x0f(%[addr0])                          \n\t"
+        "gssdrc1    %[ftmp2],   0x08(%[addr0])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x08(%[addr0])                          \n\t"
+#endif
+        "blez       %[nCoeffs], 1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64)
+        : [block]"r"((mips_reg)(block+nCoeffs)),
+          [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
+          [qmul]"f"(qmul),                  [qadd]"f"(qadd)
+        : "memory"
     );
 }
 
@@ -158,6 +199,10 @@ void ff_dct_unquantize_mpeg1_intra_mmi(MpegEncContext *s, int16_t *block,
     int64_t nCoeffs;
     const uint16_t *quant_matrix;
     int block0;
+    double ftmp[10];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    uint64_t all64;
 
     av_assert2(s->block_last_index[n]>=0);
     nCoeffs = s->intra_scantable.raster_end[s->block_last_index[n]] + 1;
@@ -171,60 +216,93 @@ void ff_dct_unquantize_mpeg1_intra_mmi(MpegEncContext *s, int16_t *block,
     quant_matrix = s->intra_matrix;
 
     __asm__ volatile (
-        "pcmpeqh $f14, $f14, $f14       \r\n"
-        "dli $10, 15                    \r\n"
-        "dmtc1 $10, $f16                \r\n"
-        "xor $f12, $f12, $f12           \r\n"
-        "lwc1 $f12, %2                  \r\n"
-        "psrlh $f14, $f14, $f16         \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "or $8, %3, $0                  \r\n"
-        ".p2align 4                     \r\n"
-        "1:                             \r\n"
-        "gsldxc1 $f0, 0($8, %0)         \r\n"
-        "gsldxc1 $f2, 8($8, %0)         \r\n"
-        "mov.d $f16, $f0                \r\n"
-        "mov.d $f18, $f2                \r\n"
-        "gsldxc1 $f8, 0($8, %1)         \r\n"
-        "gsldxc1 $f10, 8($8, %1)        \r\n"
-        "pmullh $f8, $f8, $f12          \r\n"
-        "pmullh $f10, $f10, $f12        \r\n"
-        "xor $f4, $f4, $f4              \r\n"
-        "xor $f6, $f6, $f6              \r\n"
-        "pcmpgth $f4, $f4, $f0          \r\n"
-        "pcmpgth $f6, $f6, $f2          \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "psubh $f0, $f0, $f4            \r\n"
-        "psubh $f2, $f2, $f6            \r\n"
-        "pmullh $f0, $f0, $f8           \r\n"
-        "pmullh $f2, $f2, $f10          \r\n"
-        "xor $f8, $f8, $f8              \r\n"
-        "xor $f10, $f10, $f10           \r\n"
-        "pcmpeqh $f8, $f8, $f16         \r\n"
-        "pcmpeqh $f10, $f10, $f18       \r\n"
-        "dli $10, 3                     \r\n"
-        "dmtc1 $10, $f16                \r\n"
-        "psrah $f0, $f0, $f16           \r\n"
-        "psrah $f2, $f2, $f16           \r\n"
-        "psubh $f0, $f0, $f14           \r\n"
-        "psubh $f2, $f2, $f14           \r\n"
-        "or $f0, $f0, $f14              \r\n"
-        "or $f2, $f2, $f14              \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "psubh $f0, $f0, $f4            \r\n"
-        "psubh $f2, $f2, $f6            \r\n"
-        "pandn $f8, $f8, $f0            \r\n"
-        "pandn $f10, $f10, $f2          \r\n"
-        "gssdxc1 $f8, 0($8, %0)         \r\n"
-        "gssdxc1 $f10, 8($8, %0)        \r\n"
-        "addi $8, $8, 16                \r\n"
-        "bltz $8, 1b                    \r\n"
-        ::"r"(block+nCoeffs),"r"(quant_matrix+nCoeffs),"m"(qscale),
-          "g"(-2*nCoeffs)
-        : "$8","$10","memory"
+        "dli        %[tmp0],    0x0f                                    \n\t"
+        "pcmpeqh    %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dmtc1      %[tmp0],    %[ftmp4]                                \n\t"
+        "dmtc1      %[qscale],  %[ftmp1]                                \n\t"
+        "psrlh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "packsswh   %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "or         %[addr0],   %[nCoeffs],     $0                      \n\t"
+        ".p2align   4                                                   \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp2],   0x00(%[addr0],  %[block])               \n\t"
+        "gsldxc1    %[ftmp3],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp3]                                \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp6],   0x00(%[addr0],  %[quant])               \n\t"
+        "gsldxc1    %[ftmp7],   0x08(%[addr0],  %[quant])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[quant]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "xor        %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "pcmpgth    %[ftmp9],   %[ftmp9],       %[ftmp3]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pcmpeqh    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "dli        %[tmp0],    0x03                                    \n\t"
+        "pcmpeqh    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "dmtc1      %[tmp0],    %[ftmp4]                                \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "or         %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "or         %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "pandn      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp6],   0x00(%[addr0],  %[block])               \n\t"
+        "gssdxc1    %[ftmp7],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp7]                                \n\t"
+        "usd        %[all64],   0x08(%[addr1])                          \n\t"
+#endif
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x10                    \n\t"
+        "bltz       %[addr0],   1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64)
+        : [block]"r"((mips_reg)(block+nCoeffs)),
+          [quant]"r"((mips_reg)(quant_matrix+nCoeffs)),
+          [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
+          [qscale]"r"(qscale)
+        : "memory"
     );
 
     block[0] = block0;
@@ -235,132 +313,107 @@ void ff_dct_unquantize_mpeg1_inter_mmi(MpegEncContext *s, int16_t *block,
 {
     int64_t nCoeffs;
     const uint16_t *quant_matrix;
+    double ftmp[10];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    uint64_t all64;
 
     av_assert2(s->block_last_index[n] >= 0);
     nCoeffs = s->intra_scantable.raster_end[s->block_last_index[n]] + 1;
     quant_matrix = s->inter_matrix;
 
     __asm__ volatile (
-        "pcmpeqh $f14, $f14, $f14       \r\n"
-        "dli $10, 15                    \r\n"
-        "dmtc1 $10, $f16                \r\n"
-        "xor $f12, $f12, $f12           \r\n"
-        "lwc1 $f12, %2                  \r\n"
-        "psrlh $f14, $f14, $f16         \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "packsswh $f12, $f12, $f12      \r\n"
-        "or $8, %3, $0                  \r\n"
-        ".p2align 4                     \r\n"
-        "1:                             \r\n"
-        "gsldxc1 $f0, 0($8, %0)         \r\n"
-        "gsldxc1 $f2, 8($8, %0)         \r\n"
-        "mov.d $f16, $f0                \r\n"
-        "mov.d $f18, $f2                \r\n"
-        "gsldxc1 $f8, 0($8, %1)         \r\n"
-        "gsldxc1 $f10, 8($8, %1)        \r\n"
-        "pmullh $f8, $f8, $f12          \r\n"
-        "pmullh $f10, $f10, $f12        \r\n"
-        "xor $f4, $f4, $f4              \r\n"
-        "xor $f6, $f6, $f6              \r\n"
-        "pcmpgth $f4, $f4, $f0          \r\n"
-        "pcmpgth $f6, $f6, $f2          \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "psubh $f0, $f0, $f4            \r\n"
-        "psubh $f2, $f2, $f6            \r\n"
-        "paddh $f0, $f0, $f0            \r\n"
-        "paddh $f2, $f2, $f2            \r\n"
-        "paddh $f0, $f0, $f14           \r\n"
-        "paddh $f2, $f2, $f14           \r\n"
-        "pmullh $f0, $f0, $f8           \r\n"
-        "pmullh $f2, $f2, $f10          \r\n"
-        "xor $f8, $f8, $f8              \r\n"
-        "xor $f10, $f10, $f10           \r\n"
-        "pcmpeqh $f8, $f8, $f16         \r\n"
-        "pcmpeqh $f10, $f10, $f18       \r\n"
-        "dli $10, 4                     \r\n"
-        "dmtc1 $10, $f16                \r\n"
-        "psrah $f0, $f0, $f16           \r\n"
-        "psrah $f2, $f2, $f16           \r\n"
-        "psubh $f0, $f0, $f14           \r\n"
-        "psubh $f2, $f2, $f14           \r\n"
-        "or $f0, $f0, $f14              \r\n"
-        "or $f2, $f2, $f14              \r\n"
-        "xor $f0, $f0, $f4              \r\n"
-        "xor $f2, $f2, $f6              \r\n"
-        "psubh $f0, $f0, $f4            \r\n"
-        "psubh $f2, $f2, $f6            \r\n"
-        "pandn $f8, $f8, $f0            \r\n"
-        "pandn $f10, $f10, $f2          \r\n"
-        "gssdxc1 $f8, 0($8, %0)         \r\n"
-        "gssdxc1 $f10, 8($8, %0)        \r\n"
-        "addi $8, $8, 16                \r\n"
-        "bltz $8, 1b                    \r\n"
-        ::"r"(block+nCoeffs),"r"(quant_matrix+nCoeffs),"m"(qscale),
-          "g"(-2*nCoeffs)
-        :"$8","$10","memory"
-    );
-}
-
-void ff_denoise_dct_mmi(MpegEncContext *s, int16_t *block)
-{
-    const int intra = s->mb_intra;
-    int *sum = s->dct_error_sum[intra];
-    uint16_t *offset = s->dct_offset[intra];
-
-    s->dct_count[intra]++;
-
-    __asm__ volatile(
-        "xor $f14, $f14, $f14               \r\n"
-        "1:                                 \r\n"
-        "ldc1 $f4, 0(%[block])              \r\n"
-        "xor $f0, $f0, $f0                  \r\n"
-        "ldc1 $f6, 8(%[block])              \r\n"
-        "xor $f2, $f2, $f2                  \r\n"
-        "pcmpgth $f0, $f0, $f4              \r\n"
-        "pcmpgth $f2, $f2, $f6              \r\n"
-        "xor $f4, $f4, $f0                  \r\n"
-        "xor $f6, $f6, $f2                  \r\n"
-        "psubh $f4, $f4, $f0                \r\n"
-        "psubh $f6, $f6, $f2                \r\n"
-        "ldc1 $f12, 0(%[offset])            \r\n"
-        "mov.d $f8, $f4                     \r\n"
-        "psubush $f4, $f4, $f12             \r\n"
-        "ldc1 $f12, 8(%[offset])            \r\n"
-        "mov.d $f10, $f6                    \r\n"
-        "psubush $f6, $f6, $f12             \r\n"
-        "xor $f4, $f4, $f0                  \r\n"
-        "xor $f6, $f6, $f2                  \r\n"
-        "psubh $f4, $f4, $f0                \r\n"
-        "psubh $f6, $f6, $f2                \r\n"
-        "sdc1 $f4, 0(%[block])              \r\n"
-        "sdc1 $f6, 8(%[block])              \r\n"
-        "mov.d $f4, $f8                     \r\n"
-        "mov.d $f6, $f10                    \r\n"
-        "punpcklhw $f8, $f8, $f14           \r\n"
-        "punpckhhw $f4, $f4, $f14           \r\n"
-        "punpcklhw $f10, $f10, $f14         \r\n"
-        "punpckhhw $f6, $f6, $f14           \r\n"
-        "ldc1 $f0, 0(%[sum])                \r\n"
-        "paddw $f8, $f8, $f0                \r\n"
-        "ldc1 $f0, 8(%[sum])                \r\n"
-        "paddw $f4, $f4, $f0                \r\n"
-        "ldc1 $f0, 16(%[sum])               \r\n"
-        "paddw $f10, $f10, $f0              \r\n"
-        "ldc1 $f0, 24(%[sum])               \r\n"
-        "paddw $f6, $f6, $f0                \r\n"
-        "sdc1 $f8, 0(%[sum])                \r\n"
-        "sdc1 $f4, 8(%[sum])                \r\n"
-        "sdc1 $f10, 16(%[sum])              \r\n"
-        "sdc1 $f6, 24(%[sum])               \r\n"
-        "daddiu %[block], %[block], 16      \r\n"
-        "daddiu %[sum], %[sum], 32          \r\n"
-        "daddiu %[offset], %[offset], 16    \r\n"
-        "dsubu $8, %[block1], %[block]      \r\n"
-        "bgtz $8, 1b                        \r\n"
-        : [block]"+r"(block),[sum]"+r"(sum),[offset]"+r"(offset)
-        : [block1]"r"(block+64)
-        : "$8","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14"
+        "dli        %[tmp0],    0x0f                                    \n\t"
+        "pcmpeqh    %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dmtc1      %[tmp0],    %[ftmp4]                                \n\t"
+        "dmtc1      %[qscale],  %[ftmp1]                                \n\t"
+        "psrlh      %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "packsswh   %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "packsswh   %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "or         %[addr0],   %[nCoeffs],     $0                      \n\t"
+        ".p2align   4                                                   \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp2],   0x00(%[addr0],  %[block])               \n\t"
+        "gsldxc1    %[ftmp3],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+#endif
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+        "mov.d      %[ftmp5],   %[ftmp3]                                \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp6],   0x00(%[addr0],  %[quant])               \n\t"
+        "gsldxc1    %[ftmp7],   0x08(%[addr0],  %[quant])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[quant]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp7]                                \n\t"
+#endif
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
+        "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "xor        %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "pcmpgth    %[ftmp9],   %[ftmp9],       %[ftmp3]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pcmpeqh    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
+        "dli        %[tmp0],    0x04                                    \n\t"
+        "pcmpeqh    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "dmtc1      %[tmp0],    %[ftmp4]                                \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "or         %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "or         %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "pandn      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp6],   0x00(%[addr0],  %[block])               \n\t"
+        "gssdxc1    %[ftmp7],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp7]                                \n\t"
+        "usd        %[all64],   0x08(%[addr1])                          \n\t"
+#endif
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x10                    \n\t"
+        "bltz       %[addr0],   1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64)
+        : [block]"r"((mips_reg)(block+nCoeffs)),
+          [quant]"r"((mips_reg)(quant_matrix+nCoeffs)),
+          [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
+          [qscale]"r"(qscale)
+        : "memory"
     );
 }
 
@@ -370,6 +423,10 @@ void ff_dct_unquantize_mpeg2_intra_mmi(MpegEncContext *s, int16_t *block,
     uint64_t nCoeffs;
     const uint16_t *quant_matrix;
     int block0;
+    double ftmp[10];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    uint64_t all64;
 
     assert(s->block_last_index[n]>=0);
 
@@ -386,58 +443,160 @@ void ff_dct_unquantize_mpeg2_intra_mmi(MpegEncContext *s, int16_t *block,
     quant_matrix = s->intra_matrix;
 
     __asm__ volatile (
-        "pcmpeqh $f14, $f14, $f14           \r\n"
-        "dli $10, 15                        \r\n"
-        "dmtc1 $10, $f16                    \r\n"
-        "xor $f12, $f12, $f12               \r\n"
-        "lwc1 $f12, %[qscale]               \r\n"
-        "psrlh $f14, $f14, $f16             \r\n"
-        "packsswh $f12, $f12, $f12          \r\n"
-        "packsswh $f12, $f12, $f12          \r\n"
-        "or $8, %[ncoeffs], $0              \r\n"
-        ".p2align 4                         \r\n"
-        "1:                                 \r\n"
-        "gsldxc1 $f0, 0($8, %[block])       \r\n"
-        "gsldxc1 $f2, 8($8, %[block])       \r\n"
-        "mov.d $f16, $f0                    \r\n"
-        "mov.d $f18, $f2                    \r\n"
-        "gsldxc1 $f8, 0($8, %[quant])       \r\n"
-        "gsldxc1 $f10, 0($8, %[quant])      \r\n"
-        "pmullh $f8, $f8, $f12              \r\n"
-        "pmullh $f10, $f10, $f12            \r\n"
-        "xor $f4, $f4, $f4                  \r\n"
-        "xor $f6, $f6, $f6                  \r\n"
-        "pcmpgth $f4, $f4, $f0              \r\n"
-        "pcmpgth $f6, $f6, $f2              \r\n"
-        "xor $f0, $f0, $f4                  \r\n"
-        "xor $f2, $f2, $f6                  \r\n"
-        "psubh $f0, $f0, $f4                \r\n"
-        "psubh $f2, $f2, $f6                \r\n"
-        "pmullh $f0, $f0, $f8               \r\n"
-        "pmullh $f2, $f2, $f10              \r\n"
-        "xor $f8, $f8, $f8                  \r\n"
-        "xor $f10, $f10, $f10               \r\n"
-        "pcmpeqh $f8, $f8, $f16             \r\n"
-        "pcmpeqh $f10 ,$f10, $f18           \r\n"
-        "dli $10, 3                         \r\n"
-        "dmtc1 $10, $f16                    \r\n"
-        "psrah $f0, $f0, $f16               \r\n"
-        "psrah $f2, $f2, $f16               \r\n"
-        "xor $f0, $f0, $f4                  \r\n"
-        "xor $f2, $f2, $f6                  \r\n"
-        "psubh $f0, $f0, $f4                \r\n"
-        "psubh $f2, $f2, $f6                \r\n"
-        "pandn $f8, $f8, $f0                \r\n"
-        "pandn $f10, $f10, $f2              \r\n"
-        "gssdxc1 $f8, 0($8, %[block])       \r\n"
-        "gssdxc1 $f10, 8($8, %[block])      \r\n"
-        "daddiu $8, $8, 16                  \r\n"
-        "blez $8, 1b                        \r\n"
-        ::[block]"r"(block+nCoeffs),[quant]"r"(quant_matrix+nCoeffs),
-          [qscale]"m"(qscale),[ncoeffs]"g"(-2*nCoeffs)
-        : "$8","$10","$f0","$f2","$f4","$f6","$f8","$f10","$f12","$f14","$f16",
-          "$f18"
+        "dli        %[tmp0],    0x0f                                    \n\t"
+        "pcmpeqh    %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp3]                                \n\t"
+        "mtc1       %[qscale],  %[ftmp9]                                \n\t"
+        "psrlh      %[ftmp0],   %[ftmp0],       %[ftmp3]                \n\t"
+        "packsswh   %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "packsswh   %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "or         %[addr0],   %[nCoeffs],     $0                      \n\t"
+        ".p2align   4                                                   \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp1],   0x00(%[addr0],  %[block])               \n\t"
+        "gsldxc1    %[ftmp2],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "mov.d      %[ftmp3],   %[ftmp1]                                \n\t"
+        "mov.d      %[ftmp4],   %[ftmp2]                                \n\t"
+#if HAVE_LOONGSON3
+        "gsldxc1    %[ftmp5],   0x00(%[addr0],  %[quant])               \n\t"
+        "gsldxc1    %[ftmp6],   0x00(%[addr0],  %[quant])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "uld        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp5]                                \n\t"
+        "uld        %[all64],   0x08(%[addr1])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp6]                                \n\t"
+#endif
+        "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
+        "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pcmpgth    %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
+        "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
+        "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
+        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pcmpeqh    %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
+        "dli        %[tmp0],    0x03                                    \n\t"
+        "pcmpeqh    %[ftmp6] ,  %[ftmp6],       %[ftmp4]                \n\t"
+        "mtc1       %[tmp0],    %[ftmp3]                                \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psrah      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pandn      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x10                    \n\t"
+#if HAVE_LOONGSON3
+        "gssdxc1    %[ftmp5],   0x00(%[addr0],  %[block])               \n\t"
+        "gssdxc1    %[ftmp6],   0x08(%[addr0],  %[block])               \n\t"
+#elif HAVE_LOONGSON2
+        PTR_ADDU   "%[addr1],   %[addr0],       %[block]                \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x00(%[addr1])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp6]                                \n\t"
+        "usd        %[all64],   0x08(%[addr1])                          \n\t"
+#endif
+        "blez       %[addr0],   1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64)
+        : [block]"r"((mips_reg)(block+nCoeffs)),
+          [quant]"r"((mips_reg)(quant_matrix+nCoeffs)),
+          [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
+          [qscale]"r"(qscale)
+        : "memory"
     );
 
     block[0]= block0;
 }
+
+void ff_denoise_dct_mmi(MpegEncContext *s, int16_t *block)
+{
+    const int intra = s->mb_intra;
+    int *sum = s->dct_error_sum[intra];
+    uint16_t *offset = s->dct_offset[intra];
+    double ftmp[8];
+    mips_reg addr[1];
+
+    s->dct_count[intra]++;
+
+    __asm__ volatile(
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "1:                                                             \n\t"
+        "ldc1       %[ftmp1],   0x00(%[block])                          \n\t"
+        "xor        %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "ldc1       %[ftmp3],   0x08(%[block])                          \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "pcmpgth    %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
+        "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "ldc1       %[ftmp6],   0x00(%[offset])                         \n\t"
+        "mov.d      %[ftmp5],   %[ftmp1]                                \n\t"
+        "psubush    %[ftmp1],   %[ftmp1],       %[ftmp6]                \n\t"
+        "ldc1       %[ftmp6],   0x08(%[offset])                         \n\t"
+        "mov.d      %[ftmp7],   %[ftmp3]                                \n\t"
+        "psubush    %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
+        "xor        %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "sdc1       %[ftmp1],   0x00(%[block])                          \n\t"
+        "sdc1       %[ftmp3],   0x08(%[block])                          \n\t"
+        "mov.d      %[ftmp1],   %[ftmp5]                                \n\t"
+        "mov.d      %[ftmp3],   %[ftmp7]                                \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
+        "punpckhhw  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklhw  %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
+        "punpckhhw  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "ldc1       %[ftmp2],   0x00(%[sum])                            \n\t"
+        "paddw      %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "ldc1       %[ftmp2],   0x08(%[sum])                            \n\t"
+        "paddw      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "ldc1       %[ftmp2],   0x10(%[sum])                            \n\t"
+        "paddw      %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "ldc1       %[ftmp2],   0x18(%[sum])                            \n\t"
+        "paddw      %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
+        "sdc1       %[ftmp5],   0x00(%[sum])                            \n\t"
+        "sdc1       %[ftmp1],   0x08(%[sum])                            \n\t"
+        "sdc1       %[ftmp7],   0x10(%[sum])                            \n\t"
+        "sdc1       %[ftmp3],   0x18(%[sum])                            \n\t"
+        PTR_ADDIU  "%[block],   %[block],       0x10                    \n\t"
+        PTR_ADDIU  "%[sum],     %[sum],         0x20                    \n\t"
+        PTR_SUBU   "%[addr0],   %[block1],      %[block]                \n\t"
+        PTR_ADDIU  "%[offset],  %[offset],      0x10                    \n\t"
+        "bgtz       %[addr0],   1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [addr0]"=&r"(addr[0]),
+          [block]"+&r"(block),              [sum]"+&r"(sum),
+          [offset]"+&r"(offset)
+        : [block1]"r"(block+64)
+        : "memory"
+    );
+}
diff --git a/libavcodec/mips/pixblockdsp_mmi.c b/libavcodec/mips/pixblockdsp_mmi.c
index 30631d8..07d89a6 100644
--- a/libavcodec/mips/pixblockdsp_mmi.c
+++ b/libavcodec/mips/pixblockdsp_mmi.c
@@ -22,58 +22,110 @@
  */
 
 #include "pixblockdsp_mips.h"
+#include "libavutil/mips/asmdefs.h"
 
 void ff_get_pixels_8_mmi(int16_t *av_restrict block, const uint8_t *pixels,
         ptrdiff_t line_size)
 {
+    double ftmp[6];
+    mips_reg tmp[2];
+    mips_reg addr[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "move $8, $0                    \n\t"
-        "xor $f0, $f0, $f0              \n\t"
-        "1:                             \n\t"
-        "gsldlc1 $f2, 7(%1)             \n\t"
-        "gsldrc1 $f2, 0(%1)             \n\t"
-        "punpcklbh $f4, $f2, $f0        \n\t"
-        "punpckhbh $f6, $f2, $f0        \n\t"
-        "gssdxc1 $f4, 0(%0, $8)         \n\t"
-        "gssdxc1 $f6, 8(%0, $8)         \n\t"
-        "daddiu $8, $8, 16              \n\t"
-        "daddu %1, %1, %2               \n\t"
-        "daddi %3, %3, -1               \n\t"
-        "bnez %3, 1b                    \n\t"
-        ::"r"((uint8_t *)block),"r"(pixels),"r"(line_size),"r"(8)
-        : "$8","memory"
+        "li         %[tmp1],    0x08                                    \n\t"
+        "move       %[tmp0],    $0                                      \n\t"
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[pixels])                         \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[pixels])                         \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp1],       %[ftmp0]                \n\t"
+        "gssdxc1    %[ftmp2],   0x00(%[block],  %[tmp0])                \n\t"
+        "gssdxc1    %[ftmp5],   0x08(%[block],  %[tmp0])                \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[pixels])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp1],       %[ftmp0]                \n\t"
+        PTR_ADDU   "%[addr0],   %[block],       %[tmp0]                 \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                                \n\t"
+        "usd        %[all64],   0x00(%[addr0])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp5]                                \n\t"
+        "usd        %[all64],   0x08(%[addr0])                          \n\t"
+#endif
+        PTR_ADDI   "%[tmp1],    %[tmp1],       -0x01                    \n\t"
+        PTR_ADDIU  "%[tmp0],    %[tmp0],        0x10                    \n\t"
+        PTR_ADDU   "%[pixels],  %[pixels],      %[line_size]            \n\t"
+        "bnez       %[tmp1],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [addr0]"=&r"(addr[0]),
+          [all64]"=&r"(all64),
+          [pixels]"+&r"(pixels)
+        : [block]"r"((mips_reg)block),      [line_size]"r"((mips_reg)line_size)
+        : "memory"
     );
 }
 
 void ff_diff_pixels_mmi(int16_t *av_restrict block, const uint8_t *src1,
         const uint8_t *src2, int stride)
 {
+    double ftmp[5];
+    mips_reg tmp[1];
+    uint64_t all64;
+
     __asm__ volatile (
-        "dli $2, 8                     \n\t"
-        "xor $f14, $f14, $f14          \n\t"
-        "1:                            \n\t"
-        "gsldlc1 $f0, 7(%1)            \n\t"
-        "gsldrc1 $f0, 0(%1)            \n\t"
-        "or $f2, $f0, $f0              \n\t"
-        "gsldlc1 $f4, 7(%2)            \n\t"
-        "gsldrc1 $f4, 0(%2)            \n\t"
-        "or $f6, $f4, $f4              \n\t"
-        "punpcklbh $f0, $f0, $f14      \n\t"
-        "punpckhbh $f2, $f2, $f14      \n\t"
-        "punpcklbh $f4, $f4, $f14      \n\t"
-        "punpckhbh $f6, $f6, $f14      \n\t"
-        "psubh $f0, $f0, $f4           \n\t"
-        "psubh $f2, $f2, $f6           \n\t"
-        "gssdlc1 $f0, 7(%0)            \n\t"
-        "gssdrc1 $f0, 0(%0)            \n\t"
-        "gssdlc1 $f2, 15(%0)           \n\t"
-        "gssdrc1 $f2, 8(%0)            \n\t"
-        "daddi %0, %0, 16              \n\t"
-        "daddu %1, %1, %3              \n\t"
-        "daddu %2, %2, %3              \n\t"
-        "daddi $2, $2, -1              \n\t"
-        "bgtz $2, 1b                   \n\t"
-        ::"r"(block),"r"(src1),"r"(src2),"r"(stride)
-        : "$2","memory"
+        "li         %[tmp0],    0x08                                    \n\t"
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "1:                                                             \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src1])                           \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src1])                           \n\t"
+        "or         %[ftmp1],   %[ftmp0],       %[ftmp0]                \n\t"
+        "gsldlc1    %[ftmp2],   0x07(%[src2])                           \n\t"
+        "gsldrc1    %[ftmp2],   0x00(%[src2])                           \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src1])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                                \n\t"
+        "or         %[ftmp1],   %[ftmp0],       %[ftmp0]                \n\t"
+        "uld        %[all64],   0x00(%[src2])                           \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+#endif
+        "or         %[ftmp3],   %[ftmp2],       %[ftmp2]                \n\t"
+        "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "punpckhbh  %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "punpckhbh  %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
+        "psubh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdlc1    %[ftmp1],   0x0f(%[block])                          \n\t"
+        "gssdrc1    %[ftmp1],   0x08(%[block])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[block])                          \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                                \n\t"
+        "usd        %[all64],   0x08(%[block])                          \n\t"
+#endif
+        PTR_ADDI   "%[tmp0],    %[tmp0], -0x01                          \n\t"
+        PTR_ADDIU  "%[block],   %[block], 0x10                          \n\t"
+        PTR_ADDU   "%[src1],    %[src1],        %[stride]               \n\t"
+        PTR_ADDU   "%[src2],    %[src2],        %[stride]               \n\t"
+        "bgtz       %[tmp0],    1b                                      \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),
+          [tmp0]"=&r"(tmp[0]),
+          [all64]"=&r"(all64),
+          [block]"+&r"(block),              [src1]"+&r"(src1),
+          [src2]"+&r"(src2)
+        : [stride]"r"((mips_reg)stride)
+        : "memory"
     );
 }
diff --git a/libavcodec/mips/vp8dsp_init_mips.c b/libavcodec/mips/vp8dsp_init_mips.c
index 58d1b6c..5ca5261 100644
--- a/libavcodec/mips/vp8dsp_init_mips.c
+++ b/libavcodec/mips/vp8dsp_init_mips.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2015 Manojkumar Bhosale (Manojkumar.Bhosale@imgtec.com)
+ * Copyright (c) 2015 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -105,9 +106,97 @@ static av_cold void vp8dsp_init_msa(VP8DSPContext *dsp)
 }
 #endif  // #if HAVE_MSA
 
+#if HAVE_MMI
+static av_cold void vp8dsp_init_mmi(VP8DSPContext *dsp)
+{
+    dsp->vp8_luma_dc_wht    = ff_vp8_luma_dc_wht_mmi;
+    dsp->vp8_luma_dc_wht_dc = ff_vp8_luma_dc_wht_dc_mmi;
+    dsp->vp8_idct_add       = ff_vp8_idct_add_mmi;
+    dsp->vp8_idct_dc_add    = ff_vp8_idct_dc_add_mmi;
+    dsp->vp8_idct_dc_add4y  = ff_vp8_idct_dc_add4y_mmi;
+    dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_mmi;
+
+    dsp->put_vp8_epel_pixels_tab[0][0][1] = ff_put_vp8_epel16_h4_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][0][2] = ff_put_vp8_epel16_h6_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][1][0] = ff_put_vp8_epel16_v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][1][1] = ff_put_vp8_epel16_h4v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][1][2] = ff_put_vp8_epel16_h6v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][2][0] = ff_put_vp8_epel16_v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][2][1] = ff_put_vp8_epel16_h4v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[0][2][2] = ff_put_vp8_epel16_h6v6_mmi;
+
+    dsp->put_vp8_epel_pixels_tab[1][0][1] = ff_put_vp8_epel8_h4_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][0][2] = ff_put_vp8_epel8_h6_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][1][0] = ff_put_vp8_epel8_v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][1][1] = ff_put_vp8_epel8_h4v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][1][2] = ff_put_vp8_epel8_h6v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][2][0] = ff_put_vp8_epel8_v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][2][1] = ff_put_vp8_epel8_h4v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[1][2][2] = ff_put_vp8_epel8_h6v6_mmi;
+
+    dsp->put_vp8_epel_pixels_tab[2][0][1] = ff_put_vp8_epel4_h4_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][0][2] = ff_put_vp8_epel4_h6_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][1][0] = ff_put_vp8_epel4_v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][1][1] = ff_put_vp8_epel4_h4v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][1][2] = ff_put_vp8_epel4_h6v4_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][2][0] = ff_put_vp8_epel4_v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][2][1] = ff_put_vp8_epel4_h4v6_mmi;
+    dsp->put_vp8_epel_pixels_tab[2][2][2] = ff_put_vp8_epel4_h6v6_mmi;
+
+    dsp->put_vp8_bilinear_pixels_tab[0][0][1] = ff_put_vp8_bilinear16_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][0][2] = ff_put_vp8_bilinear16_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][1][0] = ff_put_vp8_bilinear16_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][1][1] = ff_put_vp8_bilinear16_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][1][2] = ff_put_vp8_bilinear16_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][2][0] = ff_put_vp8_bilinear16_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][2][1] = ff_put_vp8_bilinear16_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][2][2] = ff_put_vp8_bilinear16_hv_mmi;
+
+    dsp->put_vp8_bilinear_pixels_tab[1][0][1] = ff_put_vp8_bilinear8_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][0][2] = ff_put_vp8_bilinear8_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][1][0] = ff_put_vp8_bilinear8_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][1][1] = ff_put_vp8_bilinear8_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][1][2] = ff_put_vp8_bilinear8_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][2][0] = ff_put_vp8_bilinear8_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][2][1] = ff_put_vp8_bilinear8_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][2][2] = ff_put_vp8_bilinear8_hv_mmi;
+
+    dsp->put_vp8_bilinear_pixels_tab[2][0][1] = ff_put_vp8_bilinear4_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][0][2] = ff_put_vp8_bilinear4_h_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][1][0] = ff_put_vp8_bilinear4_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][1][1] = ff_put_vp8_bilinear4_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][1][2] = ff_put_vp8_bilinear4_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][2][0] = ff_put_vp8_bilinear4_v_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][2][1] = ff_put_vp8_bilinear4_hv_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[2][2][2] = ff_put_vp8_bilinear4_hv_mmi;
+
+    dsp->put_vp8_epel_pixels_tab[0][0][0]     = ff_put_vp8_pixels16_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[0][0][0] = ff_put_vp8_pixels16_mmi;
+
+    dsp->put_vp8_epel_pixels_tab[1][0][0]     = ff_put_vp8_pixels8_mmi;
+    dsp->put_vp8_bilinear_pixels_tab[1][0][0] = ff_put_vp8_pixels8_mmi;
+
+    dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_mmi;
+    dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_mmi;
+    dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_mmi;
+    dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_mmi;
+
+    dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_mmi;
+    dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_mmi;
+    dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_mmi;
+    dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_mmi;
+
+    dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_mmi;
+    dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_mmi;
+}
+#endif /* HAVE_MMI */
+
 av_cold void ff_vp8dsp_init_mips(VP8DSPContext *dsp)
 {
 #if HAVE_MSA
     vp8dsp_init_msa(dsp);
 #endif  // #if HAVE_MSA
+#if HAVE_MMI
+    vp8dsp_init_mmi(dsp);
+#endif /* HAVE_MMI */
 }
diff --git a/libavcodec/mips/vp8dsp_mips.h b/libavcodec/mips/vp8dsp_mips.h
index 8e715b5..4288e00 100644
--- a/libavcodec/mips/vp8dsp_mips.h
+++ b/libavcodec/mips/vp8dsp_mips.h
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2015 Manojkumar Bhosale (Manojkumar.Bhosale@imgtec.com)
+ * Copyright (c) 2015 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -21,6 +22,8 @@
 #ifndef AVCODEC_MIPS_VP8DSP_MIPS_H
 #define AVCODEC_MIPS_VP8DSP_MIPS_H
 
+#include "libavcodec/vp8.h"
+
 void ff_put_vp8_pixels4_msa(uint8_t *dst, ptrdiff_t dststride,
                             uint8_t *src, ptrdiff_t srcstride,
                             int h, int x, int y);
@@ -160,6 +163,7 @@ void ff_vp8_v_loop_filter8uv_msa(uint8_t *dst_u, uint8_t *dst_v,
 void ff_vp8_h_loop_filter_simple_msa(uint8_t *dst, ptrdiff_t stride, int flim);
 void ff_vp8_v_loop_filter_simple_msa(uint8_t *dst, ptrdiff_t stride, int flim);
 
+
 /* Idct functions */
 void ff_vp8_luma_dc_wht_msa(int16_t block[4][4][16], int16_t dc[16]);
 void ff_vp8_idct_add_msa(uint8_t *dst, int16_t block[16], ptrdiff_t stride);
@@ -169,4 +173,116 @@ void ff_vp8_idct_dc_add4uv_msa(uint8_t *dst, int16_t block[4][16],
 void ff_vp8_idct_dc_add4y_msa(uint8_t *dst, int16_t block[4][16],
                               ptrdiff_t stride);
 
+
+void ff_vp8_luma_dc_wht_mmi(int16_t block[4][4][16], int16_t dc[16]);
+void ff_vp8_luma_dc_wht_dc_mmi(int16_t block[4][4][16], int16_t dc[16]);
+void ff_vp8_idct_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride);
+void ff_vp8_idct_dc_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride);
+void ff_vp8_idct_dc_add4y_mmi(uint8_t *dst, int16_t block[4][16],
+        ptrdiff_t stride);
+void ff_vp8_idct_dc_add4uv_mmi(uint8_t *dst, int16_t block[4][16],
+        ptrdiff_t stride);
+
+void ff_put_vp8_pixels4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y);
+void ff_put_vp8_pixels8_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y);
+void ff_put_vp8_pixels16_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y);
+
+void ff_put_vp8_epel16_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel16_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+void ff_put_vp8_epel8_h4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_h6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel8_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+void ff_put_vp8_epel4_h4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_h6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_epel4_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+void ff_put_vp8_bilinear16_h_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear16_v_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear16_hv_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+void ff_put_vp8_bilinear8_h_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear8_v_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear8_hv_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+void ff_put_vp8_bilinear4_h_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear4_v_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+void ff_put_vp8_bilinear4_hv_mmi(uint8_t *dst, ptrdiff_t dststride,
+        uint8_t *src, ptrdiff_t srcstride, int h, int mx, int my);
+
+// loop filter applied to edges between macroblocks
+void ff_vp8_v_loop_filter16_mmi(uint8_t *dst, ptrdiff_t stride, int flim_E,
+        int flim_I, int hev_thresh);
+void ff_vp8_h_loop_filter16_mmi(uint8_t *dst, ptrdiff_t stride, int flim_E,
+        int flim_I, int hev_thresh);
+void ff_vp8_v_loop_filter8uv_mmi(uint8_t *dstU, uint8_t *dstV, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh);
+void ff_vp8_h_loop_filter8uv_mmi(uint8_t *dstU, uint8_t *dstV, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh);
+
+// loop filter applied to inner macroblock edges
+void ff_vp8_v_loop_filter16_inner_mmi(uint8_t *dst, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh);
+void ff_vp8_h_loop_filter16_inner_mmi(uint8_t *dst, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh);
+void ff_vp8_v_loop_filter8uv_inner_mmi(uint8_t *dstU, uint8_t *dstV,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh);
+void ff_vp8_h_loop_filter8uv_inner_mmi(uint8_t *dstU, uint8_t *dstV,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh);
+
+void ff_vp8_v_loop_filter_simple_mmi(uint8_t *dst, ptrdiff_t stride, int flim);
+void ff_vp8_h_loop_filter_simple_mmi(uint8_t *dst, ptrdiff_t stride, int flim);
+
 #endif  // #ifndef AVCODEC_MIPS_VP8DSP_MIPS_H
diff --git a/libavcodec/mips/vp8dsp_mmi.c b/libavcodec/mips/vp8dsp_mmi.c
new file mode 100644
index 0000000..e70cddf
--- /dev/null
+++ b/libavcodec/mips/vp8dsp_mmi.c
@@ -0,0 +1,7861 @@
+/*
+ * Loongson SIMD optimized vp8dsp
+ *
+ * Copyright (c) 2015 Loongson Technology Corporation Limited
+ * Copyright (c) 2015 Zhou Xiaoyong <zhouxiaoyong@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vp8dsp_mips.h"
+#include "constants.h"
+#include "libavutil/mips/asmdefs.h"
+
+#define OK      1
+#define NOTOK   1
+
+#define TRANSPOSE_4H(f2, f4, f6, f8, f10, f12, f14, f16, f18, r8, f0, f30) \
+        "li "#r8", 0x93                          \n\t" \
+        "xor "#f0", "#f0","#f0"                  \n\t" \
+        "mtc1 "#r8", "#f30"                      \n\t" \
+        "punpcklhw "#f10", "#f2", "#f0"          \n\t" \
+        "punpcklhw "#f18", "#f4", "#f0"          \n\t" \
+        "pshufh "#f18", "#f18", "#f30"           \n\t" \
+        "or "#f10", "#f10", "#f18"               \n\t" \
+        "punpckhhw "#f12", "#f2", "#f0"          \n\t" \
+        "punpckhhw "#f18", "#f4", "#f0"          \n\t" \
+        "pshufh "#f18", "#f18", "#f30"           \n\t" \
+        "or "#f12", "#f12", "#f18"               \n\t" \
+        "punpcklhw "#f14", "#f6", "#f0"          \n\t" \
+        "punpcklhw "#f18", "#f8", "#f0"          \n\t" \
+        "pshufh "#f18", "#f18", "#f30"           \n\t" \
+        "or "#f14", "#f14", "#f18"               \n\t" \
+        "punpckhhw "#f16", "#f6", "#f0"          \n\t" \
+        "punpckhhw "#f18", "#f8", "#f0"          \n\t" \
+        "pshufh "#f18", "#f18", "#f30"           \n\t" \
+        "or "#f16", "#f16", "#f18"               \n\t" \
+        "punpcklwd "#f2", "#f10", "#f14"         \n\t" \
+        "punpckhwd "#f4", "#f10", "#f14"         \n\t" \
+        "punpcklwd "#f6", "#f12", "#f16"         \n\t" \
+        "punpckhwd "#f8", "#f12", "#f16"         \n\t" \
+
+#define clip_int8(n) (cm[(n) + 0x80] - 0x80)
+static av_always_inline void vp8_filter_common_is4tap(uint8_t *p,
+        ptrdiff_t stride)
+{
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+    int a, f1, f2;
+    const uint8_t *cm = ff_crop_tab + MAX_NEG_CROP;
+
+    a = 3 * (q0 - p0);
+    a += clip_int8(p1 - q1);
+    a = clip_int8(a);
+
+    // We deviate from the spec here with c(a+3) >> 3
+    // since that's what libvpx does.
+    f1 = FFMIN(a + 4, 127) >> 3;
+    f2 = FFMIN(a + 3, 127) >> 3;
+
+    // Despite what the spec says, we do need to clamp here to
+    // be bitexact with libvpx.
+    p[-1 * stride] = cm[p0 + f2];
+    p[ 0 * stride] = cm[q0 - f1];
+}
+
+static av_always_inline void vp8_filter_common_isnot4tap(uint8_t *p,
+        ptrdiff_t stride)
+{
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+    int a, f1, f2;
+    const uint8_t *cm = ff_crop_tab + MAX_NEG_CROP;
+
+    a = 3 * (q0 - p0);
+    a = clip_int8(a);
+
+    // We deviate from the spec here with c(a+3) >> 3
+    // since that's what libvpx does.
+    f1 = FFMIN(a + 4, 127) >> 3;
+    f2 = FFMIN(a + 3, 127) >> 3;
+
+    // Despite what the spec says, we do need to clamp here to
+    // be bitexact with libvpx.
+    p[-1 * stride] = cm[p0 + f2];
+    p[ 0 * stride] = cm[q0 - f1];
+    a              = (f1 + 1) >> 1;
+    p[-2 * stride] = cm[p1 + a];
+    p[ 1 * stride] = cm[q1 - a];
+}
+
+static av_always_inline int vp8_simple_limit(uint8_t *p, ptrdiff_t stride,
+        int flim)
+{
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+
+    return 2 * FFABS(p0 - q0) + (FFABS(p1 - q1) >> 1) <= flim;
+}
+
+static av_always_inline int hev(uint8_t *p, ptrdiff_t stride, int thresh)
+{
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+
+    return FFABS(p1 - p0) > thresh || FFABS(q1 - q0) > thresh;
+}
+
+static av_always_inline void filter_mbedge(uint8_t *p, ptrdiff_t stride)
+{
+    int a0, a1, a2, w;
+    const uint8_t *cm = ff_crop_tab + MAX_NEG_CROP;
+
+    int av_unused p2 = p[-3 * stride];
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+    int av_unused q2 = p[ 2 * stride];
+
+    w = clip_int8(p1 - q1);
+    w = clip_int8(w + 3 * (q0 - p0));
+
+    a0 = (27 * w + 63) >> 7;
+    a1 = (18 * w + 63) >> 7;
+    a2 =  (9 * w + 63) >> 7;
+
+    p[-3 * stride] = cm[p2 + a2];
+    p[-2 * stride] = cm[p1 + a1];
+    p[-1 * stride] = cm[p0 + a0];
+    p[ 0 * stride] = cm[q0 - a0];
+    p[ 1 * stride] = cm[q1 - a1];
+    p[ 2 * stride] = cm[q2 - a2];
+}
+
+static av_always_inline int vp8_normal_limit(uint8_t *p, ptrdiff_t stride,
+        int E, int I)
+{
+    int av_unused p3 = p[-4 * stride];
+    int av_unused p2 = p[-3 * stride];
+    int av_unused p1 = p[-2 * stride];
+    int av_unused p0 = p[-1 * stride];
+    int av_unused q0 = p[ 0 * stride];
+    int av_unused q1 = p[ 1 * stride];
+    int av_unused q2 = p[ 2 * stride];
+    int av_unused q3 = p[ 3 * stride];
+
+    return vp8_simple_limit(p, stride, E) &&
+           FFABS(p3 - p2) <= I && FFABS(p2 - p1) <= I &&
+           FFABS(p1 - p0) <= I && FFABS(q3 - q2) <= I &&
+           FFABS(q2 - q1) <= I && FFABS(q1 - q0) <= I;
+}
+
+static av_always_inline void vp8_v_loop_filter8_mmi(uint8_t *dst,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 8; i++)
+        if (vp8_normal_limit(dst + i * 1, stride, flim_E, flim_I)) {
+            if (hev(dst + i * 1, stride, hev_thresh))
+                vp8_filter_common_is4tap(dst + i * 1, stride);
+            else
+                filter_mbedge(dst + i * 1, stride);
+        }
+}
+
+static av_always_inline void vp8_v_loop_filter8_inner_mmi(uint8_t *dst,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 8; i++)
+        if (vp8_normal_limit(dst + i * 1, stride, flim_E, flim_I)) {
+            int hv = hev(dst + i * 1, stride, hev_thresh);
+            if (hv)
+                vp8_filter_common_is4tap(dst + i * 1, stride);
+            else
+                vp8_filter_common_isnot4tap(dst + i * 1, stride);
+        }
+}
+
+static av_always_inline void vp8_h_loop_filter8_mmi(uint8_t *dst,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 8; i++)
+        if (vp8_normal_limit(dst + i * stride, 1, flim_E, flim_I)) {
+            if (hev(dst + i * stride, 1, hev_thresh))
+                vp8_filter_common_is4tap(dst + i * stride, 1);
+            else
+                filter_mbedge(dst + i * stride, 1);
+        }
+}
+
+static av_always_inline void vp8_h_loop_filter8_inner_mmi(uint8_t *dst,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 8; i++)
+        if (vp8_normal_limit(dst + i * stride, 1, flim_E, flim_I)) {
+            int hv = hev(dst + i * stride, 1, hev_thresh);
+            if (hv)
+                vp8_filter_common_is4tap(dst + i * stride, 1);
+            else
+                vp8_filter_common_isnot4tap(dst + i * stride, 1);
+        }
+}
+
+void ff_vp8_luma_dc_wht_mmi(int16_t block[4][4][16], int16_t dc[16])
+{
+#if OK
+    double ftmp[8];
+    uint64_t all64;
+
+    __asm__ volatile (
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[dc])                         \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[dc])                         \n\t"
+        "gsldlc1    %[ftmp1],   0x0f(%[dc])                         \n\t"
+        "gsldrc1    %[ftmp1],   0x08(%[dc])                         \n\t"
+        "gsldlc1    %[ftmp2],   0x17(%[dc])                         \n\t"
+        "gsldrc1    %[ftmp2],   0x10(%[dc])                         \n\t"
+        "gsldlc1    %[ftmp3],   0x1f(%[dc])                         \n\t"
+        "gsldrc1    %[ftmp3],   0x18(%[dc])                         \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[dc])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                            \n\t"
+        "uld        %[all64],   0x08(%[dc])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+        "uld        %[all64],   0x10(%[dc])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+        "uld        %[all64],   0x18(%[dc])                         \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+#endif
+        "paddsh     %[ftmp4],   %[ftmp0],       %[ftmp3]            \n\t"
+        "psubsh     %[ftmp5],   %[ftmp0],       %[ftmp3]            \n\t"
+        "paddsh     %[ftmp6],   %[ftmp1],       %[ftmp2]            \n\t"
+        "psubsh     %[ftmp7],   %[ftmp1],       %[ftmp2]            \n\t"
+        "paddsh     %[ftmp0],   %[ftmp4],       %[ftmp6]            \n\t"
+        "paddsh     %[ftmp1],   %[ftmp5],       %[ftmp7]            \n\t"
+        "psubsh     %[ftmp2],   %[ftmp4],       %[ftmp6]            \n\t"
+        "psubsh     %[ftmp3],   %[ftmp5],       %[ftmp7]            \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp1],   0x0f(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp1],   0x08(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp2],   0x17(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp2],   0x10(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp3],   0x1f(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp3],   0x18(%[dc])                         \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                            \n\t"
+        "usd        %[all64],   0x00(%[dc])                         \n\t"
+        "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+        "usd        %[all64],   0x08(%[dc])                         \n\t"
+        "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+        "usd        %[all64],   0x10(%[dc])                         \n\t"
+        "dmfc1      %[all64],   %[ftmp3]                            \n\t"
+        "usd        %[all64],   0x18(%[dc])                         \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [all64]"=&r"(all64)
+        : [dc]"r"((uint8_t*)dc)
+        : "memory"
+    );
+
+    block[0][0][0] = (dc[0] + dc[3] + 3 + dc[1] + dc[2]) >> 3;
+    block[0][1][0] = (dc[0] - dc[3] + 3 + dc[1] - dc[2]) >> 3;
+    block[0][2][0] = (dc[0] + dc[3] + 3 - dc[1] - dc[2]) >> 3;
+    block[0][3][0] = (dc[0] - dc[3] + 3 - dc[1] + dc[2]) >> 3;
+
+    block[1][0][0] = (dc[4] + dc[7] + 3 + dc[5] + dc[6]) >> 3;
+    block[1][1][0] = (dc[4] - dc[7] + 3 + dc[5] - dc[6]) >> 3;
+    block[1][2][0] = (dc[4] + dc[7] + 3 - dc[5] - dc[6]) >> 3;
+    block[1][3][0] = (dc[4] - dc[7] + 3 - dc[5] + dc[6]) >> 3;
+
+    block[2][0][0] = (dc[8] + dc[11] + 3 + dc[9] + dc[10]) >> 3;
+    block[2][1][0] = (dc[8] - dc[11] + 3 + dc[9] - dc[10]) >> 3;
+    block[2][2][0] = (dc[8] + dc[11] + 3 - dc[9] - dc[10]) >> 3;
+    block[2][3][0] = (dc[8] - dc[11] + 3 - dc[9] + dc[10]) >> 3;
+
+    block[3][0][0] = (dc[12] + dc[15] + 3 + dc[13] + dc[14]) >> 3;
+    block[3][1][0] = (dc[12] - dc[15] + 3 + dc[13] - dc[14]) >> 3;
+    block[3][2][0] = (dc[12] + dc[15] + 3 - dc[13] - dc[14]) >> 3;
+    block[3][3][0] = (dc[12] - dc[15] + 3 - dc[13] + dc[14]) >> 3;
+
+    __asm__ volatile (
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp0],   0x0f(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp0],   0x08(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp0],   0x17(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp0],   0x10(%[dc])                         \n\t"
+        "gssdlc1    %[ftmp0],   0x1f(%[dc])                         \n\t"
+        "gssdrc1    %[ftmp0],   0x18(%[dc])                         \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                            \n\t"
+        "usd        %[all64],   0x00(%[dc])                         \n\t"
+        "usd        %[all64],   0x08(%[dc])                         \n\t"
+        "usd        %[all64],   0x10(%[dc])                         \n\t"
+        "usd        %[all64],   0x18(%[dc])                         \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [all64]"=&r"(all64)
+        : [dc]"r"((uint8_t *)dc)
+        : "memory"
+    );
+#else
+    int t00, t01, t02, t03, t10, t11, t12, t13, t20, t21, t22, t23, t30, t31, t32, t33;
+
+    t00 = dc[0] + dc[12];
+    t10 = dc[1] + dc[13];
+    t20 = dc[2] + dc[14];
+    t30 = dc[3] + dc[15];
+
+    t03 = dc[0] - dc[12];
+    t13 = dc[1] - dc[13];
+    t23 = dc[2] - dc[14];
+    t33 = dc[3] - dc[15];
+
+    t01 = dc[4] + dc[ 8];
+    t11 = dc[5] + dc[ 9];
+    t21 = dc[6] + dc[10];
+    t31 = dc[7] + dc[11];
+
+    t02 = dc[4] - dc[ 8];
+    t12 = dc[5] - dc[ 9];
+    t22 = dc[6] - dc[10];
+    t32 = dc[7] - dc[11];
+
+    dc[ 0] = t00 + t01;
+    dc[ 1] = t10 + t11;
+    dc[ 2] = t20 + t21;
+    dc[ 3] = t30 + t31;
+
+    dc[ 4] = t03 + t02;
+    dc[ 5] = t13 + t12;
+    dc[ 6] = t23 + t22;
+    dc[ 7] = t33 + t32;
+
+    dc[ 8] = t00 - t01;
+    dc[ 9] = t10 - t11;
+    dc[10] = t20 - t21;
+    dc[11] = t30 - t31;
+
+    dc[12] = t03 - t02;
+    dc[13] = t13 - t12;
+    dc[14] = t23 - t22;
+    dc[15] = t33 - t32;
+
+    block[0][0][0] = (dc[0] + dc[3] + 3 + dc[1] + dc[2]) >> 3;
+    block[0][1][0] = (dc[0] - dc[3] + 3 + dc[1] - dc[2]) >> 3;
+    block[0][2][0] = (dc[0] + dc[3] + 3 - dc[1] - dc[2]) >> 3;
+    block[0][3][0] = (dc[0] - dc[3] + 3 - dc[1] + dc[2]) >> 3;
+
+    block[1][0][0] = (dc[4] + dc[7] + 3 + dc[5] + dc[6]) >> 3;
+    block[1][1][0] = (dc[4] - dc[7] + 3 + dc[5] - dc[6]) >> 3;
+    block[1][2][0] = (dc[4] + dc[7] + 3 - dc[5] - dc[6]) >> 3;
+    block[1][3][0] = (dc[4] - dc[7] + 3 - dc[5] + dc[6]) >> 3;
+
+    block[2][0][0] = (dc[8] + dc[11] + 3 + dc[9] + dc[10]) >> 3;
+    block[2][1][0] = (dc[8] - dc[11] + 3 + dc[9] - dc[10]) >> 3;
+    block[2][2][0] = (dc[8] + dc[11] + 3 - dc[9] - dc[10]) >> 3;
+    block[2][3][0] = (dc[8] - dc[11] + 3 - dc[9] + dc[10]) >> 3;
+
+    block[3][0][0] = (dc[12] + dc[15] + 3 + dc[13] + dc[14]) >> 3;
+    block[3][1][0] = (dc[12] - dc[15] + 3 + dc[13] - dc[14]) >> 3;
+    block[3][2][0] = (dc[12] + dc[15] + 3 - dc[13] - dc[14]) >> 3;
+    block[3][3][0] = (dc[12] - dc[15] + 3 - dc[13] + dc[14]) >> 3;
+
+    AV_ZERO64(dc + 0);
+    AV_ZERO64(dc + 4);
+    AV_ZERO64(dc + 8);
+    AV_ZERO64(dc + 12);
+#endif
+}
+
+void ff_vp8_luma_dc_wht_dc_mmi(int16_t block[4][4][16], int16_t dc[16])
+{
+    int val = (dc[0] + 3) >> 3;
+
+    dc[0] = 0;
+
+    block[0][0][0] = val;
+    block[0][1][0] = val;
+    block[0][2][0] = val;
+    block[0][3][0] = val;
+    block[1][0][0] = val;
+    block[1][1][0] = val;
+    block[1][2][0] = val;
+    block[1][3][0] = val;
+    block[2][0][0] = val;
+    block[2][1][0] = val;
+    block[2][2][0] = val;
+    block[2][3][0] = val;
+    block[3][0][0] = val;
+    block[3][1][0] = val;
+    block[3][2][0] = val;
+    block[3][3][0] = val;
+}
+
+#define MUL_20091(a) ((((a) * 20091) >> 16) + (a))
+#define MUL_35468(a)  (((a) * 35468) >> 16)
+void ff_vp8_idct_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-xxx
+    DECLARE_ALIGNED(8, const uint64_t, ff_ph_20091) = {0x4e7b4e7b4e7b4e7bULL};
+    DECLARE_ALIGNED(8, const uint64_t, ff_ph_35468) = {0x8a8c8a8c8a8c8a8cULL};
+    double ftmp[15];
+    uint64_t tmp[1];
+    int low32;
+    uint64_t all64;
+
+    __asm__ volatile (
+        /*
+        t0  = block[0] + block[ 8];
+        t4  = block[1] + block[ 9];
+        t8  = block[2] + block[10];
+        t12 = block[3] + block[11];
+
+        t1  = block[0] - block[ 8];
+        t5  = block[1] - block[ 9];
+        t9  = block[2] - block[10];
+        t13 = block[3] - block[11];
+
+        t2  = MUL_35468(block[4]) - MUL_20091(block[12]);
+        t6  = MUL_35468(block[5]) - MUL_20091(block[13]);
+        t10 = MUL_35468(block[6]) - MUL_20091(block[14]);
+        t14 = MUL_35468(block[7]) - MUL_20091(block[15]);
+
+        t3  = MUL_20091(block[4]) + MUL_35468(block[12]);
+        t7  = MUL_20091(block[5]) + MUL_35468(block[13]);
+        t11 = MUL_20091(block[6]) + MUL_35468(block[14]);
+        t15 = MUL_20091(block[7]) + MUL_35468(block[15]);
+
+        block[ 0] = 0;
+        block[ 4] = 0;
+        block[ 8] = 0;
+        block[12] = 0;
+        block[ 1] = 0;
+        block[ 5] = 0;
+        block[ 9] = 0;
+        block[13] = 0;
+        block[ 2] = 0;
+        block[ 6] = 0;
+        block[10] = 0;
+        block[14] = 0;
+        block[ 3] = 0;
+        block[ 7] = 0;
+        block[11] = 0;
+        block[15] = 0;
+
+        tmp[ 0] = t0  + t3;
+        tmp[ 4] = t4  + t7;
+        tmp[ 8] = t8  + t11;
+        tmp[12] = t12 + t15;
+
+        tmp[ 1] = t1  + t2;
+        tmp[ 5] = t5  + t6;
+        tmp[ 9] = t9  + t10;
+        tmp[13] = t13 + t14;
+
+        tmp[ 2] = t1  - t2;
+        tmp[ 6] = t5  - t6;
+        tmp[10] = t9  - t10;
+        tmp[14] = t13 - t14;
+
+        tmp[ 3] = t0  - t3;
+        tmp[ 7] = t4  - t7;
+        tmp[11] = t8  - t11;
+        tmp[15] = t12 - t15;
+        */
+
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "dmtc1      %[ff_ph_35468],             %[ftmp13]               \n\t"
+        "dmtc1      %[ff_ph_20091],             %[ftmp14]               \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[block])                          \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[block])                          \n\t"
+        "gsldlc1    %[ftmp2],   0x0f(%[block])                          \n\t"
+        "gsldrc1    %[ftmp2],   0x08(%[block])                          \n\t"
+        "gsldlc1    %[ftmp3],   0x17(%[block])                          \n\t"
+        "gsldrc1    %[ftmp3],   0x10(%[block])                          \n\t"
+        "gsldlc1    %[ftmp4],   0x1f(%[block])                          \n\t"
+        "gsldrc1    %[ftmp4],   0x18(%[block])                          \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                                \n\t"
+        "uld        %[all64],   0x08(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp2]                                \n\t"
+        "uld        %[all64],   0x10(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp3]                                \n\t"
+        "uld        %[all64],   0x18(%[block])                          \n\t"
+        "dmtc1      %[all64],   %[ftmp4]                                \n\t"
+#endif
+
+        "pmulhh     %[ftmp9],   %[ftmp2],       %[ftmp13]               \n\t"
+        "pmulhh     %[ftmp10],  %[ftmp4],       %[ftmp14]               \n\t"
+        "paddh      %[ftmp10],  %[ftmp10],      %[ftmp4]                \n\t"
+        "pmulhh     %[ftmp11],  %[ftmp2],       %[ftmp14]               \n\t"
+        "paddh      %[ftmp11],  %[ftmp11],      %[ftmp2]                \n\t"
+        "pmulhh     %[ftmp12],  %[ftmp4],       %[ftmp13]               \n\t"
+
+        "paddh      %[ftmp5],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp9],       %[ftmp10]               \n\t"
+        "paddh      %[ftmp8],   %[ftmp11],      %[ftmp12]               \n\t"
+
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[block])                          \n\t"
+        "gssdlc1    %[ftmp0],   0x0f(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x08(%[block])                          \n\t"
+        "gssdlc1    %[ftmp0],   0x17(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x10(%[block])                          \n\t"
+        "gssdlc1    %[ftmp0],   0x1f(%[block])                          \n\t"
+        "gssdrc1    %[ftmp0],   0x18(%[block])                          \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                                \n\t"
+        "usd        %[all64],   0x00(%[block])                          \n\t"
+        "usd        %[all64],   0x08(%[block])                          \n\t"
+        "usd        %[all64],   0x10(%[block])                          \n\t"
+        "usd        %[all64],   0x18(%[block])                          \n\t"
+#endif
+
+        "paddh     %[ftmp1],   %[ftmp5],       %[ftmp8]                 \n\t"
+        "paddh     %[ftmp2],   %[ftmp6],       %[ftmp7]                 \n\t"
+        "psubh     %[ftmp3],   %[ftmp6],       %[ftmp7]                 \n\t"
+        "psubh     %[ftmp4],   %[ftmp5],       %[ftmp8]                 \n\t"
+
+        TRANSPOSE_4H(%[ftmp1], %[ftmp2], %[ftmp3], %[ftmp4],
+                     %[ftmp5], %[ftmp6], %[ftmp7], %[ftmp8],
+                     %[ftmp9], %[tmp0],  %[ftmp0], %[ftmp14])
+        /*
+        t0  = tmp[0] + tmp[ 8];
+        t4  = tmp[1] + tmp[ 9];
+        t8  = tmp[2] + tmp[10];
+        t12 = tmp[3] + tmp[11];
+
+        t1  = tmp[0] - tmp[ 8];
+        t5  = tmp[1] - tmp[ 9];
+        t9  = tmp[2] - tmp[10];
+        t13 = tmp[3] - tmp[11];
+
+        t2  = MUL_35468(tmp[4]) - MUL_20091(tmp[12]);
+        t6  = MUL_35468(tmp[5]) - MUL_20091(tmp[13]);
+        t10 = MUL_35468(tmp[6]) - MUL_20091(tmp[14]);
+        t14 = MUL_35468(tmp[7]) - MUL_20091(tmp[15]);
+
+        t3  = MUL_20091(tmp[4]) + MUL_35468(tmp[12]);
+        t7  = MUL_20091(tmp[5]) + MUL_35468(tmp[13]);
+        t11 = MUL_20091(tmp[6]) + MUL_35468(tmp[14]);
+        t15 = MUL_20091(tmp[7]) + MUL_35468(tmp[15]);
+
+        dst[0]          = av_clip_uint8(dst[0] +          ((t0  + t3  + 4) >> 3));
+        dst[0+1*stride] = av_clip_uint8(dst[0+1*stride] + ((t4  + t7  + 4) >> 3));
+        dst[0+2*stride] = av_clip_uint8(dst[0+2*stride] + ((t8  + t11 + 4) >> 3));
+        dst[0+3*stride] = av_clip_uint8(dst[0+3*stride] + ((t12 + t15 + 4) >> 3));
+
+        dst[1]          = av_clip_uint8(dst[1] +          ((t1  + t2  + 4) >> 3));
+        dst[1+1*stride] = av_clip_uint8(dst[1+1*stride] + ((t5  + t6  + 4) >> 3));
+        dst[1+2*stride] = av_clip_uint8(dst[1+2*stride] + ((t9  + t10 + 4) >> 3));
+        dst[1+3*stride] = av_clip_uint8(dst[1+3*stride] + ((t13 + t14 + 4) >> 3));
+
+        dst[2]          = av_clip_uint8(dst[2] +          ((t1  - t2  + 4) >> 3));
+        dst[2+1*stride] = av_clip_uint8(dst[2+1*stride] + ((t5  - t6  + 4) >> 3));
+        dst[2+2*stride] = av_clip_uint8(dst[2+2*stride] + ((t9  - t10 + 4) >> 3));
+        dst[2+3*stride] = av_clip_uint8(dst[2+3*stride] + ((t13 - t14 + 4) >> 3));
+
+        dst[3]          = av_clip_uint8(dst[3] +          ((t0  - t3  + 4) >> 3));
+        dst[3+1*stride] = av_clip_uint8(dst[3+1*stride] + ((t4  - t7  + 4) >> 3));
+        dst[3+2*stride] = av_clip_uint8(dst[3+2*stride] + ((t8  - t11 + 4) >> 3));
+        dst[3+3*stride] = av_clip_uint8(dst[3+3*stride] + ((t12 - t15 + 4) >> 3));
+        */
+
+        "li         %[tmp0],    0x03                                    \n\t"
+        "dmtc1      %[ff_ph_35468],             %[ftmp0]                \n\t"
+        "dmtc1      %[ff_ph_20091],             %[ftmp14]               \n\t"
+        "paddh      %[ftmp5],   %[ftmp1],       %[ftmp3]                \n\t"
+        "psubh      %[ftmp6],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pmulhh     %[ftmp7],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pmulhh     %[ftmp8],   %[ftmp4],       %[ftmp14]               \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp4]                \n\t"
+        "psubh      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
+        "pmulhh     %[ftmp8],   %[ftmp2],       %[ftmp14]               \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
+        "pmulhh     %[ftmp9],   %[ftmp4],       %[ftmp0]                \n\t"
+        "paddh      %[ftmp8],   %[ftmp8],       %[ftmp9]                \n\t"
+
+        "mtc1       %[tmp0],    %[ftmp0]                                \n\t"
+        "paddh      %[ftmp10],  %[ftmp5],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp10],  %[ftmp10],      %[ff_pw_4]              \n\t"
+        "psrah      %[ftmp10],  %[ftmp10],      %[ftmp0]                \n\t"
+
+        "paddh      %[ftmp11],  %[ftmp6],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp11],  %[ftmp11],      %[ff_pw_4]              \n\t"
+        "psrah      %[ftmp11],  %[ftmp11],      %[ftmp0]                \n\t"
+
+        "psubh      %[ftmp12],  %[ftmp6],       %[ftmp7]                \n\t"
+        "paddh      %[ftmp12],  %[ftmp12],      %[ff_pw_4]              \n\t"
+        "psrah      %[ftmp12],  %[ftmp12],      %[ftmp0]                \n\t"
+
+        "psubh      %[ftmp13],  %[ftmp5],       %[ftmp8]                \n\t"
+        "paddh      %[ftmp13],  %[ftmp13],      %[ff_pw_4]              \n\t"
+        "psrah      %[ftmp13],  %[ftmp13],      %[ftmp0]                \n\t"
+
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "ulw        %[low32],   0x00(%[dst0])                           \n\t"
+        "mtc1       %[low32],   %[ftmp1]                                \n\t"
+        "ulw        %[low32],   0x00(%[dst1])                           \n\t"
+        "mtc1       %[low32],   %[ftmp2]                                \n\t"
+        "ulw        %[low32],   0x00(%[dst2])                           \n\t"
+        "mtc1       %[low32],   %[ftmp3]                                \n\t"
+        "ulw        %[low32],   0x00(%[dst3])                           \n\t"
+        "mtc1       %[low32],   %[ftmp4]                                \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+
+        TRANSPOSE_4H(%[ftmp1], %[ftmp2], %[ftmp3], %[ftmp4],
+                     %[ftmp5], %[ftmp6], %[ftmp7], %[ftmp8],
+                     %[ftmp9], %[tmp0],  %[ftmp0], %[ftmp14])
+
+        "paddh      %[ftmp1],   %[ftmp1],       %[ftmp10]               \n\t"
+        "paddh      %[ftmp2],   %[ftmp2],       %[ftmp11]               \n\t"
+        "paddh      %[ftmp3],   %[ftmp3],       %[ftmp12]               \n\t"
+        "paddh      %[ftmp4],   %[ftmp4],       %[ftmp13]               \n\t"
+
+        TRANSPOSE_4H(%[ftmp1], %[ftmp2], %[ftmp3], %[ftmp4],
+                     %[ftmp5], %[ftmp6], %[ftmp7], %[ftmp8],
+                     %[ftmp9], %[tmp0],  %[ftmp0], %[ftmp14])
+
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp0]                \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[dst0])                           \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[dst0])                           \n\t"
+        "gsswlc1    %[ftmp2],   0x03(%[dst1])                           \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst1])                           \n\t"
+        "gsswlc1    %[ftmp3],   0x03(%[dst2])                           \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst2])                           \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[dst3])                           \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[dst3])                           \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                                \n\t"
+        "usw        %[low32],   0x00(%[dst0])                           \n\t"
+        "mfc1       %[low32],   %[ftmp2]                                \n\t"
+        "usw        %[low32],   0x00(%[dst1])                           \n\t"
+        "mfc1       %[low32],   %[ftmp3]                                \n\t"
+        "usw        %[low32],   0x00(%[dst2])                           \n\t"
+        "mfc1       %[low32],   %[ftmp4]                                \n\t"
+        "usw        %[low32],   0x00(%[dst3])                           \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+          [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+          [ftmp10]"=&f"(ftmp[10]),          [ftmp11]"=&f"(ftmp[11]),
+          [ftmp12]"=&f"(ftmp[12]),          [ftmp13]"=&f"(ftmp[13]),
+          [ftmp14]"=&f"(ftmp[14]),
+          [tmp0]"=&r"(tmp[0]),
+          [low32]"=&r"(low32),              [all64]"=&r"(all64)
+        : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
+          [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
+          [block]"r"(block),                [ff_pw_4]"f"(ff_pw_4),
+          [ff_ph_20091]"r"(ff_ph_20091),    [ff_ph_35468]"r"(ff_ph_35468)
+        : "memory"
+    );
+#else
+    int i, t0, t1, t2, t3;
+    int16_t tmp[16];
+
+    for (i = 0; i < 4; i++) {
+        t0 = block[0 + i] + block[8 + i];
+        t1 = block[0 + i] - block[8 + i];
+        t2 = MUL_35468(block[4 + i]) - MUL_20091(block[12 + i]);
+        t3 = MUL_20091(block[4 + i]) + MUL_35468(block[12 + i]);
+        block[ 0 + i] = 0;
+        block[ 4 + i] = 0;
+        block[ 8 + i] = 0;
+        block[12 + i] = 0;
+
+        tmp[i * 4 + 0] = t0 + t3;
+        tmp[i * 4 + 1] = t1 + t2;
+        tmp[i * 4 + 2] = t1 - t2;
+        tmp[i * 4 + 3] = t0 - t3;
+    }
+
+    for (i = 0; i < 4; i++) {
+        t0 = tmp[0 + i] + tmp[8 + i];
+        t1 = tmp[0 + i] - tmp[8 + i];
+        t2 = MUL_35468(tmp[4 + i]) - MUL_20091(tmp[12 + i]);
+        t3 = MUL_20091(tmp[4 + i]) + MUL_35468(tmp[12 + i]);
+
+        dst[0] = av_clip_uint8(dst[0] + ((t0 + t3 + 4) >> 3));
+        dst[1] = av_clip_uint8(dst[1] + ((t1 + t2 + 4) >> 3));
+        dst[2] = av_clip_uint8(dst[2] + ((t1 - t2 + 4) >> 3));
+        dst[3] = av_clip_uint8(dst[3] + ((t0 - t3 + 4) >> 3));
+        dst   += stride;
+    }
+#endif
+}
+
+void ff_vp8_idct_dc_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride)
+{
+#if OK
+    int dc = (block[0] + 4) >> 3;
+    double ftmp[6];
+    int low32;
+
+    block[0] = 0;
+
+    __asm__ volatile (
+        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]             \n\t"
+        "mtc1       %[dc],      %[ftmp5]                             \n\t"
+        "ulw        %[low32],   0x00(%[dst0])                        \n\t"
+        "mtc1       %[low32],   %[ftmp1]                             \n\t"
+        "ulw        %[low32],   0x00(%[dst1])                        \n\t"
+        "mtc1       %[low32],   %[ftmp2]                             \n\t"
+        "ulw        %[low32],   0x00(%[dst2])                        \n\t"
+        "mtc1       %[low32],   %[ftmp3]                             \n\t"
+        "ulw        %[low32],   0x00(%[dst3])                        \n\t"
+        "mtc1       %[low32],   %[ftmp4]                             \n\t"
+        "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]             \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]             \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]             \n\t"
+        "punpcklbh  %[ftmp3],   %[ftmp3],       %[ftmp0]             \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],       %[ftmp0]             \n\t"
+        "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp5]             \n\t"
+        "paddsh     %[ftmp2],   %[ftmp2],       %[ftmp5]             \n\t"
+        "paddsh     %[ftmp3],   %[ftmp3],       %[ftmp5]             \n\t"
+        "paddsh     %[ftmp4],   %[ftmp4],       %[ftmp5]             \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]             \n\t"
+        "packushb   %[ftmp2],   %[ftmp2],       %[ftmp0]             \n\t"
+        "packushb   %[ftmp3],   %[ftmp3],       %[ftmp0]             \n\t"
+        "packushb   %[ftmp4],   %[ftmp4],       %[ftmp0]             \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp1],   0x03(%[dst0])                        \n\t"
+        "gsswrc1    %[ftmp1],   0x00(%[dst0])                        \n\t"
+        "gsswlc1    %[ftmp2],   0x03(%[dst1])                        \n\t"
+        "gsswrc1    %[ftmp2],   0x00(%[dst1])                        \n\t"
+        "gsswlc1    %[ftmp3],   0x03(%[dst2])                        \n\t"
+        "gsswrc1    %[ftmp3],   0x00(%[dst2])                        \n\t"
+        "gsswlc1    %[ftmp4],   0x03(%[dst3])                        \n\t"
+        "gsswrc1    %[ftmp4],   0x00(%[dst3])                        \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp1]                             \n\t"
+        "usw        %[low32],   0x00(%[dst0])                        \n\t"
+        "mfc1       %[low32],   %[ftmp2]                             \n\t"
+        "usw        %[low32],   0x00(%[dst1])                        \n\t"
+        "mfc1       %[low32],   %[ftmp3]                             \n\t"
+        "usw        %[low32],   0x00(%[dst2])                        \n\t"
+        "mfc1       %[low32],   %[ftmp4]                             \n\t"
+        "usw        %[low32],   0x00(%[dst3])                        \n\t"
+#endif
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+          [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+          [low32]"=&r"(low32)
+        : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
+          [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
+          [dc]"r"(dc)
+        : "memory"
+    );
+#else
+    int i, dc = (block[0] + 4) >> 3;
+
+    block[0] = 0;
+
+    for (i = 0; i < 4; i++) {
+        dst[0] = av_clip_uint8(dst[0] + dc);
+        dst[1] = av_clip_uint8(dst[1] + dc);
+        dst[2] = av_clip_uint8(dst[2] + dc);
+        dst[3] = av_clip_uint8(dst[3] + dc);
+        dst   += stride;
+    }
+#endif
+}
+
+void ff_vp8_idct_dc_add4y_mmi(uint8_t *dst, int16_t block[4][16],
+        ptrdiff_t stride)
+{
+    ff_vp8_idct_dc_add_mmi(dst +  0, block[0], stride);
+    ff_vp8_idct_dc_add_mmi(dst +  4, block[1], stride);
+    ff_vp8_idct_dc_add_mmi(dst +  8, block[2], stride);
+    ff_vp8_idct_dc_add_mmi(dst + 12, block[3], stride);
+}
+
+void ff_vp8_idct_dc_add4uv_mmi(uint8_t *dst, int16_t block[4][16],
+        ptrdiff_t stride)
+{
+    ff_vp8_idct_dc_add_mmi(dst + stride * 0 + 0, block[0], stride);
+    ff_vp8_idct_dc_add_mmi(dst + stride * 0 + 4, block[1], stride);
+    ff_vp8_idct_dc_add_mmi(dst + stride * 4 + 0, block[2], stride);
+    ff_vp8_idct_dc_add_mmi(dst + stride * 4 + 4, block[3], stride);
+}
+
+// loop filter applied to edges between macroblocks
+void ff_vp8_v_loop_filter16_mmi(uint8_t *dst, ptrdiff_t stride, int flim_E,
+        int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_normal_limit(dst + i * 1, stride, flim_E, flim_I)) {
+            if (hev(dst + i * 1, stride, hev_thresh))
+                vp8_filter_common_is4tap(dst + i * 1, stride);
+            else
+                filter_mbedge(dst + i * 1, stride);
+        }
+}
+
+void ff_vp8_h_loop_filter16_mmi(uint8_t *dst, ptrdiff_t stride, int flim_E,
+        int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_normal_limit(dst + i * stride, 1, flim_E, flim_I)) {
+            if (hev(dst + i * stride, 1, hev_thresh))
+                vp8_filter_common_is4tap(dst + i * stride, 1);
+            else
+                filter_mbedge(dst + i * stride, 1);
+        }
+}
+
+void ff_vp8_v_loop_filter8uv_mmi(uint8_t *dstU, uint8_t *dstV, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh)
+{
+    vp8_v_loop_filter8_mmi(dstU, stride, flim_E, flim_I, hev_thresh);
+    vp8_v_loop_filter8_mmi(dstV, stride, flim_E, flim_I, hev_thresh);
+}
+
+void ff_vp8_h_loop_filter8uv_mmi(uint8_t *dstU, uint8_t *dstV, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh)
+{
+    vp8_h_loop_filter8_mmi(dstU, stride, flim_E, flim_I, hev_thresh);
+    vp8_h_loop_filter8_mmi(dstV, stride, flim_E, flim_I, hev_thresh);
+}
+
+// loop filter applied to inner macroblock edges
+void ff_vp8_v_loop_filter16_inner_mmi(uint8_t *dst, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_normal_limit(dst + i * 1, stride, flim_E, flim_I)) {
+            int hv = hev(dst + i * 1, stride, hev_thresh);
+            if (hv)
+                vp8_filter_common_is4tap(dst + i * 1, stride);
+            else
+                vp8_filter_common_isnot4tap(dst + i * 1, stride);
+        }
+}
+
+void ff_vp8_h_loop_filter16_inner_mmi(uint8_t *dst, ptrdiff_t stride,
+        int flim_E, int flim_I, int hev_thresh)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_normal_limit(dst + i * stride, 1, flim_E, flim_I)) {
+            int hv = hev(dst + i * stride, 1, hev_thresh);
+            if (hv)
+                vp8_filter_common_is4tap(dst + i * stride, 1);
+            else
+                vp8_filter_common_isnot4tap(dst + i * stride, 1);
+        }
+}
+
+void ff_vp8_v_loop_filter8uv_inner_mmi(uint8_t *dstU, uint8_t *dstV,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    vp8_v_loop_filter8_inner_mmi(dstU, stride, flim_E, flim_I, hev_thresh);
+    vp8_v_loop_filter8_inner_mmi(dstV, stride, flim_E, flim_I, hev_thresh);
+}
+
+void ff_vp8_h_loop_filter8uv_inner_mmi(uint8_t *dstU, uint8_t *dstV,
+        ptrdiff_t stride, int flim_E, int flim_I, int hev_thresh)
+{
+    vp8_h_loop_filter8_inner_mmi(dstU, stride, flim_E, flim_I, hev_thresh);
+    vp8_h_loop_filter8_inner_mmi(dstV, stride, flim_E, flim_I, hev_thresh);
+}
+
+void ff_vp8_v_loop_filter_simple_mmi(uint8_t *dst, ptrdiff_t stride, int flim)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_simple_limit(dst + i, stride, flim))
+            vp8_filter_common_is4tap(dst + i, stride);
+}
+
+void ff_vp8_h_loop_filter_simple_mmi(uint8_t *dst, ptrdiff_t stride, int flim)
+{
+    int i;
+
+    for (i = 0; i < 16; i++)
+        if (vp8_simple_limit(dst + i * stride, 1, flim))
+            vp8_filter_common_is4tap(dst + i * stride, 1);
+}
+
+void ff_put_vp8_pixels16_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y)
+{
+#if OK
+    double ftmp[2];
+    uint64_t tmp[2];
+    mips_reg addr[2];
+    uint64_t all64;
+
+    __asm__ volatile (
+        "1:                                                         \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[srcstride]        \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src])                        \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                        \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                            \n\t"
+#endif
+        "ldl        %[tmp0],    0x0f(%[src])                        \n\t"
+        "ldr        %[tmp0],    0x08(%[src])                        \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp1],   0x07(%[addr0])                      \n\t"
+        "gsldrc1    %[ftmp1],   0x00(%[addr0])                      \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[addr0])                      \n\t"
+        "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+        "ldl        %[tmp1],    0x0f(%[addr0])                      \n\t"
+        "ldr        %[tmp1],    0x08(%[addr0])                      \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[dststride]        \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[dst])                        \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                            \n\t"
+        "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+        "sdl        %[tmp0],    0x0f(%[dst])                        \n\t"
+        "sdr        %[tmp0],    0x08(%[dst])                        \n\t"
+        "daddi      %[h],       %[h],           -0x02               \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp1],   0x07(%[addr1])                      \n\t"
+        "gssdrc1    %[ftmp1],   0x00(%[addr1])                      \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+        "usd        %[all64],   0x00(%[addr1])                      \n\t"
+#endif
+        PTR_ADDU   "%[src],     %[addr0],       %[srcstride]        \n\t"
+        "sdl        %[tmp1],    0x0f(%[addr1])                      \n\t"
+        "sdr        %[tmp1],    0x08(%[addr1])                      \n\t"
+        PTR_ADDU   "%[dst],     %[addr1],       %[dststride]        \n\t"
+        "bnez       %[h],       1b                                  \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+          [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [h]"+&r"(h)
+        : [dststride]"r"((mips_reg)dststride),
+          [srcstride]"r"((mips_reg)srcstride)
+        : "memory"
+    );
+#else
+    int i;
+
+    for (i = 0; i < h; i++, dst += dststride, src += srcstride)
+        memcpy(dst, src, 16);
+#endif
+}
+
+void ff_put_vp8_pixels8_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y)
+{
+#if OK
+    double ftmp[1];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    uint64_t all64;
+
+    __asm__ volatile (
+        "1:                                                         \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[srcstride]        \n\t"
+#if HAVE_LOONGSON3
+        "gsldlc1    %[ftmp0],   0x07(%[src])                        \n\t"
+        "gsldrc1    %[ftmp0],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+        "uld        %[all64],   0x00(%[src])                        \n\t"
+        "dmtc1      %[all64],   %[ftmp0]                            \n\t"
+#endif
+        "ldl        %[tmp0],    0x07(%[addr0])                      \n\t"
+        "ldr        %[tmp0],    0x00(%[addr0])                      \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[dststride]        \n\t"
+#if HAVE_LOONGSON3
+        "gssdlc1    %[ftmp0],   0x07(%[dst])                        \n\t"
+        "gssdrc1    %[ftmp0],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+        "dmfc1      %[all64],   %[ftmp0]                            \n\t"
+        "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+        "daddi      %[h],       %[h],           -0x02               \n\t"
+        "sdl        %[tmp0],    0x07(%[addr1])                      \n\t"
+        "sdr        %[tmp0],    0x00(%[addr1])                      \n\t"
+        PTR_ADDU   "%[src],     %[addr0],       %[srcstride]        \n\t"
+        PTR_ADDU   "%[dst],     %[addr1],       %[dststride]        \n\t"
+        "bnez       %[h],       1b                                  \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [all64]"=&r"(all64),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [h]"+&r"(h)
+        : [dststride]"r"((mips_reg)dststride),
+          [srcstride]"r"((mips_reg)srcstride)
+        : "memory"
+    );
+#else
+    int i;
+
+    for (i = 0; i < h; i++, dst += dststride, src += srcstride)
+        memcpy(dst, src, 8);
+#endif
+}
+
+void ff_put_vp8_pixels4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int x, int y)
+{
+#if OK
+    double ftmp[1];
+    uint64_t tmp[1];
+    mips_reg addr[2];
+    int low32;
+
+    __asm__ volatile (
+        "1:                                                         \n\t"
+        PTR_ADDU   "%[addr0],   %[src],         %[srcstride]        \n\t"
+        "ulw        %[low32],   0x00(%[src])                        \n\t"
+        "mtc1       %[low32],   %[ftmp0]                            \n\t"
+        "lwl        %[tmp0],    0x03(%[addr0])                      \n\t"
+        "lwr        %[tmp0],    0x00(%[addr0])                      \n\t"
+        PTR_ADDU   "%[addr1],   %[dst],         %[dststride]        \n\t"
+#if HAVE_LOONGSON3
+        "gsswlc1    %[ftmp0],   0x03(%[dst])                        \n\t"
+        "gsswrc1    %[ftmp0],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+        "mfc1       %[low32],   %[ftmp0]                            \n\t"
+        "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+        "daddi      %[h],       %[h],           -0x02               \n\t"
+        "swl        %[tmp0],    0x03(%[addr1])                      \n\t"
+        "swr        %[tmp0],    0x00(%[addr1])                      \n\t"
+        PTR_ADDU   "%[src],     %[addr0],       %[srcstride]        \n\t"
+        PTR_ADDU   "%[dst],     %[addr1],       %[dststride]        \n\t"
+        "bnez       %[h],       1b                                  \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),            [tmp0]"=&r"(tmp[0]),
+          [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1]),
+          [dst]"+&r"(dst),                  [src]"+&r"(src),
+          [h]"+&r"(h),
+          [low32]"=&r"(low32)
+        : [dststride]"r"((mips_reg)dststride),
+          [srcstride]"r"((mips_reg)srcstride)
+        : "memory"
+    );
+#else
+    int i;
+
+    for (i = 0; i < h; i++, dst += dststride, src += srcstride)
+        memcpy(dst, src, 4);
+#endif
+}
+
+static const uint8_t subpel_filters[7][6] = {
+    { 0,  6, 123,  12,  1, 0 },
+    { 2, 11, 108,  36,  8, 1 },
+    { 0,  9,  93,  50,  6, 0 },
+    { 3, 16,  77,  77, 16, 3 },
+    { 0,  6,  50,  93,  9, 0 },
+    { 1,  8,  36, 108, 11, 2 },
+    { 0,  1,  12, 123,  6, 0 },
+};
+
+#define FILTER_6TAP(src, F, stride)                                           \
+    cm[(F[2] * src[x + 0 * stride] - F[1] * src[x - 1 * stride] +             \
+        F[0] * src[x - 2 * stride] + F[3] * src[x + 1 * stride] -             \
+        F[4] * src[x + 2 * stride] + F[5] * src[x + 3 * stride] + 64) >> 7]
+
+#define FILTER_4TAP(src, F, stride)                                           \
+    cm[(F[2] * src[x + 0 * stride] - F[1] * src[x - 1 * stride] +             \
+        F[3] * src[x + 1 * stride] - F[4] * src[x + 2 * stride] + 64) >> 7]
+
+void ff_put_vp8_epel16_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if OK
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[10];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        dst[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        dst[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        dst[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        dst[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2] * src[ 8] - filter[1] * src[ 7] + filter[3] * src[ 9] - filter[4] * src[10] + 64) >> 7];
+        dst[ 9] = cm[(filter[2] * src[ 9] - filter[1] * src[ 8] + filter[3] * src[10] - filter[4] * src[11] + 64) >> 7];
+        dst[10] = cm[(filter[2] * src[10] - filter[1] * src[ 9] + filter[3] * src[11] - filter[4] * src[12] + 64) >> 7];
+        dst[11] = cm[(filter[2] * src[11] - filter[1] * src[10] + filter[3] * src[12] - filter[4] * src[13] + 64) >> 7];
+        dst[12] = cm[(filter[2] * src[12] - filter[1] * src[11] + filter[3] * src[13] - filter[4] * src[14] + 64) >> 7];
+        dst[13] = cm[(filter[2] * src[13] - filter[1] * src[12] + filter[3] * src[14] - filter[4] * src[15] + 64) >> 7];
+        dst[14] = cm[(filter[2] * src[14] - filter[1] * src[13] + filter[3] * src[15] - filter[4] * src[16] + 64) >> 7];
+        dst[15] = cm[(filter[2] * src[15] - filter[1] * src[14] + filter[3] * src[16] - filter[4] * src[17] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0c(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_4TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[7];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        dst[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        dst[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        dst[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        dst[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_4TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[5];
+    uint64_t tmp[1];
+    int low32;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp[0]),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_4TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[10];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        dst[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        dst[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        dst[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        dst[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2]*src[ 8] - filter[1]*src[ 7] + filter[0]*src[ 6] + filter[3]*src[ 9] - filter[4]*src[10] + filter[5]*src[11] + 64) >> 7];
+        dst[ 9] = cm[(filter[2]*src[ 9] - filter[1]*src[ 8] + filter[0]*src[ 7] + filter[3]*src[10] - filter[4]*src[11] + filter[5]*src[12] + 64) >> 7];
+        dst[10] = cm[(filter[2]*src[10] - filter[1]*src[ 9] + filter[0]*src[ 8] + filter[3]*src[11] - filter[4]*src[12] + filter[5]*src[13] + 64) >> 7];
+        dst[11] = cm[(filter[2]*src[11] - filter[1]*src[10] + filter[0]*src[ 9] + filter[3]*src[12] - filter[4]*src[13] + filter[5]*src[14] + 64) >> 7];
+        dst[12] = cm[(filter[2]*src[12] - filter[1]*src[11] + filter[0]*src[10] + filter[3]*src[13] - filter[4]*src[14] + filter[5]*src[15] + 64) >> 7];
+        dst[13] = cm[(filter[2]*src[13] - filter[1]*src[12] + filter[0]*src[11] + filter[3]*src[14] - filter[4]*src[15] + filter[5]*src[16] + 64) >> 7];
+        dst[14] = cm[(filter[2]*src[14] - filter[1]*src[13] + filter[0]*src[12] + filter[3]*src[15] - filter[4]*src[16] + filter[5]*src[17] + 64) >> 7];
+        dst[15] = cm[(filter[2]*src[15] - filter[1]*src[14] + filter[0]*src[13] + filter[3]*src[16] - filter[4]*src[17] + filter[5]*src[18] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x10(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x09(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x12(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0b(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0b(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_6TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[7];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        dst[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        dst[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        dst[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        dst[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_6TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    double ftmp[6];
+    uint64_t tmp[1];
+    int low32;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   -0x02(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x03(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [tmp0]"=&r"(tmp[0]),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_6TAP(src, filter, 1);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[10];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[1-srcstride] + filter[3] * src[1+srcstride] - filter[4] * src[1+2*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[2-srcstride] + filter[3] * src[2+srcstride] - filter[4] * src[2+2*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[3-srcstride] + filter[3] * src[3+srcstride] - filter[4] * src[3+2*srcstride] + 64) >> 7];
+        dst[4] = cm[(filter[2] * src[4] - filter[1] * src[4-srcstride] + filter[3] * src[4+srcstride] - filter[4] * src[4+2*srcstride] + 64) >> 7];
+        dst[5] = cm[(filter[2] * src[5] - filter[1] * src[5-srcstride] + filter[3] * src[5+srcstride] - filter[4] * src[5+2*srcstride] + 64) >> 7];
+        dst[6] = cm[(filter[2] * src[6] - filter[1] * src[6-srcstride] + filter[3] * src[6+srcstride] - filter[4] * src[6+2*srcstride] + 64) >> 7];
+        dst[7] = cm[(filter[2] * src[7] - filter[1] * src[7-srcstride] + filter[3] * src[7+srcstride] - filter[4] * src[7+2*srcstride] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2] * src[ 8] - filter[1] * src[ 8-srcstride] + filter[3] * src[ 8+srcstride] - filter[4] * src[ 8+2*srcstride] + 64) >> 7];
+        dst[ 9] = cm[(filter[2] * src[ 9] - filter[1] * src[ 9-srcstride] + filter[3] * src[ 9+srcstride] - filter[4] * src[ 9+2*srcstride] + 64) >> 7];
+        dst[10] = cm[(filter[2] * src[10] - filter[1] * src[10-srcstride] + filter[3] * src[10+srcstride] - filter[4] * src[10+2*srcstride] + 64) >> 7];
+        dst[11] = cm[(filter[2] * src[11] - filter[1] * src[11-srcstride] + filter[3] * src[11+srcstride] - filter[4] * src[11+2*srcstride] + 64) >> 7];
+        dst[12] = cm[(filter[2] * src[12] - filter[1] * src[12-srcstride] + filter[3] * src[12+srcstride] - filter[4] * src[12+2*srcstride] + 64) >> 7];
+        dst[13] = cm[(filter[2] * src[13] - filter[1] * src[13-srcstride] + filter[3] * src[13+srcstride] - filter[4] * src[13+2*srcstride] + 64) >> 7];
+        dst[14] = cm[(filter[2] * src[14] - filter[1] * src[14-srcstride] + filter[3] * src[14+srcstride] - filter[4] * src[14+2*srcstride] + 64) >> 7];
+        dst[15] = cm[(filter[2] * src[15] - filter[1] * src[15-srcstride] + filter[3] * src[15+srcstride] - filter[4] * src[15+2*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+#if HAVE_LOONGSON3
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "gsldlc1    %[ftmp1],   0x07(%[src0])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src0])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src0])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src0])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src1])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src2])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src2])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src2])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src2])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src3])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src3])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src3])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src3])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src3])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src3])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src+srcstride),
+              [src3]"r"(src+2*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_4TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[7];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[1-srcstride] + filter[3] * src[1+srcstride] - filter[4] * src[1+2*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[2-srcstride] + filter[3] * src[2+srcstride] - filter[4] * src[2+2*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[3-srcstride] + filter[3] * src[3+srcstride] - filter[4] * src[3+2*srcstride] + 64) >> 7];
+        dst[4] = cm[(filter[2] * src[4] - filter[1] * src[4-srcstride] + filter[3] * src[4+srcstride] - filter[4] * src[4+2*srcstride] + 64) >> 7];
+        dst[5] = cm[(filter[2] * src[5] - filter[1] * src[5-srcstride] + filter[3] * src[5+srcstride] - filter[4] * src[5+2*srcstride] + 64) >> 7];
+        dst[6] = cm[(filter[2] * src[6] - filter[1] * src[6-srcstride] + filter[3] * src[6+srcstride] - filter[4] * src[6+2*srcstride] + 64) >> 7];
+        dst[7] = cm[(filter[2] * src[7] - filter[1] * src[7-srcstride] + filter[3] * src[7+srcstride] - filter[4] * src[7+2*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src0])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src2])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src3])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src3])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src3])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src+srcstride),
+              [src3]"r"(src+2*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_4TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[6];
+    uint64_t tmp[1];
+    int low32;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2] * src[1] - filter[1] * src[1-srcstride] + filter[3] * src[1+srcstride] - filter[4] * src[1+2*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2] * src[2] - filter[1] * src[2-srcstride] + filter[3] * src[2+srcstride] - filter[4] * src[2+2*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2] * src[3] - filter[1] * src[3-srcstride] + filter[3] * src[3+srcstride] - filter[4] * src[3+2*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src0])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src1])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src2])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src3])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [tmp0]"=&r"(tmp[0]),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src+srcstride),
+              [src3]"r"(src+2*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_4TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[10];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[1-srcstride] + filter[0]*src[1-2*srcstride] + filter[3]*src[1+srcstride] - filter[4]*src[1+2*srcstride] + filter[5]*src[1+3*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[2-srcstride] + filter[0]*src[2-2*srcstride] + filter[3]*src[2+srcstride] - filter[4]*src[2+2*srcstride] + filter[5]*src[2+3*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[3-srcstride] + filter[0]*src[3-2*srcstride] + filter[3]*src[3+srcstride] - filter[4]*src[3+2*srcstride] + filter[5]*src[3+3*srcstride] + 64) >> 7];
+        dst[4] = cm[(filter[2]*src[4] - filter[1]*src[4-srcstride] + filter[0]*src[4-2*srcstride] + filter[3]*src[4+srcstride] - filter[4]*src[4+2*srcstride] + filter[5]*src[4+3*srcstride] + 64) >> 7];
+        dst[5] = cm[(filter[2]*src[5] - filter[1]*src[5-srcstride] + filter[0]*src[5-2*srcstride] + filter[3]*src[5+srcstride] - filter[4]*src[5+2*srcstride] + filter[5]*src[5+3*srcstride] + 64) >> 7];
+        dst[6] = cm[(filter[2]*src[6] - filter[1]*src[6-srcstride] + filter[0]*src[6-2*srcstride] + filter[3]*src[6+srcstride] - filter[4]*src[6+2*srcstride] + filter[5]*src[6+3*srcstride] + 64) >> 7];
+        dst[7] = cm[(filter[2]*src[7] - filter[1]*src[7-srcstride] + filter[0]*src[7-2*srcstride] + filter[3]*src[7+srcstride] - filter[4]*src[7+2*srcstride] + filter[5]*src[7+3*srcstride] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2]*src[ 8] - filter[1]*src[ 8-srcstride] + filter[0]*src[ 8-2*srcstride] + filter[3]*src[ 8+srcstride] - filter[4]*src[ 8+2*srcstride] + filter[5]*src[ 8+3*srcstride] + 64) >> 7];
+        dst[ 9] = cm[(filter[2]*src[ 9] - filter[1]*src[ 9-srcstride] + filter[0]*src[ 9-2*srcstride] + filter[3]*src[ 9+srcstride] - filter[4]*src[ 9+2*srcstride] + filter[5]*src[ 9+3*srcstride] + 64) >> 7];
+        dst[10] = cm[(filter[2]*src[10] - filter[1]*src[10-srcstride] + filter[0]*src[10-2*srcstride] + filter[3]*src[10+srcstride] - filter[4]*src[10+2*srcstride] + filter[5]*src[10+3*srcstride] + 64) >> 7];
+        dst[11] = cm[(filter[2]*src[11] - filter[1]*src[11-srcstride] + filter[0]*src[11-2*srcstride] + filter[3]*src[11+srcstride] - filter[4]*src[11+2*srcstride] + filter[5]*src[11+3*srcstride] + 64) >> 7];
+        dst[12] = cm[(filter[2]*src[12] - filter[1]*src[12-srcstride] + filter[0]*src[12-2*srcstride] + filter[3]*src[12+srcstride] - filter[4]*src[12+2*srcstride] + filter[5]*src[12+3*srcstride] + 64) >> 7];
+        dst[13] = cm[(filter[2]*src[13] - filter[1]*src[13-srcstride] + filter[0]*src[13-2*srcstride] + filter[3]*src[13+srcstride] - filter[4]*src[13+2*srcstride] + filter[5]*src[13+3*srcstride] + 64) >> 7];
+        dst[14] = cm[(filter[2]*src[14] - filter[1]*src[14-srcstride] + filter[0]*src[14-2*srcstride] + filter[3]*src[14+srcstride] - filter[4]*src[14+2*srcstride] + filter[5]*src[14+3*srcstride] + 64) >> 7];
+        dst[15] = cm[(filter[2]*src[15] - filter[1]*src[15-srcstride] + filter[0]*src[15-2*srcstride] + filter[3]*src[15+srcstride] - filter[4]*src[15+2*srcstride] + filter[5]*src[15+3*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src0])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src0])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src0])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src0])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src1])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src2])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src2])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src2])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src2])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src3])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src3])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src3])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src3])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src3])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src3])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src4])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src4])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src4])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src4])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src4])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src4])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src5])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src5])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src5])                       \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src5])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src5])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src5])                       \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src-2*srcstride),
+              [src3]"r"(src+srcstride),         [src4]"r"(src+2*srcstride),
+              [src5]"r"(src+3*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_6TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[7];
+    uint64_t tmp[1];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[1-srcstride] + filter[0]*src[1-2*srcstride] + filter[3]*src[1+srcstride] - filter[4]*src[1+2*srcstride] + filter[5]*src[1+3*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[2-srcstride] + filter[0]*src[2-2*srcstride] + filter[3]*src[2+srcstride] - filter[4]*src[2+2*srcstride] + filter[5]*src[2+3*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[3-srcstride] + filter[0]*src[3-2*srcstride] + filter[3]*src[3+srcstride] - filter[4]*src[3+2*srcstride] + filter[5]*src[3+3*srcstride] + 64) >> 7];
+        dst[4] = cm[(filter[2]*src[4] - filter[1]*src[4-srcstride] + filter[0]*src[4-2*srcstride] + filter[3]*src[4+srcstride] - filter[4]*src[4+2*srcstride] + filter[5]*src[4+3*srcstride] + 64) >> 7];
+        dst[5] = cm[(filter[2]*src[5] - filter[1]*src[5-srcstride] + filter[0]*src[5-2*srcstride] + filter[3]*src[5+srcstride] - filter[4]*src[5+2*srcstride] + filter[5]*src[5+3*srcstride] + 64) >> 7];
+        dst[6] = cm[(filter[2]*src[6] - filter[1]*src[6-srcstride] + filter[0]*src[6-2*srcstride] + filter[3]*src[6+srcstride] - filter[4]*src[6+2*srcstride] + filter[5]*src[6+3*srcstride] + 64) >> 7];
+        dst[7] = cm[(filter[2]*src[7] - filter[1]*src[7-srcstride] + filter[0]*src[7-2*srcstride] + filter[3]*src[7+srcstride] - filter[4]*src[7+2*srcstride] + filter[5]*src[7+3*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src0])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src0])                       \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src2])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src3])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src3])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src3])                       \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src4])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src4])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src4])                       \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src5])                       \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src5])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src5])                       \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp[0]),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src-2*srcstride),
+              [src3]"r"(src+srcstride),         [src4]"r"(src+2*srcstride),
+              [src5]"r"(src+3*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_6TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[my - 1];
+    int y;
+    double ftmp[5];
+    uint64_t tmp[1];
+    int low32;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[1-srcstride] + filter[0]*src[1-2*srcstride] + filter[3]*src[1+srcstride] - filter[4]*src[1+2*srcstride] + filter[5]*src[1+3*srcstride] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[2-srcstride] + filter[0]*src[2-2*srcstride] + filter[3]*src[2+srcstride] - filter[4]*src[2+2*srcstride] + filter[5]*src[2+3*srcstride] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[3-srcstride] + filter[0]*src[3-2*srcstride] + filter[3]*src[3+srcstride] - filter[4]*src[3+2*srcstride] + filter[5]*src[3+3*srcstride] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src0])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src1])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src2])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src3])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src4])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x00(%[src5])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp[0]),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                    [src0]"r"(src),
+              [src1]"r"(src-srcstride),         [src2]"r"(src-2*srcstride),
+              [src3]"r"(src+srcstride),         [src4]"r"(src+2*srcstride),
+              [src5]"r"(src+3*srcstride),       [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),          [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),          [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),          [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[my - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_6TAP(src, filter, srcstride);
+        dst += dststride;
+        src += srcstride;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if OK
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[560];
+    uint8_t *tmp = tmp_array;
+    double ftmp[10];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        tmp[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        tmp[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        tmp[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        tmp[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+
+        tmp[ 8] = cm[(filter[2] * src[ 8] - filter[1] * src[ 7] + filter[3] * src[ 9] - filter[4] * src[10] + 64) >> 7];
+        tmp[ 9] = cm[(filter[2] * src[ 9] - filter[1] * src[ 8] + filter[3] * src[10] - filter[4] * src[11] + 64) >> 7];
+        tmp[10] = cm[(filter[2] * src[10] - filter[1] * src[ 9] + filter[3] * src[11] - filter[4] * src[12] + 64) >> 7];
+        tmp[11] = cm[(filter[2] * src[11] - filter[1] * src[10] + filter[3] * src[12] - filter[4] * src[13] + 64) >> 7];
+        tmp[12] = cm[(filter[2] * src[12] - filter[1] * src[11] + filter[3] * src[13] - filter[4] * src[14] + 64) >> 7];
+        tmp[13] = cm[(filter[2] * src[13] - filter[1] * src[12] + filter[3] * src[14] - filter[4] * src[15] + 64) >> 7];
+        tmp[14] = cm[(filter[2] * src[14] - filter[1] * src[13] + filter[3] * src[15] - filter[4] * src[16] + 64) >> 7];
+        tmp[15] = cm[(filter[2] * src[15] - filter[1] * src[14] + filter[3] * src[16] - filter[4] * src[17] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0], %[ftmp0]                  \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4], %[ftmp0]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3], %[ftmp4]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3], %[ftmp4]                  \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4], %[ftmp0]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "psubush    %[ftmp5],   %[ftmp5], %[ftmp2]                  \n\t"
+            "psubush    %[ftmp6],   %[ftmp6], %[ftmp3]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "psubush    %[ftmp8],   %[ftmp8], %[ftmp2]                  \n\t"
+            "psubush    %[ftmp9],   %[ftmp9], %[ftmp3]                  \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4], %[ftmp0]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "paddush    %[ftmp5],   %[ftmp5], %[ftmp2]                  \n\t"
+            "paddush    %[ftmp6],   %[ftmp6], %[ftmp3]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "paddush    %[ftmp8],   %[ftmp8], %[ftmp2]                  \n\t"
+            "paddush    %[ftmp9],   %[ftmp9], %[ftmp3]                  \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4], %[ftmp0]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "psubush    %[ftmp5],   %[ftmp5], %[ftmp2]                  \n\t"
+            "psubush    %[ftmp6],   %[ftmp6], %[ftmp3]                  \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7], %[ftmp0]                  \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7], %[ftmp0]                  \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2], %[ftmp4]                  \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3], %[ftmp4]                  \n\t"
+            "psubush    %[ftmp8],   %[ftmp8], %[ftmp2]                  \n\t"
+            "psubush    %[ftmp9],   %[ftmp9], %[ftmp3]                  \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5], %[ff_pw_64]               \n\t"
+            "paddush    %[ftmp6],   %[ftmp6], %[ff_pw_64]               \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5], %[ftmp4]                  \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6], %[ftmp4]                  \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5], %[ftmp6]                  \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 16;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[560];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 16; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 16;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 16;
+    filter = subpel_filters[my - 1];
+
+#if OK
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-16] + filter[3] * tmp[16] - filter[4] * tmp[32] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-15] + filter[3] * tmp[17] - filter[4] * tmp[33] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-14] + filter[3] * tmp[18] - filter[4] * tmp[34] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-13] + filter[3] * tmp[19] - filter[4] * tmp[35] + 64) >> 7];
+        dst[4] = cm[(filter[2] * tmp[4] - filter[1] * tmp[-12] + filter[3] * tmp[20] - filter[4] * tmp[36] + 64) >> 7];
+        dst[5] = cm[(filter[2] * tmp[5] - filter[1] * tmp[-11] + filter[3] * tmp[21] - filter[4] * tmp[37] + 64) >> 7];
+        dst[6] = cm[(filter[2] * tmp[6] - filter[1] * tmp[-10] + filter[3] * tmp[22] - filter[4] * tmp[38] + 64) >> 7];
+        dst[7] = cm[(filter[2] * tmp[7] - filter[1] * tmp[ -9] + filter[3] * tmp[23] - filter[4] * tmp[39] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2] * tmp[ 8] - filter[1] * tmp[-8] + filter[3] * tmp[24] - filter[4] * tmp[40] + 64) >> 7];
+        dst[ 9] = cm[(filter[2] * tmp[ 9] - filter[1] * tmp[-7] + filter[3] * tmp[25] - filter[4] * tmp[41] + 64) >> 7];
+        dst[10] = cm[(filter[2] * tmp[10] - filter[1] * tmp[-6] + filter[3] * tmp[26] - filter[4] * tmp[42] + 64) >> 7];
+        dst[11] = cm[(filter[2] * tmp[11] - filter[1] * tmp[-5] + filter[3] * tmp[27] - filter[4] * tmp[43] + 64) >> 7];
+        dst[12] = cm[(filter[2] * tmp[12] - filter[1] * tmp[-4] + filter[3] * tmp[28] - filter[4] * tmp[44] + 64) >> 7];
+        dst[13] = cm[(filter[2] * tmp[13] - filter[1] * tmp[-3] + filter[3] * tmp[29] - filter[4] * tmp[45] + 64) >> 7];
+        dst[14] = cm[(filter[2] * tmp[14] - filter[1] * tmp[-2] + filter[3] * tmp[30] - filter[4] * tmp[46] + 64) >> 7];
+        dst[15] = cm[(filter[2] * tmp[15] - filter[1] * tmp[-1] + filter[3] * tmp[31] - filter[4] * tmp[47] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[tmp])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x0f(%[tmp])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x0f(%[tmp])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x1f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x18(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x18(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x27(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x20(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x2f(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x28(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x20(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x28(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[dst])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[dst])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 16;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 16);
+        dst += dststride;
+        tmp += 16;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[152];
+    uint8_t *tmp = tmp_array;
+    double ftmp[7];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        tmp[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        tmp[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        tmp[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        tmp[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 8;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[152];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 8; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 8;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 8;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-8] + filter[3] * tmp[ 8] - filter[4] * tmp[16] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-7] + filter[3] * tmp[ 9] - filter[4] * tmp[17] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-6] + filter[3] * tmp[10] - filter[4] * tmp[18] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-5] + filter[3] * tmp[11] - filter[4] * tmp[19] + 64) >> 7];
+        dst[4] = cm[(filter[2] * tmp[4] - filter[1] * tmp[-4] + filter[3] * tmp[12] - filter[4] * tmp[20] + 64) >> 7];
+        dst[5] = cm[(filter[2] * tmp[5] - filter[1] * tmp[-3] + filter[3] * tmp[13] - filter[4] * tmp[21] + 64) >> 7];
+        dst[6] = cm[(filter[2] * tmp[6] - filter[1] * tmp[-2] + filter[3] * tmp[14] - filter[4] * tmp[22] + 64) >> 7];
+        dst[7] = cm[(filter[2] * tmp[7] - filter[1] * tmp[-1] + filter[3] * tmp[15] - filter[4] * tmp[23] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                    [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 8;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 8);
+        dst += dststride;
+        tmp += 8;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h4v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[44];
+    uint8_t *tmp = tmp_array;
+    double ftmp[5];
+    uint64_t tmp0;
+    int low32;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[tmp])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [tmp]"r"(tmp),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 4;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[44];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 4; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 4;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 4;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-4] + filter[3] * tmp[4] - filter[4] * tmp[ 8] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-3] + filter[3] * tmp[5] - filter[4] * tmp[ 9] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-2] + filter[3] * tmp[6] - filter[4] * tmp[10] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-1] + filter[3] * tmp[7] - filter[4] * tmp[11] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x04(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x04(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                    [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 4;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 4);
+        dst += dststride;
+        tmp += 4;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[592];
+    uint8_t *tmp = tmp_array;
+    double ftmp[10];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        tmp[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        tmp[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        tmp[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        tmp[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+
+        tmp[ 8] = cm[(filter[2] * src[ 8] - filter[1] * src[ 7] + filter[3] * src[ 9] - filter[4] * src[10] + 64) >> 7];
+        tmp[ 9] = cm[(filter[2] * src[ 9] - filter[1] * src[ 8] + filter[3] * src[10] - filter[4] * src[11] + 64) >> 7];
+        tmp[10] = cm[(filter[2] * src[10] - filter[1] * src[ 9] + filter[3] * src[11] - filter[4] * src[12] + 64) >> 7];
+        tmp[11] = cm[(filter[2] * src[11] - filter[1] * src[10] + filter[3] * src[12] - filter[4] * src[13] + 64) >> 7];
+        tmp[12] = cm[(filter[2] * src[12] - filter[1] * src[11] + filter[3] * src[13] - filter[4] * src[14] + 64) >> 7];
+        tmp[13] = cm[(filter[2] * src[13] - filter[1] * src[12] + filter[3] * src[14] - filter[4] * src[15] + 64) >> 7];
+        tmp[14] = cm[(filter[2] * src[14] - filter[1] * src[13] + filter[3] * src[15] - filter[4] * src[16] + 64) >> 7];
+        tmp[15] = cm[(filter[2] * src[15] - filter[1] * src[14] + filter[3] * src[16] - filter[4] * src[17] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x08(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),            [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),            [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                    [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),          [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),          [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 16;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[592];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        for (x = 0; x < 16; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 16;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 32;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 008
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-16] + filter[0]*tmp[-32] + filter[3]*tmp[17] - filter[4]*tmp[32] + filter[5]*tmp[48] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-15] + filter[0]*tmp[-31] + filter[3]*tmp[18] - filter[4]*tmp[33] + filter[5]*tmp[49] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-14] + filter[0]*tmp[-30] + filter[3]*tmp[19] - filter[4]*tmp[34] + filter[5]*tmp[50] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-13] + filter[0]*tmp[-29] + filter[3]*tmp[20] - filter[4]*tmp[35] + filter[5]*tmp[51] + 64) >> 7];
+        dst[4] = cm[(filter[2]*tmp[4] - filter[1]*tmp[-12] + filter[0]*tmp[-28] + filter[3]*tmp[21] - filter[4]*tmp[36] + filter[5]*tmp[52] + 64) >> 7];
+        dst[5] = cm[(filter[2]*tmp[5] - filter[1]*tmp[-11] + filter[0]*tmp[-27] + filter[3]*tmp[22] - filter[4]*tmp[37] + filter[5]*tmp[53] + 64) >> 7];
+        dst[6] = cm[(filter[2]*tmp[6] - filter[1]*tmp[-10] + filter[0]*tmp[-26] + filter[3]*tmp[23] - filter[4]*tmp[38] + filter[5]*tmp[54] + 64) >> 7];
+        dst[7] = cm[(filter[2]*tmp[7] - filter[1]*tmp[ -9] + filter[0]*tmp[-25] + filter[3]*tmp[24] - filter[4]*tmp[39] + filter[5]*tmp[55] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2]*tmp[ 8] - filter[1]*tmp[-8] + filter[0]*tmp[-24] + filter[3]*tmp[25] - filter[4]*tmp[40] + filter[5]*tmp[56] + 64) >> 7];
+        dst[ 9] = cm[(filter[2]*tmp[ 9] - filter[1]*tmp[-7] + filter[0]*tmp[-23] + filter[3]*tmp[26] - filter[4]*tmp[41] + filter[5]*tmp[57] + 64) >> 7];
+        dst[10] = cm[(filter[2]*tmp[10] - filter[1]*tmp[-6] + filter[0]*tmp[-22] + filter[3]*tmp[27] - filter[4]*tmp[42] + filter[5]*tmp[58] + 64) >> 7];
+        dst[11] = cm[(filter[2]*tmp[11] - filter[1]*tmp[-5] + filter[0]*tmp[-21] + filter[3]*tmp[28] - filter[4]*tmp[43] + filter[5]*tmp[59] + 64) >> 7];
+        dst[12] = cm[(filter[2]*tmp[12] - filter[1]*tmp[-4] + filter[0]*tmp[-20] + filter[3]*tmp[29] - filter[4]*tmp[44] + filter[5]*tmp[60] + 64) >> 7];
+        dst[13] = cm[(filter[2]*tmp[13] - filter[1]*tmp[-3] + filter[0]*tmp[-19] + filter[3]*tmp[30] - filter[4]*tmp[45] + filter[5]*tmp[61] + 64) >> 7];
+        dst[14] = cm[(filter[2]*tmp[14] - filter[1]*tmp[-2] + filter[0]*tmp[-18] + filter[3]*tmp[31] - filter[4]*tmp[46] + filter[5]*tmp[62] + 64) >> 7];
+        dst[15] = cm[(filter[2]*tmp[15] - filter[1]*tmp[-1] + filter[0]*tmp[-17] + filter[3]*tmp[32] - filter[4]*tmp[47] + filter[5]*tmp[63] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[tmp])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x10(%[tmp])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x01(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x10(%[tmp])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x19(%[tmp])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x20(%[tmp])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x11(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x18(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x20(%[tmp])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x18(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x18(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x11(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x20(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x19(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x11(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x19(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x27(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x20(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x2f(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x28(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x20(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x28(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x37(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x30(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x3f(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x38(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x30(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x38(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 16;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 16);
+        dst += dststride;
+        tmp += 16;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[168];
+    uint8_t *tmp = tmp_array;
+    double ftmp[7];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 6 - 1; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        tmp[4] = cm[(filter[2] * src[4] - filter[1] * src[ 3] + filter[3] * src[5] - filter[4] * src[6] + 64) >> 7];
+        tmp[5] = cm[(filter[2] * src[5] - filter[1] * src[ 4] + filter[3] * src[6] - filter[4] * src[7] + 64) >> 7];
+        tmp[6] = cm[(filter[2] * src[6] - filter[1] * src[ 5] + filter[3] * src[7] - filter[4] * src[8] + 64) >> 7];
+        tmp[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),      [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),      [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 8;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[168];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 6 - 1; y++) {
+        for (x = 0; x < 8; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 8;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 16;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-8] + filter[0]*tmp[-16] + filter[3]*tmp[ 8] - filter[4]*tmp[16] + filter[5]*tmp[24] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-7] + filter[0]*tmp[-15] + filter[3]*tmp[ 9] - filter[4]*tmp[17] + filter[5]*tmp[25] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-6] + filter[0]*tmp[-14] + filter[3]*tmp[10] - filter[4]*tmp[18] + filter[5]*tmp[26] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-5] + filter[0]*tmp[-13] + filter[3]*tmp[11] - filter[4]*tmp[19] + filter[5]*tmp[27] + 64) >> 7];
+        dst[4] = cm[(filter[2]*tmp[4] - filter[1]*tmp[-4] + filter[0]*tmp[-12] + filter[3]*tmp[12] - filter[4]*tmp[20] + filter[5]*tmp[28] + 64) >> 7];
+        dst[5] = cm[(filter[2]*tmp[5] - filter[1]*tmp[-3] + filter[0]*tmp[-11] + filter[3]*tmp[13] - filter[4]*tmp[21] + filter[5]*tmp[29] + 64) >> 7];
+        dst[6] = cm[(filter[2]*tmp[6] - filter[1]*tmp[-2] + filter[0]*tmp[-10] + filter[3]*tmp[14] - filter[4]*tmp[22] + filter[5]*tmp[30] + 64) >> 7];
+        dst[7] = cm[(filter[2]*tmp[7] - filter[1]*tmp[-1] + filter[0]*tmp[ -9] + filter[3]*tmp[15] - filter[4]*tmp[23] + filter[5]*tmp[31] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x10(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x10(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x1f(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x18(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x18(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 8;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 8);
+        dst += dststride;
+        tmp += 8;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h4v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[52];
+    uint8_t *tmp = tmp_array;
+    double ftmp[5];
+    uint64_t tmp0;
+    int low32;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        /*
+        tmp[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
+        tmp[1] = cm[(filter[2] * src[1] - filter[1] * src[ 0] + filter[3] * src[2] - filter[4] * src[3] + 64) >> 7];
+        tmp[2] = cm[(filter[2] * src[2] - filter[1] * src[ 1] + filter[3] * src[3] - filter[4] * src[4] + 64) >> 7];
+        tmp[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[tmp])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),      [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),      [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        tmp += 4;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[52];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        for (x = 0; x < 4; x++)
+            tmp[x] = FILTER_4TAP(src, filter, 1);
+        tmp += 4;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 8;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-4] + filter[0]*tmp[-8] + filter[3]*tmp[4] - filter[4]*tmp[ 8] + filter[5]*tmp[12] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-3] + filter[0]*tmp[-7] + filter[3]*tmp[5] - filter[4]*tmp[ 9] + filter[5]*tmp[13] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-2] + filter[0]*tmp[-6] + filter[3]*tmp[6] - filter[4]*tmp[10] + filter[5]*tmp[14] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-1] + filter[0]*tmp[-5] + filter[3]*tmp[7] - filter[4]*tmp[11] + filter[5]*tmp[15] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x04(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x04(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x0c(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 4;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 4);
+        dst += dststride;
+        tmp += 4;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[560];
+    uint8_t *tmp = tmp_array;
+    double ftmp[10];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        dst[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        dst[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        dst[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        dst[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        dst[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        dst[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        dst[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2]*src[ 8] - filter[1]*src[ 7] + filter[0]*src[ 6] + filter[3]*src[ 9] - filter[4]*src[10] + filter[5]*src[11] + 64) >> 7];
+        dst[ 9] = cm[(filter[2]*src[ 9] - filter[1]*src[ 8] + filter[0]*src[ 7] + filter[3]*src[10] - filter[4]*src[11] + filter[5]*src[12] + 64) >> 7];
+        dst[10] = cm[(filter[2]*src[10] - filter[1]*src[ 9] + filter[0]*src[ 8] + filter[3]*src[11] - filter[4]*src[12] + filter[5]*src[13] + 64) >> 7];
+        dst[11] = cm[(filter[2]*src[11] - filter[1]*src[10] + filter[0]*src[ 9] + filter[3]*src[12] - filter[4]*src[13] + filter[5]*src[14] + 64) >> 7];
+        dst[12] = cm[(filter[2]*src[12] - filter[1]*src[11] + filter[0]*src[10] + filter[3]*src[13] - filter[4]*src[14] + filter[5]*src[15] + 64) >> 7];
+        dst[13] = cm[(filter[2]*src[13] - filter[1]*src[12] + filter[0]*src[11] + filter[3]*src[14] - filter[4]*src[15] + filter[5]*src[16] + 64) >> 7];
+        dst[14] = cm[(filter[2]*src[14] - filter[1]*src[13] + filter[0]*src[12] + filter[3]*src[15] - filter[4]*src[16] + filter[5]*src[17] + 64) >> 7];
+        dst[15] = cm[(filter[2]*src[15] - filter[1]*src[14] + filter[0]*src[13] + filter[3]*src[16] - filter[4]*src[17] + filter[5]*src[18] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x10(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x09(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x12(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0b(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0b(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 16;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[560];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 16; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 16;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 16;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK // this is ok
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-16] + filter[3] * tmp[16] - filter[4] * tmp[32] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-15] + filter[3] * tmp[17] - filter[4] * tmp[33] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-14] + filter[3] * tmp[18] - filter[4] * tmp[34] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-13] + filter[3] * tmp[19] - filter[4] * tmp[35] + 64) >> 7];
+        dst[4] = cm[(filter[2] * tmp[4] - filter[1] * tmp[-12] + filter[3] * tmp[20] - filter[4] * tmp[36] + 64) >> 7];
+        dst[5] = cm[(filter[2] * tmp[5] - filter[1] * tmp[-11] + filter[3] * tmp[21] - filter[4] * tmp[37] + 64) >> 7];
+        dst[6] = cm[(filter[2] * tmp[6] - filter[1] * tmp[-10] + filter[3] * tmp[22] - filter[4] * tmp[38] + 64) >> 7];
+        dst[7] = cm[(filter[2] * tmp[7] - filter[1] * tmp[ -9] + filter[3] * tmp[23] - filter[4] * tmp[39] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2] * tmp[ 8] - filter[1] * tmp[-8] + filter[3] * tmp[24] - filter[4] * tmp[40] + 64) >> 7];
+        dst[ 9] = cm[(filter[2] * tmp[ 9] - filter[1] * tmp[-7] + filter[3] * tmp[25] - filter[4] * tmp[41] + 64) >> 7];
+        dst[10] = cm[(filter[2] * tmp[10] - filter[1] * tmp[-6] + filter[3] * tmp[26] - filter[4] * tmp[42] + 64) >> 7];
+        dst[11] = cm[(filter[2] * tmp[11] - filter[1] * tmp[-5] + filter[3] * tmp[27] - filter[4] * tmp[43] + 64) >> 7];
+        dst[12] = cm[(filter[2] * tmp[12] - filter[1] * tmp[-4] + filter[3] * tmp[28] - filter[4] * tmp[44] + 64) >> 7];
+        dst[13] = cm[(filter[2] * tmp[13] - filter[1] * tmp[-3] + filter[3] * tmp[29] - filter[4] * tmp[45] + 64) >> 7];
+        dst[14] = cm[(filter[2] * tmp[14] - filter[1] * tmp[-2] + filter[3] * tmp[30] - filter[4] * tmp[46] + 64) >> 7];
+        dst[15] = cm[(filter[2] * tmp[15] - filter[1] * tmp[-1] + filter[3] * tmp[31] - filter[4] * tmp[47] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[src])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x10(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x08(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x10(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x08(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp1],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),      [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),      [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 16;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 16);
+        dst += dststride;
+        tmp += 16;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[152];
+    uint8_t *tmp = tmp_array;
+    double ftmp[7];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        tmp[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        tmp[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        tmp[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        tmp[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        tmp[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        tmp[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        tmp[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        tmp[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 8;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[152];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 8; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 8;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 8;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-8] + filter[3] * tmp[ 8] - filter[4] * tmp[16] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-7] + filter[3] * tmp[ 9] - filter[4] * tmp[17] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-6] + filter[3] * tmp[10] - filter[4] * tmp[18] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-5] + filter[3] * tmp[11] - filter[4] * tmp[19] + 64) >> 7];
+        dst[4] = cm[(filter[2] * tmp[4] - filter[1] * tmp[-4] + filter[3] * tmp[12] - filter[4] * tmp[20] + 64) >> 7];
+        dst[5] = cm[(filter[2] * tmp[5] - filter[1] * tmp[-3] + filter[3] * tmp[13] - filter[4] * tmp[21] + 64) >> 7];
+        dst[6] = cm[(filter[2] * tmp[6] - filter[1] * tmp[-2] + filter[3] * tmp[14] - filter[4] * tmp[22] + 64) >> 7];
+        dst[7] = cm[(filter[2] * tmp[7] - filter[1] * tmp[-1] + filter[3] * tmp[15] - filter[4] * tmp[23] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),      [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),      [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 8;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 8);
+        dst += dststride;
+        tmp += 8;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h6v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[44];
+    uint8_t *tmp = tmp_array;
+    double ftmp[7];
+    uint64_t tmp0;
+    int low32;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        /*
+        tmp[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[3] + 64) >> 7];
+        tmp[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[4] + 64) >> 7];
+        tmp[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[5] + 64) >> 7];
+        tmp[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[6] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   -0x02(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x03(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[tmp])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 4;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[44];
+    uint8_t *tmp = tmp_array;
+
+    src -= srcstride;
+
+    for (y = 0; y < h + 3; y++) {
+        for (x = 0; x < 4; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 4;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 4;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2] * tmp[0] - filter[1] * tmp[-4] + filter[3] * tmp[4] - filter[4] * tmp[ 8] + 64) >> 7];
+        dst[1] = cm[(filter[2] * tmp[1] - filter[1] * tmp[-3] + filter[3] * tmp[5] - filter[4] * tmp[ 9] + 64) >> 7];
+        dst[2] = cm[(filter[2] * tmp[2] - filter[1] * tmp[-2] + filter[3] * tmp[6] - filter[4] * tmp[10] + 64) >> 7];
+        dst[3] = cm[(filter[2] * tmp[3] - filter[1] * tmp[-1] + filter[3] * tmp[7] - filter[4] * tmp[11] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x04(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x04(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter1]"r"(filter[1]),      [filter2]"r"(filter[2]),
+              [filter3]"r"(filter[3]),      [filter4]"r"(filter[4])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 4;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_4TAP(tmp, filter, 4);
+        dst += dststride;
+        tmp += 4;
+    }
+#endif
+}
+
+void ff_put_vp8_epel16_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[592];
+    uint8_t *tmp = tmp_array;
+    double ftmp[10];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        /*
+        tmp[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        tmp[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        tmp[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        tmp[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        tmp[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        tmp[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        tmp[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        tmp[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+
+        tmp[ 8] = cm[(filter[2]*src[ 8] - filter[1]*src[ 7] + filter[0]*src[ 6] + filter[3]*src[ 9] - filter[4]*src[10] + filter[5]*src[11] + 64) >> 7];
+        tmp[ 9] = cm[(filter[2]*src[ 9] - filter[1]*src[ 8] + filter[0]*src[ 7] + filter[3]*src[10] - filter[4]*src[11] + filter[5]*src[12] + 64) >> 7];
+        tmp[10] = cm[(filter[2]*src[10] - filter[1]*src[ 9] + filter[0]*src[ 8] + filter[3]*src[11] - filter[4]*src[12] + filter[5]*src[13] + 64) >> 7];
+        tmp[11] = cm[(filter[2]*src[11] - filter[1]*src[10] + filter[0]*src[ 9] + filter[3]*src[12] - filter[4]*src[13] + filter[5]*src[14] + 64) >> 7];
+        tmp[12] = cm[(filter[2]*src[12] - filter[1]*src[11] + filter[0]*src[10] + filter[3]*src[13] - filter[4]*src[14] + filter[5]*src[15] + 64) >> 7];
+        tmp[13] = cm[(filter[2]*src[13] - filter[1]*src[12] + filter[0]*src[11] + filter[3]*src[14] - filter[4]*src[15] + filter[5]*src[16] + 64) >> 7];
+        tmp[14] = cm[(filter[2]*src[14] - filter[1]*src[13] + filter[0]*src[12] + filter[3]*src[15] - filter[4]*src[16] + filter[5]*src[17] + 64) >> 7];
+        tmp[15] = cm[(filter[2]*src[15] - filter[1]*src[14] + filter[0]*src[13] + filter[3]*src[16] - filter[4]*src[17] + filter[5]*src[18] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0e(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x07(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+            "gsldlc1    %[ftmp7],   0x0d(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x06(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x10(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x09(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x11(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0a(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x12(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x0b(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x0b(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 16;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[592];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        for (x = 0; x < 16; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 16;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 32;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-16] + filter[0]*tmp[-32] + filter[3]*tmp[16] - filter[4]*tmp[32] + filter[5]*tmp[48] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-15] + filter[0]*tmp[-31] + filter[3]*tmp[17] - filter[4]*tmp[33] + filter[5]*tmp[49] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-14] + filter[0]*tmp[-30] + filter[3]*tmp[18] - filter[4]*tmp[34] + filter[5]*tmp[50] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-13] + filter[0]*tmp[-29] + filter[3]*tmp[19] - filter[4]*tmp[35] + filter[5]*tmp[51] + 64) >> 7];
+        dst[4] = cm[(filter[2]*tmp[4] - filter[1]*tmp[-12] + filter[0]*tmp[-28] + filter[3]*tmp[20] - filter[4]*tmp[36] + filter[5]*tmp[52] + 64) >> 7];
+        dst[5] = cm[(filter[2]*tmp[5] - filter[1]*tmp[-11] + filter[0]*tmp[-27] + filter[3]*tmp[21] - filter[4]*tmp[37] + filter[5]*tmp[53] + 64) >> 7];
+        dst[6] = cm[(filter[2]*tmp[6] - filter[1]*tmp[-10] + filter[0]*tmp[-26] + filter[3]*tmp[22] - filter[4]*tmp[38] + filter[5]*tmp[54] + 64) >> 7];
+        dst[7] = cm[(filter[2]*tmp[7] - filter[1]*tmp[ -9] + filter[0]*tmp[-25] + filter[3]*tmp[23] - filter[4]*tmp[39] + filter[5]*tmp[55] + 64) >> 7];
+
+        dst[ 8] = cm[(filter[2]*tmp[ 8] - filter[1]*tmp[-8] + filter[0]*tmp[-24] + filter[3]*tmp[24] - filter[4]*tmp[40] + filter[5]*tmp[56] + 64) >> 7];
+        dst[ 9] = cm[(filter[2]*tmp[ 9] - filter[1]*tmp[-7] + filter[0]*tmp[-23] + filter[3]*tmp[25] - filter[4]*tmp[41] + filter[5]*tmp[57] + 64) >> 7];
+        dst[10] = cm[(filter[2]*tmp[10] - filter[1]*tmp[-6] + filter[0]*tmp[-22] + filter[3]*tmp[26] - filter[4]*tmp[42] + filter[5]*tmp[58] + 64) >> 7];
+        dst[11] = cm[(filter[2]*tmp[11] - filter[1]*tmp[-5] + filter[0]*tmp[-21] + filter[3]*tmp[27] - filter[4]*tmp[43] + filter[5]*tmp[59] + 64) >> 7];
+        dst[12] = cm[(filter[2]*tmp[12] - filter[1]*tmp[-4] + filter[0]*tmp[-20] + filter[3]*tmp[28] - filter[4]*tmp[44] + filter[5]*tmp[60] + 64) >> 7];
+        dst[13] = cm[(filter[2]*tmp[13] - filter[1]*tmp[-3] + filter[0]*tmp[-19] + filter[3]*tmp[29] - filter[4]*tmp[45] + filter[5]*tmp[61] + 64) >> 7];
+        dst[14] = cm[(filter[2]*tmp[14] - filter[1]*tmp[-2] + filter[0]*tmp[-18] + filter[3]*tmp[30] - filter[4]*tmp[46] + filter[5]*tmp[62] + 64) >> 7];
+        dst[15] = cm[(filter[2]*tmp[15] - filter[1]*tmp[-1] + filter[0]*tmp[-17] + filter[3]*tmp[31] - filter[4]*tmp[47] + filter[5]*tmp[63] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp9],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[tmp])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x10(%[tmp])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x10(%[tmp])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x19(%[tmp])                       \n\t"
+            "gsldrc1    %[ftmp1],   -0x20(%[tmp])                       \n\t"
+            "gsldlc1    %[ftmp7],   -0x11(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   -0x18(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x20(%[tmp])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   -0x18(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x1f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x18(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x18(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x27(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x20(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x2f(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x28(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x20(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x28(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x37(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x30(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp7],   0x3f(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp7],   0x38(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x30(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x38(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp7]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp8],   %[ftmp8],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp9],   %[ftmp9],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp9],   %[ftmp9],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp2],   %[ftmp8],       %[ftmp9]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp2],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp2],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp2]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 16;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 16);
+        dst += dststride;
+        tmp += 16;
+    }
+#endif
+}
+
+void ff_put_vp8_epel8_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[168];
+    uint8_t *tmp = tmp_array;
+    double ftmp[7];
+    uint64_t tmp0;
+    uint64_t all64;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        /*
+        tmp[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        tmp[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        tmp[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        tmp[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        tmp[4] = cm[(filter[2]*src[4] - filter[1]*src[ 3] + filter[0]*src[ 2] + filter[3]*src[5] - filter[4]*src[6] + filter[5]*src[ 7] + 64) >> 7];
+        tmp[5] = cm[(filter[2]*src[5] - filter[1]*src[ 4] + filter[0]*src[ 3] + filter[3]*src[6] - filter[4]*src[7] + filter[5]*src[ 8] + 64) >> 7];
+        tmp[6] = cm[(filter[2]*src[6] - filter[1]*src[ 5] + filter[0]*src[ 4] + filter[3]*src[7] - filter[4]*src[8] + filter[5]*src[ 9] + 64) >> 7];
+        tmp[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x06(%[src])                        \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x01(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x01(%[src])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x05(%[src])                        \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x02(%[src])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x02(%[src])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x08(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x09(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x02(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x02(%[src])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0a(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x03(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x03(%[src])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 8;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[168];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        for (x = 0; x < 8; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 8;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 16;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-8] + filter[0]*tmp[-16] + filter[3]*tmp[ 8] - filter[4]*tmp[16] + filter[5]*tmp[24] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-7] + filter[0]*tmp[-15] + filter[3]*tmp[ 9] - filter[4]*tmp[17] + filter[5]*tmp[25] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-6] + filter[0]*tmp[-14] + filter[3]*tmp[10] - filter[4]*tmp[18] + filter[5]*tmp[26] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-5] + filter[0]*tmp[-13] + filter[3]*tmp[11] - filter[4]*tmp[19] + filter[5]*tmp[27] + 64) >> 7];
+        dst[4] = cm[(filter[2]*tmp[4] - filter[1]*tmp[-4] + filter[0]*tmp[-12] + filter[3]*tmp[12] - filter[4]*tmp[20] + filter[5]*tmp[28] + 64) >> 7];
+        dst[5] = cm[(filter[2]*tmp[5] - filter[1]*tmp[-3] + filter[0]*tmp[-11] + filter[3]*tmp[13] - filter[4]*tmp[21] + filter[5]*tmp[29] + 64) >> 7];
+        dst[6] = cm[(filter[2]*tmp[6] - filter[1]*tmp[-2] + filter[0]*tmp[-10] + filter[3]*tmp[14] - filter[4]*tmp[22] + filter[5]*tmp[30] + 64) >> 7];
+        dst[7] = cm[(filter[2]*tmp[7] - filter[1]*tmp[-1] + filter[0]*tmp[ -9] + filter[3]*tmp[15] - filter[4]*tmp[23] + filter[5]*tmp[31] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[filter2], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp3],       %[ftmp4]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x01(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x08(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[filter1], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   -0x09(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   -0x10(%[tmp])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   -0x10(%[tmp])                       \n\t"
+            "mtc1       %[filter0], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[filter3], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x17(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x10(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "mtc1       %[filter4], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "psubush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "psubush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x1f(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x18(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x18(%[tmp])                        \n\t"
+            "mtc1       %[filter5], %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp4]            \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ftmp2]            \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp5],   %[ftmp5],       %[ff_pw_64]         \n\t"
+            "paddush    %[ftmp6],   %[ftmp6],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp4]            \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp4]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),
+              [tmp0]"=&r"(tmp0),
+              [all64]"=&r"(all64)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+        dst += dststride;
+        tmp += 8;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 8);
+        dst += dststride;
+        tmp += 8;
+    }
+#endif
+}
+
+void ff_put_vp8_epel4_h6v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
+        ptrdiff_t srcstride, int h, int mx, int my)
+{
+#if NOTOK //FIXME fate-vp8-test-vector-002 006 009
+    const uint8_t *filter = subpel_filters[mx - 1];
+    int y;
+    uint8_t tmp_array[52];
+    uint8_t *tmp = tmp_array;
+    double ftmp[5];
+    uint64_t tmp0;
+    int low32;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        /*
+        tmp[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
+        tmp[1] = cm[(filter[2]*src[1] - filter[1]*src[ 0] + filter[0]*src[-1] + filter[3]*src[2] - filter[4]*src[3] + filter[5]*src[ 4] + 64) >> 7];
+        tmp[2] = cm[(filter[2]*src[2] - filter[1]*src[ 1] + filter[0]*src[ 0] + filter[3]*src[3] - filter[4]*src[4] + filter[5]*src[ 5] + 64) >> 7];
+        tmp[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x01(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   -0x02(%[src])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x02(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x03(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[tmp])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [tmp]"r"(tmp),                [src]"r"(src),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        tmp += 4;
+        src += srcstride;
+    }
+#else
+    const uint8_t *filter = subpel_filters[mx - 1];
+    const uint8_t *cm     = ff_crop_tab + MAX_NEG_CROP;
+    int x, y;
+    uint8_t tmp_array[52];
+    uint8_t *tmp = tmp_array;
+
+    src -= 2 * srcstride;
+
+    for (y = 0; y < h + 5; y++) {
+        for (x = 0; x < 4; x++)
+            tmp[x] = FILTER_6TAP(src, filter, 1);
+        tmp += 4;
+        src += srcstride;
+    }
+#endif
+
+    tmp    = tmp_array + 8;
+    filter = subpel_filters[my - 1];
+
+#if NOTOK //FIXME fate-vp8-test-vector-006
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = cm[(filter[2]*tmp[0] - filter[1]*tmp[-4] + filter[0]*tmp[-8] + filter[3]*tmp[4] - filter[4]*tmp[ 8] + filter[5]*tmp[12] + 64) >> 7];
+        dst[1] = cm[(filter[2]*tmp[1] - filter[1]*tmp[-3] + filter[0]*tmp[-7] + filter[3]*tmp[5] - filter[4]*tmp[ 9] + filter[5]*tmp[13] + 64) >> 7];
+        dst[2] = cm[(filter[2]*tmp[2] - filter[1]*tmp[-2] + filter[0]*tmp[-6] + filter[3]*tmp[6] - filter[4]*tmp[10] + filter[5]*tmp[14] + 64) >> 7];
+        dst[3] = cm[(filter[2]*tmp[3] - filter[1]*tmp[-1] + filter[0]*tmp[-5] + filter[3]*tmp[7] - filter[4]*tmp[11] + filter[5]*tmp[15] + 64) >> 7];
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter2], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp4],   %[ftmp2],       %[ftmp3]            \n\t"
+
+            "ulw        %[low32],   -0x04(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter1], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   -0x08(%[tmp])                       \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter0], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x04(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter3], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter4], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "psubush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "ulw        %[low32],   0x0c(%[tmp])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[filter5], %[ftmp3]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp3]            \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ftmp2]            \n\t"
+
+            "li         %[tmp0],    0x07                                \n\t"
+            "paddush    %[ftmp4],   %[ftmp4],       %[ff_pw_64]         \n\t"
+            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp3]            \n\t"
+
+            "packushb   %[ftmp1],   %[ftmp4],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsswlc1    %[ftmp1],   0x03(%[dst])                        \n\t"
+            "gsswrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "mfc1       %[low32],   %[ftmp1]                            \n\t"
+            "usw        %[low32],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp0]"=&r"(tmp0),
+              [low32]"=&r"(low32)
+            : [dst]"r"(dst),                [tmp]"r"(tmp),
+              [ff_pw_64]"f"(ff_pw_64),
+              [filter0]"r"(filter[0]),      [filter1]"r"(filter[1]),
+              [filter2]"r"(filter[2]),      [filter3]"r"(filter[3]),
+              [filter4]"r"(filter[4]),      [filter5]"r"(filter[5])
+            : "memory"
+        );
+
+        dst += dststride;
+        tmp += 4;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = FILTER_6TAP(tmp, filter, 4);
+        dst += dststride;
+        tmp += 4;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear16_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int y;
+    double ftmp[15];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        dst[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        dst[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        dst[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        dst[4] = (a * src[4] + b * src[5] + 4) >> 3;
+        dst[5] = (a * src[5] + b * src[6] + 4) >> 3;
+        dst[6] = (a * src[6] + b * src[7] + 4) >> 3;
+        dst[7] = (a * src[7] + b * src[8] + 4) >> 3;
+
+        dst[ 8] = (a * src[ 8] + b * src[ 9] + 4) >> 3;
+        dst[ 9] = (a * src[ 9] + b * src[10] + 4) >> 3;
+        dst[10] = (a * src[10] + b * src[11] + 4) >> 3;
+        dst[11] = (a * src[11] + b * src[12] + 4) >> 3;
+        dst[12] = (a * src[12] + b * src[13] + 4) >> 3;
+        dst[13] = (a * src[13] + b * src[14] + 4) >> 3;
+        dst[14] = (a * src[14] + b * src[15] + 4) >> 3;
+        dst[15] = (a * src[15] + b * src[16] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x0f(%[src])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "gsldlc1    %[ftmp3],   0x08(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp13]                           \n\t"
+            "gsldrc1    %[ftmp3],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp4],   0x10(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp14]                           \n\t"
+            "gsldrc1    %[ftmp4],   0x09(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp13]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x09(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp14]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+            "pshufh     %[ftmp13],  %[ftmp13],      %[ftmp0]            \n\t"
+            "pshufh     %[ftmp14],  %[ftmp14],      %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp9],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp10],  %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp11],  %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp12],  %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp14]           \n\t"
+            "pmullh     %[ftmp10],  %[ftmp10],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp11],  %[ftmp11],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp12],  %[ftmp12],      %[ftmp14]           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp9]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp11]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp12]           \n\t"
+            "dmtc1      %[ff_pw_4], %[ftmp14]                           \n\t"
+            "dmtc1      %[ff_pw_3], %[ftmp13]                           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp14]           \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp7],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp7],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
+              [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
+              [ftmp14]"=&f"(ftmp[14]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"r"(ff_pw_4),        [ff_pw_3]"r"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear16_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int c = 8 - my, d = my;
+    int y;
+    double ftmp[15];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
+        dst[1] = (c * src[1] + d * src[1 + sstride] + 4) >> 3;
+        dst[2] = (c * src[2] + d * src[2 + sstride] + 4) >> 3;
+        dst[3] = (c * src[3] + d * src[3 + sstride] + 4) >> 3;
+        dst[4] = (c * src[4] + d * src[4 + sstride] + 4) >> 3;
+        dst[5] = (c * src[5] + d * src[5 + sstride] + 4) >> 3;
+        dst[6] = (c * src[6] + d * src[6 + sstride] + 4) >> 3;
+        dst[7] = (c * src[7] + d * src[7 + sstride] + 4) >> 3;
+
+        dst[ 8] = (c * src[ 8] + d * src[ 8 + sstride] + 4) >> 3;
+        dst[ 9] = (c * src[ 9] + d * src[ 9 + sstride] + 4) >> 3;
+        dst[10] = (c * src[10] + d * src[10 + sstride] + 4) >> 3;
+        dst[11] = (c * src[11] + d * src[11 + sstride] + 4) >> 3;
+        dst[12] = (c * src[12] + d * src[12 + sstride] + 4) >> 3;
+        dst[13] = (c * src[13] + d * src[13 + sstride] + 4) >> 3;
+        dst[14] = (c * src[14] + d * src[14 + sstride] + 4) >> 3;
+        dst[15] = (c * src[15] + d * src[15 + sstride] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+            "gsldlc1    %[ftmp2],   0x0f(%[src1])                       \n\t"
+            "gsldrc1    %[ftmp2],   0x08(%[src1])                       \n\t"
+            "gsldlc1    %[ftmp3],   0x07(%[src2])                       \n\t"
+            "mtc1       %[c],       %[ftmp13]                           \n\t"
+            "gsldrc1    %[ftmp3],   0x00(%[src2])                       \n\t"
+            "gsldlc1    %[ftmp4],   0x0f(%[src2])                       \n\t"
+            "mtc1       %[d],       %[ftmp14]                           \n\t"
+            "gsldrc1    %[ftmp4],   0x08(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src1])                       \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "mtc1       %[c],       %[ftmp13]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x08(%[src2])                       \n\t"
+            "mtc1       %[d],       %[ftmp14]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+            "pshufh     %[ftmp13],  %[ftmp13],      %[ftmp0]            \n\t"
+            "pshufh     %[ftmp14],  %[ftmp14],      %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp9],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp10],  %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp11],  %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp12],  %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp14]           \n\t"
+            "pmullh     %[ftmp10],  %[ftmp10],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp11],  %[ftmp11],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp12],  %[ftmp12],      %[ftmp14]           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp9]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp11]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp12]           \n\t"
+            "dmtc1      %[ff_pw_4], %[ftmp14]                           \n\t"
+            "dmtc1      %[ff_pw_3], %[ftmp13]                           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp14]          \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp14]          \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp14]          \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp14]          \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp13]          \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp13]          \n\t"
+            "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp13]          \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp13]          \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp7],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp7],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
+              [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
+              [ftmp14]"=&f"(ftmp[14]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [src1]"r"(src),               [src2]"r"(src+sstride),
+              [c]"r"(c),                    [d]"r"(d),
+              [ff_pw_4]"r"(ff_pw_4),        [ff_pw_3]"r"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int c = 8 - my, d = my;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = (c * src[x] + d * src[x + sstride] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear16_hv_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int y;
+    uint8_t tmp_array[528];
+    uint8_t *tmp = tmp_array;
+    double ftmp[15];
+    uint64_t all64;
+
+    for (y = 0; y < h + 1; y++) {
+        /*
+        tmp[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        tmp[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        tmp[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        tmp[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        tmp[4] = (a * src[4] + b * src[5] + 4) >> 3;
+        tmp[5] = (a * src[5] + b * src[6] + 4) >> 3;
+        tmp[6] = (a * src[6] + b * src[7] + 4) >> 3;
+        tmp[7] = (a * src[7] + b * src[8] + 4) >> 3;
+
+        tmp[ 8] = (a * src[ 8] + b * src[ 9] + 4) >> 3;
+        tmp[ 9] = (a * src[ 9] + b * src[10] + 4) >> 3;
+        tmp[10] = (a * src[10] + b * src[11] + 4) >> 3;
+        tmp[11] = (a * src[11] + b * src[12] + 4) >> 3;
+        tmp[12] = (a * src[12] + b * src[13] + 4) >> 3;
+        tmp[13] = (a * src[13] + b * src[14] + 4) >> 3;
+        tmp[14] = (a * src[14] + b * src[15] + 4) >> 3;
+        tmp[15] = (a * src[15] + b * src[16] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x0f(%[src])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "gsldlc1    %[ftmp3],   0x08(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp13]                           \n\t"
+            "gsldrc1    %[ftmp3],   0x01(%[src])                        \n\t"
+            "gsldlc1    %[ftmp4],   0x10(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp14]                           \n\t"
+            "gsldrc1    %[ftmp4],   0x09(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[src])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp13]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x09(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp14]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+            "pshufh     %[ftmp13],  %[ftmp13],      %[ftmp0]            \n\t"
+            "pshufh     %[ftmp14],  %[ftmp14],      %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp9],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp10],  %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp11],  %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp12],  %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp14]           \n\t"
+            "pmullh     %[ftmp10],  %[ftmp10],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp11],  %[ftmp11],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp12],  %[ftmp12],      %[ftmp14]           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp9]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp11]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp12]           \n\t"
+            "dmtc1      %[ff_pw_4], %[ftmp14]                           \n\t"
+            "dmtc1      %[ff_pw_3], %[ftmp13]                           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp14]           \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[tmp])                        \n\t"
+            "gssdlc1    %[ftmp7],   0x0f(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp7],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
+              [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
+              [ftmp14]"=&f"(ftmp[14]),
+              [all64]"=&r"(all64),
+              [tmp]"+&r"(tmp)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"r"(ff_pw_4),        [ff_pw_3]"r"(ff_pw_3)
+            : "memory"
+        );
+
+        tmp += 16;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int x, y;
+    uint8_t tmp_array[528];
+    uint8_t *tmp = tmp_array;
+
+    for (y = 0; y < h + 1; y++) {
+        for (x = 0; x < 16; x++)
+            tmp[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        tmp += 16;
+        src += sstride;
+    }
+#endif
+
+    tmp = tmp_array;
+
+#if OK
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * tmp[0] + d * tmp[16] + 4) >> 3;
+        dst[1] = (c * tmp[1] + d * tmp[17] + 4) >> 3;
+        dst[2] = (c * tmp[2] + d * tmp[18] + 4) >> 3;
+        dst[3] = (c * tmp[3] + d * tmp[19] + 4) >> 3;
+        dst[4] = (c * tmp[4] + d * tmp[20] + 4) >> 3;
+        dst[5] = (c * tmp[5] + d * tmp[21] + 4) >> 3;
+        dst[6] = (c * tmp[6] + d * tmp[22] + 4) >> 3;
+        dst[7] = (c * tmp[7] + d * tmp[23] + 4) >> 3;
+
+        dst[ 8] = (c * tmp[ 8] + d * tmp[24] + 4) >> 3;
+        dst[ 9] = (c * tmp[ 9] + d * tmp[25] + 4) >> 3;
+        dst[10] = (c * tmp[10] + d * tmp[26] + 4) >> 3;
+        dst[11] = (c * tmp[11] + d * tmp[27] + 4) >> 3;
+        dst[12] = (c * tmp[12] + d * tmp[28] + 4) >> 3;
+        dst[13] = (c * tmp[13] + d * tmp[29] + 4) >> 3;
+        dst[14] = (c * tmp[14] + d * tmp[30] + 4) >> 3;
+        dst[15] = (c * tmp[15] + d * tmp[31] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x0f(%[tmp])                        \n\t"
+            "gsldrc1    %[ftmp2],   0x08(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp3],   0x17(%[tmp])                        \n\t"
+            "mtc1       %[c],       %[ftmp13]                           \n\t"
+            "gsldrc1    %[ftmp3],   0x10(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp4],   0x1f(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp14]                           \n\t"
+            "gsldrc1    %[ftmp4],   0x18(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+            "uld        %[all64],   0x10(%[tmp])                        \n\t"
+            "mtc1       %[c],       %[ftmp13]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp3]                            \n\t"
+            "uld        %[all64],   0x18(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp14]                           \n\t"
+            "dmtc1      %[all64],   %[ftmp4]                            \n\t"
+#endif
+            "pshufh     %[ftmp13],  %[ftmp13],      %[ftmp0]            \n\t"
+            "pshufh     %[ftmp14],  %[ftmp14],      %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp9],   %[ftmp3],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp10],  %[ftmp3],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp11],  %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp12],  %[ftmp4],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "pmullh     %[ftmp9],   %[ftmp9],       %[ftmp14]           \n\t"
+            "pmullh     %[ftmp10],  %[ftmp10],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp11],  %[ftmp11],      %[ftmp14]           \n\t"
+            "pmullh     %[ftmp12],  %[ftmp12],      %[ftmp14]           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp9]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp10]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp11]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp12]           \n\t"
+            "dmtc1      %[ff_pw_4], %[ftmp14]                           \n\t"
+            "dmtc1      %[ff_pw_3], %[ftmp13]                           \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp7],   %[ftmp7],       %[ftmp14]           \n\t"
+            "paddsh     %[ftmp8],   %[ftmp8],       %[ftmp14]           \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp13]           \n\t"
+            "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp13]           \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+            "packushb   %[ftmp7],   %[ftmp7],       %[ftmp8]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+            "gssdlc1    %[ftmp7],   0x0f(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp7],   0x08(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+            "dmfc1      %[all64],   %[ftmp7]                            \n\t"
+            "usd        %[all64],   0x08(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
+              [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
+              [ftmp14]"=&f"(ftmp[14]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [tmp]"r"(tmp),                [c]"r"(c),
+              [d]"r"(d),
+              [ff_pw_4]"r"(ff_pw_4),        [ff_pw_3]"r"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        tmp += 16;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 16; x++)
+            dst[x] = (c * tmp[x] + d * tmp[x + 16] + 4) >> 3;
+        dst += dstride;
+        tmp += 16;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear8_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int y;
+    double ftmp[9];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        dst[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        dst[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        dst[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        dst[4] = (a * src[4] + b * src[5] + 4) >> 3;
+        dst[5] = (a * src[5] + b * src[6] + 4) >> 3;
+        dst[6] = (a * src[6] + b * src[7] + 4) >> 3;
+        dst[7] = (a * src[7] + b * src[8] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp2],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ff_pw_4]          \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ff_pw_3]          \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear8_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int c = 8 - my, d = my;
+    int y;
+    double ftmp[9];
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
+        dst[1] = (c * src[1] + d * src[1 + sstride] + 4) >> 3;
+        dst[2] = (c * src[2] + d * src[2 + sstride] + 4) >> 3;
+        dst[3] = (c * src[3] + d * src[3 + sstride] + 4) >> 3;
+        dst[4] = (c * src[4] + d * src[4 + sstride] + 4) >> 3;
+        dst[5] = (c * src[5] + d * src[5 + sstride] + 4) >> 3;
+        dst[6] = (c * src[6] + d * src[6 + sstride] + 4) >> 3;
+        dst[7] = (c * src[7] + d * src[7 + sstride] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src1])                       \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src1])                       \n\t"
+            "gsldlc1    %[ftmp2],   0x07(%[src2])                       \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp2],   0x00(%[src2])                       \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src1])                       \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x00(%[src2])                       \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ff_pw_4]          \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ff_pw_3]          \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [src1]"r"(src),               [src2]"r"(src+sstride),
+              [c]"r"(c),                    [d]"r"(d),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int c = 8 - my, d = my;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = (c * src[x] + d * src[x + sstride] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear8_hv_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int y;
+    uint8_t tmp_array[136];
+    uint8_t *tmp = tmp_array;
+    double ftmp[9];
+    uint64_t all64;
+
+    for (y = 0; y < h + 1; y++) {
+        /*
+        tmp[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        tmp[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        tmp[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        tmp[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        tmp[4] = (a * src[4] + b * src[5] + 4) >> 3;
+        tmp[5] = (a * src[5] + b * src[6] + 4) >> 3;
+        tmp[6] = (a * src[6] + b * src[7] + 4) >> 3;
+        tmp[7] = (a * src[7] + b * src[8] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[src])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x08(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp2],   0x01(%[src])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[src])                        \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x01(%[src])                        \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ff_pw_4]          \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ff_pw_3]          \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),
+              [all64]"=&r"(all64),
+              [tmp]"+&r"(tmp)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+        tmp += 8;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int x, y;
+    uint8_t tmp_array[136];
+    uint8_t *tmp = tmp_array;
+
+    for (y = 0; y < h + 1; y++) {
+        for (x = 0; x < 8; x++)
+            tmp[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        tmp += 8;
+        src += sstride;
+    }
+#endif
+
+    tmp = tmp_array;
+
+#if OK
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * tmp[0] + d * tmp[ 8] + 4) >> 3;
+        dst[1] = (c * tmp[1] + d * tmp[ 9] + 4) >> 3;
+        dst[2] = (c * tmp[2] + d * tmp[10] + 4) >> 3;
+        dst[3] = (c * tmp[3] + d * tmp[11] + 4) >> 3;
+        dst[4] = (c * tmp[4] + d * tmp[12] + 4) >> 3;
+        dst[5] = (c * tmp[5] + d * tmp[13] + 4) >> 3;
+        dst[6] = (c * tmp[6] + d * tmp[14] + 4) >> 3;
+        dst[7] = (c * tmp[7] + d * tmp[15] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+            "gsldlc1    %[ftmp2],   0x0f(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp2],   0x08(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+            "uld        %[all64],   0x08(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp2]                            \n\t"
+#endif
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp4]            \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ftmp7]            \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ftmp8]            \n\t"
+            "paddsh     %[ftmp5],   %[ftmp5],       %[ff_pw_4]          \n\t"
+            "paddsh     %[ftmp6],   %[ftmp6],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp5],   %[ftmp5],       %[ff_pw_3]          \n\t"
+            "psrlh      %[ftmp6],   %[ftmp6],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp5],   %[ftmp5],       %[ftmp6]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp5],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp5],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp5]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [tmp]"r"(tmp),                [c]"r"(c),
+              [d]"r"(d),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        tmp += 8;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 8; x++)
+            dst[x] = (c * tmp[x] + d * tmp[x + 8] + 4) >> 3;
+        dst += dstride;
+        tmp += 8;
+    }
+#endif
+}
+
+//no test
+void ff_put_vp8_bilinear4_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int y;
+    double ftmp[5];
+    int low32;
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        dst[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        dst[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        dst[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [dst]"+&r"(dst),
+              [low32]"=&r"(low32),          [all64]"=&r"(all64)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear4_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int c = 8 - my, d = my;
+    int y;
+    double ftmp[5];
+    int low32;
+    uint64_t all64;
+
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
+        dst[1] = (c * src[1] + d * src[1 + sstride] + 4) >> 3;
+        dst[2] = (c * src[2] + d * src[2 + sstride] + 4) >> 3;
+        dst[3] = (c * src[3] + d * src[3 + sstride] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src1])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+            "ulw        %[low32],   0x00(%[src2])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [dst]"+&r"(dst),
+              [low32]"=&r"(low32),          [all64]"=&r"(all64)
+            : [src1]"r"(src),               [src2]"r"(src+sstride),
+              [c]"r"(c),                    [d]"r"(d),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        src += sstride;
+    }
+#else
+    int c = 8 - my, d = my;
+    int x, y;
+
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = (c * src[x] + d * src[x + sstride] + 4) >> 3;
+        dst += dstride;
+        src += sstride;
+    }
+#endif
+}
+
+void ff_put_vp8_bilinear4_hv_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+        ptrdiff_t sstride, int h, int mx, int my)
+{
+#if OK
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int y;
+    uint8_t tmp_array[36];
+    uint8_t *tmp = tmp_array;
+    double ftmp[5];
+    int low32;
+    uint64_t all64;
+
+    for (y = 0; y < h + 1; y++) {
+        /*
+        tmp[0] = (a * src[0] + b * src[1] + 4) >> 3;
+        tmp[1] = (a * src[1] + b * src[2] + 4) >> 3;
+        tmp[2] = (a * src[2] + b * src[3] + 4) >> 3;
+        tmp[3] = (a * src[3] + b * src[4] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "ulw        %[low32],   0x00(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp1]                            \n\t"
+            "mtc1       %[a],       %[ftmp3]                            \n\t"
+            "ulw        %[low32],   0x01(%[src])                        \n\t"
+            "mtc1       %[low32],   %[ftmp2]                            \n\t"
+            "mtc1       %[b],       %[ftmp4]                            \n\t"
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[tmp])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [tmp]"+&r"(tmp),
+              [low32]"=&r"(low32),          [all64]"=&r"(all64)
+            : [src]"r"(src),                [a]"r"(a),
+              [b]"r"(b),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        tmp += 4;
+        src += sstride;
+    }
+#else
+    int a = 8 - mx, b = mx;
+    int c = 8 - my, d = my;
+    int x, y;
+    uint8_t tmp_array[36];
+    uint8_t *tmp = tmp_array;
+
+    for (y = 0; y < h + 1; y++) {
+        for (x = 0; x < 4; x++)
+            tmp[x] = (a * src[x] + b * src[x + 1] + 4) >> 3;
+        tmp += 4;
+        src += sstride;
+    }
+#endif
+
+    tmp = tmp_array;
+
+#if OK
+    for (y = 0; y < h; y++) {
+        /*
+        dst[0] = (c * tmp[0] + d * tmp[4] + 4) >> 3;
+        dst[1] = (c * tmp[1] + d * tmp[5] + 4) >> 3;
+        dst[2] = (c * tmp[2] + d * tmp[6] + 4) >> 3;
+        dst[3] = (c * tmp[3] + d * tmp[7] + 4) >> 3;
+        */
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "mtc1       %[c],       %[ftmp3]                            \n\t"
+#if HAVE_LOONGSON3
+            "gsldlc1    %[ftmp1],   0x07(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "gsldrc1    %[ftmp1],   0x00(%[tmp])                        \n\t"
+#elif HAVE_LOONGSON2
+            "uld        %[all64],   0x00(%[tmp])                        \n\t"
+            "mtc1       %[d],       %[ftmp4]                            \n\t"
+            "dmtc1      %[all64],   %[ftmp1]                            \n\t"
+#endif
+            "pshufh     %[ftmp3],   %[ftmp3],       %[ftmp0]            \n\t"
+            "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp0]            \n\t"
+            "punpckhbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
+            "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            "paddsh     %[ftmp1],   %[ftmp1],       %[ff_pw_4]          \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ff_pw_3]          \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+#if HAVE_LOONGSON3
+            "gssdlc1    %[ftmp1],   0x07(%[dst])                        \n\t"
+            "gssdrc1    %[ftmp1],   0x00(%[dst])                        \n\t"
+#elif HAVE_LOONGSON2
+            "dmfc1      %[all64],   %[ftmp1]                            \n\t"
+            "usd        %[all64],   0x00(%[dst])                        \n\t"
+#endif
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),
+              [all64]"=&r"(all64),
+              [dst]"+&r"(dst)
+            : [tmp]"r"(tmp),                [c]"r"(c),
+              [d]"r"(d),
+              [ff_pw_4]"f"(ff_pw_4),        [ff_pw_3]"f"(ff_pw_3)
+            : "memory"
+        );
+
+        dst += dstride;
+        tmp += 4;
+    }
+#else
+    for (y = 0; y < h; y++) {
+        for (x = 0; x < 4; x++)
+            dst[x] = (c * tmp[x] + d * tmp[x + 4] + 4) >> 3;
+        dst += dstride;
+        tmp += 4;
+    }
+#endif
+}
diff --git a/libavutil/mips/asmdefs.h b/libavutil/mips/asmdefs.h
index fdf82a0..7481199 100644
--- a/libavutil/mips/asmdefs.h
+++ b/libavutil/mips/asmdefs.h
@@ -28,19 +28,31 @@
 #define AVUTIL_MIPS_ASMDEFS_H
 
 #if defined(_ABI64) && _MIPS_SIM == _ABI64
+# define mips_reg       int64_t
 # define PTRSIZE        " 8 "
 # define PTRLOG         " 3 "
 # define PTR_ADDU       "daddu "
 # define PTR_ADDIU      "daddiu "
+# define PTR_ADDI       "daddi "
 # define PTR_SUBU       "dsubu "
 # define PTR_L          "ld "
+# define PTR_S          "sd "
+# define PTR_SRA        "dsra "
+# define PTR_SRL        "dsrl "
+# define PTR_SLL        "dsll "
 #else
+# define mips_reg       int32_t
 # define PTRSIZE        " 4 "
 # define PTRLOG         " 2 "
 # define PTR_ADDU       "addu "
 # define PTR_ADDIU      "addiu "
+# define PTR_ADDI       "addi "
 # define PTR_SUBU       "subu "
 # define PTR_L          "lw "
+# define PTR_S          "sw "
+# define PTR_SRA        "sra "
+# define PTR_SRL        "srl "
+# define PTR_SLL        "sll "
 #endif
 
 #endif /* AVCODEC_MIPS_ASMDEFS_H */
-- 
2.9.2

